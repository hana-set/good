---
title:  "0.Cross Validation"
excerpt: "각 분석에 맞는 CV 방법을 알아보자"
categories:
  - Py_Analysis
tags:
  - 기초
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
---

# 📝Intro
- data 의 Cross validation 를 통해서 내 모델의 성능을 가늠할 수 있으므로 매우 중요하다.
- 하지만 데이터를 쪼갤때에 마구잡이로 쪼갤 수 없는데, 그때에 고려해야할 점은 아래와 같다.

**고려사항**

-  Classification 의 경우 그 클래스를 비율에 맞게 쪼개야 한다.
    - 그렇지 않으면 소수 Class 의 경우 한쪽에만 쏠리게 되는 불상사가 발생할 수 있다.
    - 그러면 Cross validation 을 진행하게되면 성능이 들쭉날쭉하게 된다.
- data 가 randomly 하게 섞여있는지 확인
    - data 를 그냥 순서대로 잘라버리면, 시계열데이터 등의 경우 편향이 일어나게 됨


```python
from IPython.display import Image
```


```python
from sklearn.datasets import load_boston
import pandas as pd
boston = load_boston() 
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = pd.DataFrame(boston.target, columns=["MEDV"])
df = pd.concat([X, y], axis=1)
```


```python
df
# 데이터는 보스턴 집값예측으로, MEDV 를 예측해야한다. 곁측치가 하나도 없는 데이터임.
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 14 columns</p>
</div>



# Train_test_Split

- Train_test_split 을 할 때에도 y 의 클래스에 따라서 그 클래스의 비율을 유지하게 할 수 있다.


```python
from sklearn.datasets import load_iris
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, dtype="category")
```


```python
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, 
                                                  test_size=0.20, # test size 는 기본적으로 0.2~0.3 을 사용
                                                  random_state=0, # random state 를 고정해야 재현성 확보
                                                  stratify=y) # 주로 y 가 선택되게 된다.
```


```python
y_train.value_counts()
```




    2    40
    1    40
    0    40
    dtype: int64




```python
y_val.value_counts()
```




    2    10
    1    10
    0    10
    dtype: int64



# K-folds

## Shuffle = False


```python
Image('Pictures/Cross Validation1.png')
# 아래 처럼 K-fold 는 Testing 과 Training 을 나눌 뿐, class 의 비율 등에 관심을 가지지 않는다.
```




    
<img src="/assets/images/0.Cross Validation/output_13_0.png">
    




```python
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold

X=np.array([
    [ 1, 2, 3, 4],
    [11,12,13,14],
    [21,22,23,24],
    [31,32,33,34],
    [41,42,43,44],
    [51,52,53,54],
    [61,62,63,64],
    [71,72,73,74]
])

y=np.array([0,0,0,0,0,0,0,0])

kfold = KFold(n_splits=4,shuffle=False) 
# suffle 을 false 로 두고 돌리게 되면, 아래와 같이 validation 이 그냥 순차적으로 짤리게 된다.
print(X.shape, y.shape)

for train_index, validate_index in kfold.split(X):
    # kfold 는 2개의 index 를 내뱉는데, 그건 train index 와 test index 이다. 
    # 그 두개의 인덱스를 아래와 같이 train, validation 으로 구분하여서 받게 되면 우리가 원하는 X_train/test 가 되는것이다.
    print("train index:", train_index, "validate index:", validate_index)
    # train_dindex 와 validation index 를 list 형태로 받게된다.
    X_train, X_validate = X[train_index], X[validate_index]
    y_train, y_validate = y[train_index], y[validate_index]
    print("train data")
    print(X_train, y_train)
    print("validate data")
    print(X_validate, y_validate)
    # 이제 이 부분에서 fitting 과, 평가를 진행하고, 그 에러를 n_splits 만큼 나누게 된다면 cv 를 진행한것이 된다.
```

    (8, 4) (8,)
    train index: [2 3 4 5 6 7] validate index: [0 1]
    train data
    [[21 22 23 24]
     [31 32 33 34]
     [41 42 43 44]
     [51 52 53 54]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[ 1  2  3  4]
     [11 12 13 14]] [0 0]
    train index: [0 1 4 5 6 7] validate index: [2 3]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [41 42 43 44]
     [51 52 53 54]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[21 22 23 24]
     [31 32 33 34]] [0 0]
    train index: [0 1 2 3 6 7] validate index: [4 5]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [21 22 23 24]
     [31 32 33 34]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[41 42 43 44]
     [51 52 53 54]] [0 0]
    train index: [0 1 2 3 4 5] validate index: [6 7]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [21 22 23 24]
     [31 32 33 34]
     [41 42 43 44]
     [51 52 53 54]] [0 0 0 0 0 0]
    validate data
    [[61 62 63 64]
     [71 72 73 74]] [0 0]
    

## Shuffle = True

- Shuffle 을 True 로 놓게 된다면, 우리가 사전에 정의한 random state 에 따라서, index 들을 랜덤하게 나누게 된다.
- 이때 Cross validation 에 들어갓던 index 들은 그 이후 set에서 나오지 않는다.
    - 즉 validation index 가 겹치지 않는다는것


```python
kfold = KFold(n_splits=4,random_state=0,shuffle=True) 
# suffle 을 false 로 두고 돌리게 되면, 아래와 같이 validation 이 그냥 순차적으로 짤리게 된다.
print(X.shape, y.shape)

for train_index, validate_index in kfold.split(X):
    print("train index:", train_index, "validate index:", validate_index)
    X_train, X_validate = X[train_index], X[validate_index]
    y_train, y_validate = y[train_index], y[validate_index]
    print("train data")
    print(X_train, y_train)
    print("validate data")
    print(X_validate, y_validate)
```

    (8, 4) (8,)
    train index: [0 1 3 4 5 7] validate index: [2 6]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [31 32 33 34]
     [41 42 43 44]
     [51 52 53 54]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[21 22 23 24]
     [61 62 63 64]] [0 0]
    train index: [0 2 3 4 5 6] validate index: [1 7]
    train data
    [[ 1  2  3  4]
     [21 22 23 24]
     [31 32 33 34]
     [41 42 43 44]
     [51 52 53 54]
     [61 62 63 64]] [0 0 0 0 0 0]
    validate data
    [[11 12 13 14]
     [71 72 73 74]] [0 0]
    train index: [1 2 4 5 6 7] validate index: [0 3]
    train data
    [[11 12 13 14]
     [21 22 23 24]
     [41 42 43 44]
     [51 52 53 54]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[ 1  2  3  4]
     [31 32 33 34]] [0 0]
    train index: [0 1 2 3 6 7] validate index: [4 5]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [21 22 23 24]
     [31 32 33 34]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[41 42 43 44]
     [51 52 53 54]] [0 0]
    

# Shuffle & Split

- 각 index 들을 shuffle 한 이후에 랜덤하게 뽑는다는 의미이다
- 전에 쓴 인덱스 들도, 다시 사용하는게 가능하다. 
- 그에 따라서 인덱스 겹침 현상이 나타나고, 중복된 데이터를 사용하는 현상이 나타날 수 있다.


```python
Image('Pictures/Cross Validation2.png')
# 아래 처럼 Shuffle/split 은 인덱스가 '겹칠' 수 있다.
```




    
<img src="/assets/images/0.Cross Validation/output_20_0.png">
    




```python
from sklearn.model_selection import ShuffleSplit
```


```python
shuffle = ShuffleSplit(n_splits=4,random_state=0,test_size=0.25) 
print(X.shape, y.shape)

for train_index, validate_index in shuffle.split(X):
    print("train index:", train_index, "validate index:", validate_index)
    X_train, X_validate = X[train_index], X[validate_index]
    y_train, y_validate = y[train_index], y[validate_index]
    print("train data")
    print(X_train, y_train)
    print("validate data")
    print(X_validate, y_validate)
```

    (8, 4) (8,)
    train index: [1 7 3 0 5 4] validate index: [6 2]
    train data
    [[11 12 13 14]
     [71 72 73 74]
     [31 32 33 34]
     [ 1  2  3  4]
     [51 52 53 54]
     [41 42 43 44]] [0 0 0 0 0 0]
    validate data
    [[61 62 63 64]
     [21 22 23 24]] [0 0]
    train index: [3 7 0 4 2 5] validate index: [1 6]
    train data
    [[31 32 33 34]
     [71 72 73 74]
     [ 1  2  3  4]
     [41 42 43 44]
     [21 22 23 24]
     [51 52 53 54]] [0 0 0 0 0 0]
    validate data
    [[11 12 13 14]
     [61 62 63 64]] [0 0]
    train index: [3 4 7 0 6 1] validate index: [5 2]
    train data
    [[31 32 33 34]
     [41 42 43 44]
     [71 72 73 74]
     [ 1  2  3  4]
     [61 62 63 64]
     [11 12 13 14]] [0 0 0 0 0 0]
    validate data
    [[51 52 53 54]
     [21 22 23 24]] [0 0]
    train index: [6 7 3 4 1 0] validate index: [2 5]
    train data
    [[61 62 63 64]
     [71 72 73 74]
     [31 32 33 34]
     [41 42 43 44]
     [11 12 13 14]
     [ 1  2  3  4]] [0 0 0 0 0 0]
    validate data
    [[21 22 23 24]
     [51 52 53 54]] [0 0]
    

# StratifiedKFold

- 우리의 target 이 categorical 일 때에만 사용한다. regression 일 때에는 kfold 를 써야함.


```python
Image('Pictures/Cross Validation3.png')
```




    
<img src="/assets/images/0.Cross Validation/output_25_0.png">
    



- 나눌때 최대한 class, group 등에 대한 분포를 유지한 채로 Testing 과 training 을 나누려고 한다.


```python
stratifiedkfold = StratifiedKFold(n_splits=4,random_state=0,shuffle=True)
print(X.shape, y.shape)

print("\nStratifiedKFold**************")
for train_index, validate_index in stratifiedkfold.split(X,y):
    print("train index:", train_index, "validate index:", validate_index)
    X_train, X_validate = X[train_index], X[validate_index]
    y_train, y_validate = y[train_index], y[validate_index]
    print("train data")
    print(X_train, y_train)
    print("validate data")
    print(X_validate, y_validate)
```

    (8, 4) (8,)
    
    StratifiedKFold**************
    train index: [0 1 3 4 6 7] validate index: [2 5]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [31 32 33 34]
     [41 42 43 44]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[21 22 23 24]
     [51 52 53 54]] [0 0]
    train index: [0 2 3 5 6 7] validate index: [1 4]
    train data
    [[ 1  2  3  4]
     [21 22 23 24]
     [31 32 33 34]
     [51 52 53 54]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[11 12 13 14]
     [41 42 43 44]] [0 0]
    train index: [0 1 2 3 4 5] validate index: [6 7]
    train data
    [[ 1  2  3  4]
     [11 12 13 14]
     [21 22 23 24]
     [31 32 33 34]
     [41 42 43 44]
     [51 52 53 54]] [0 0 0 0 0 0]
    validate data
    [[61 62 63 64]
     [71 72 73 74]] [0 0]
    train index: [1 2 4 5 6 7] validate index: [0 3]
    train data
    [[11 12 13 14]
     [21 22 23 24]
     [41 42 43 44]
     [51 52 53 54]
     [61 62 63 64]
     [71 72 73 74]] [0 0 0 0 0 0]
    validate data
    [[ 1  2  3  4]
     [31 32 33 34]] [0 0]
    


```python
# 아래와 같이 y 의 클래스가 3개 인 0 값이 있고, 이 값은 4보다 작아서 에러가 난다.
# 어쩃든 split 이, class 의 수 보다 많은 상황인데 이런 상황은 매우 드물어서(실제데이터는 거의 몇만개인데 당연히 각 정답 클래스는 적어도 10개 이상일것. 그리고 split 의 수도 대게 5개만 쓰므로) 
# 그냥 알아두기만 하자. 
```


```python
y=np.array([1,2,3,4,5,0,0,0])
print("\nStratifiedKFold**************")
for train_index, validate_index in stratifiedkfold.split(X,y):
    print("train index:", train_index, "validate index:", validate_index)
    X_train, X_validate = X[train_index], X[validate_index]
    y_train, y_validate = y[train_index], y[validate_index]
    print("train data")
    print(X_train, y_train)
    print("validate data")
    print(X_validate, y_validate)
```

    
    StratifiedKFold**************
    


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-59-73b24ad0cdca> in <module>
          1 y=np.array([1,2,3,4,5,0,0,0])
          2 print("\nStratifiedKFold**************")
    ----> 3 for train_index, validate_index in stratifiedkfold.split(X,y):
          4     print("train index:", train_index, "validate index:", validate_index)
          5     X_train, X_validate = X[train_index], X[validate_index]
    

    ~\Anaconda3\envs\tensor\lib\site-packages\sklearn\model_selection\_split.py in split(self, X, y, groups)
        330                 .format(self.n_splits, n_samples))
        331 
    --> 332         for train, test in super().split(X, y, groups):
        333             yield train, test
        334 
    

    ~\Anaconda3\envs\tensor\lib\site-packages\sklearn\model_selection\_split.py in split(self, X, y, groups)
         78         X, y, groups = indexable(X, y, groups)
         79         indices = np.arange(_num_samples(X))
    ---> 80         for test_index in self._iter_test_masks(X, y, groups):
         81             train_index = indices[np.logical_not(test_index)]
         82             test_index = indices[test_index]
    

    ~\Anaconda3\envs\tensor\lib\site-packages\sklearn\model_selection\_split.py in _iter_test_masks(self, X, y, groups)
        691 
        692     def _iter_test_masks(self, X, y=None, groups=None):
    --> 693         test_folds = self._make_test_folds(X, y)
        694         for i in range(self.n_splits):
        695             yield test_folds == i
    

    ~\Anaconda3\envs\tensor\lib\site-packages\sklearn\model_selection\_split.py in _make_test_folds(self, X, y)
        662             raise ValueError("n_splits=%d cannot be greater than the"
        663                              " number of members in each class."
    --> 664                              % (self.n_splits))
        665         if self.n_splits > min_groups:
        666             warnings.warn(("The least populated class in y has only %d"
    

    ValueError: n_splits=4 cannot be greater than the number of members in each class.


# 실제 적용 Example

## 모듈 import


```python
import numpy as np 
import pandas as pd 
import os
from scipy import stats
from sklearn.model_selection import KFold
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn import linear_model
import datetime
```


```python
train = df[:400] 
test = df[400:]
```


```python
# 훈련용 데이터 준비
x_train = train.iloc[:,:-1]
y_train = train.iloc[:,-1]
x_test = test.iloc[:,:-1]
y_test = train.iloc[:,-1]

# K Fold 및 기본 데이터 준비
# fold 는 기본적으로 shffle 을 주고, random state 를 지정하여 재현성을 갖춘다.
folds = KFold(n_splits = 5, shuffle = True, random_state = 42)
oof_preds = np.zeros(train.shape[0])
```

## Catboost


```python
# catboost 이용
print("using catboost")
sub_preds_catboost = np.zeros(test.shape[0])
# sub pred 는 여기서 제출용의 pred 값이다. test set 은 
oof_preds_catboost = np.zeros(train.shape[0])
# oof_pred 란, CV 를 시행시 Train set 에서 설정한 fold 로 나누어주는 과정을 거치는데, 그 과정 내에서(fold 가 5라면) 
# 4/5 는 training ,나미저 1/5 는 예측하는 과정을 5번 반복하여, 결국에는 full train set 에 대해 예측을 하게 된다.
# 그 예측값을 oof predict(Out-of-fold predictions) 이라고 한다.
# 그 예측값을 담아내기 위해 oof_pre_catboost 라는 행렬을 만들었고, 그 길이는 train 의 row(데이터 수) 만큼으로 생성
for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)) :
    # enumerate 를 통해서 몇번째 fold 인지(n_fold) 를 구분할 수 있게 해준다. 
    # 물론 그냥 없이 할 수 있지만, 구분하기 쉬워지고, n_fold 를 통해 각 CV 의 성능을 알아볼 수 도 있다.
    trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]
    val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]
    cb_model = CatBoostRegressor(iterations=100,
                                 random_seed=42)
    cb_model.fit(trn_x, trn_y,
             eval_set=(val_x, val_y), # validation 을 통해서 iteration 의 크기를 정해주어야 한다.
             cat_features=[],
             use_best_model=True, # 최고 모델만 사용
             verbose=True)
    oof_preds_catboost[val_idx] = cb_model.predict(val_x) # validation 에 대한 prediction 
    sub_preds_catboost += cb_model.predict(x_test) / folds.n_splits 
    # 각 n fold 에 대해서 test 에 대한 예측값을 모두 더해준 후, prediction 값을 folds 만큼 더해준다. (즉 n.splits 수만큼의 모델 예측 평균)
    # 왜 위의 코드가 필요한가? : tree boosting 모델은 iteration 조절을 위해 eval set 이 필요하다. -> 이는 결국 train set/ eval set 을 나누면 train set에 오버피팅이 있을 수 있다. 
    # 즉 이런 overfitting 을 최대한 줄이고자 각 k-fold 로 나눈 샘플에 대해 5번 fitting 을 진행해서 overfitting 문제를 줄여나간거.
```

    using catboost
    Learning rate set to 0.155941
    0:	learn: 8.3794960	test: 8.6058789	best: 8.6058789 (0)	total: 164ms	remaining: 16.2s
    1:	learn: 7.6512218	test: 7.9748982	best: 7.9748982 (1)	total: 165ms	remaining: 8.1s
    2:	learn: 7.1115945	test: 7.4292963	best: 7.4292963 (2)	total: 167ms	remaining: 5.39s
    3:	learn: 6.5287709	test: 6.8045174	best: 6.8045174 (3)	total: 168ms	remaining: 4.03s
    4:	learn: 6.0060762	test: 6.2888373	best: 6.2888373 (4)	total: 169ms	remaining: 3.21s
    5:	learn: 5.5935779	test: 5.9185912	best: 5.9185912 (5)	total: 170ms	remaining: 2.67s
    6:	learn: 5.2926162	test: 5.6323119	best: 5.6323119 (6)	total: 172ms	remaining: 2.28s
    7:	learn: 4.9076659	test: 5.2981542	best: 5.2981542 (7)	total: 173ms	remaining: 1.99s
    8:	learn: 4.6553936	test: 5.0712208	best: 5.0712208 (8)	total: 175ms	remaining: 1.77s
    9:	learn: 4.3118714	test: 4.7917082	best: 4.7917082 (9)	total: 176ms	remaining: 1.59s
    10:	learn: 4.0152523	test: 4.5080613	best: 4.5080613 (10)	total: 178ms	remaining: 1.44s
    11:	learn: 3.7816760	test: 4.3046700	best: 4.3046700 (11)	total: 179ms	remaining: 1.31s
    12:	learn: 3.6634146	test: 4.2229959	best: 4.2229959 (12)	total: 180ms	remaining: 1.2s
    13:	learn: 3.4753463	test: 4.0957130	best: 4.0957130 (13)	total: 181ms	remaining: 1.11s
    14:	learn: 3.3053353	test: 3.9550233	best: 3.9550233 (14)	total: 182ms	remaining: 1.03s
    15:	learn: 3.1537400	test: 3.8510974	best: 3.8510974 (15)	total: 184ms	remaining: 964ms
    16:	learn: 3.0571814	test: 3.7666109	best: 3.7666109 (16)	total: 185ms	remaining: 901ms
    17:	learn: 2.9616676	test: 3.7103497	best: 3.7103497 (17)	total: 186ms	remaining: 847ms
    18:	learn: 2.8292124	test: 3.6240304	best: 3.6240304 (18)	total: 187ms	remaining: 797ms
    19:	learn: 2.7601225	test: 3.5878418	best: 3.5878418 (19)	total: 189ms	remaining: 755ms
    20:	learn: 2.6793626	test: 3.5588446	best: 3.5588446 (20)	total: 190ms	remaining: 715ms
    21:	learn: 2.5976204	test: 3.5073435	best: 3.5073435 (21)	total: 191ms	remaining: 678ms
    22:	learn: 2.5350467	test: 3.4702166	best: 3.4702166 (22)	total: 193ms	remaining: 644ms
    23:	learn: 2.4640086	test: 3.4402500	best: 3.4402500 (23)	total: 194ms	remaining: 613ms
    24:	learn: 2.4091643	test: 3.4132759	best: 3.4132759 (24)	total: 195ms	remaining: 584ms
    25:	learn: 2.3598778	test: 3.3904603	best: 3.3904603 (25)	total: 196ms	remaining: 558ms
    26:	learn: 2.2958900	test: 3.3646026	best: 3.3646026 (26)	total: 197ms	remaining: 533ms
    27:	learn: 2.2484038	test: 3.3482530	best: 3.3482530 (27)	total: 198ms	remaining: 510ms
    28:	learn: 2.2086756	test: 3.3176093	best: 3.3176093 (28)	total: 199ms	remaining: 488ms
    29:	learn: 2.1695764	test: 3.3126856	best: 3.3126856 (29)	total: 201ms	remaining: 468ms
    30:	learn: 2.1314599	test: 3.3087798	best: 3.3087798 (30)	total: 202ms	remaining: 451ms
    31:	learn: 2.1020211	test: 3.3021320	best: 3.3021320 (31)	total: 204ms	remaining: 433ms
    32:	learn: 2.0668633	test: 3.2881442	best: 3.2881442 (32)	total: 205ms	remaining: 416ms
    33:	learn: 2.0405534	test: 3.2794816	best: 3.2794816 (33)	total: 206ms	remaining: 400ms
    34:	learn: 2.0098619	test: 3.2728403	best: 3.2728403 (34)	total: 207ms	remaining: 385ms
    35:	learn: 1.9760801	test: 3.2712758	best: 3.2712758 (35)	total: 209ms	remaining: 371ms
    36:	learn: 1.9448650	test: 3.2547184	best: 3.2547184 (36)	total: 210ms	remaining: 357ms
    37:	learn: 1.9160246	test: 3.2354943	best: 3.2354943 (37)	total: 211ms	remaining: 344ms
    38:	learn: 1.8794020	test: 3.2143674	best: 3.2143674 (38)	total: 212ms	remaining: 332ms
    39:	learn: 1.8538454	test: 3.1937626	best: 3.1937626 (39)	total: 213ms	remaining: 320ms
    40:	learn: 1.8231252	test: 3.1946057	best: 3.1937626 (39)	total: 215ms	remaining: 309ms
    41:	learn: 1.7961163	test: 3.1909965	best: 3.1909965 (41)	total: 216ms	remaining: 298ms
    42:	learn: 1.7714350	test: 3.1963020	best: 3.1909965 (41)	total: 217ms	remaining: 288ms
    43:	learn: 1.7427412	test: 3.1962841	best: 3.1909965 (41)	total: 218ms	remaining: 278ms
    44:	learn: 1.7229120	test: 3.1948506	best: 3.1909965 (41)	total: 219ms	remaining: 268ms
    45:	learn: 1.7016388	test: 3.2008064	best: 3.1909965 (41)	total: 220ms	remaining: 259ms
    46:	learn: 1.6736757	test: 3.2004001	best: 3.1909965 (41)	total: 222ms	remaining: 250ms
    47:	learn: 1.6522090	test: 3.1897041	best: 3.1897041 (47)	total: 223ms	remaining: 241ms
    48:	learn: 1.6378718	test: 3.1881830	best: 3.1881830 (48)	total: 224ms	remaining: 233ms
    49:	learn: 1.6139135	test: 3.1909531	best: 3.1881830 (48)	total: 225ms	remaining: 225ms
    50:	learn: 1.5897510	test: 3.1901680	best: 3.1881830 (48)	total: 226ms	remaining: 217ms
    51:	learn: 1.5769633	test: 3.1829538	best: 3.1829538 (51)	total: 227ms	remaining: 210ms
    52:	learn: 1.5518453	test: 3.1842242	best: 3.1829538 (51)	total: 228ms	remaining: 203ms
    53:	learn: 1.5394358	test: 3.1791651	best: 3.1791651 (53)	total: 230ms	remaining: 196ms
    54:	learn: 1.5243920	test: 3.1688168	best: 3.1688168 (54)	total: 231ms	remaining: 189ms
    55:	learn: 1.5177724	test: 3.1628911	best: 3.1628911 (55)	total: 232ms	remaining: 182ms
    56:	learn: 1.4944326	test: 3.1628327	best: 3.1628327 (56)	total: 233ms	remaining: 176ms
    57:	learn: 1.4699420	test: 3.1547178	best: 3.1547178 (57)	total: 234ms	remaining: 170ms
    58:	learn: 1.4606110	test: 3.1509847	best: 3.1509847 (58)	total: 235ms	remaining: 164ms
    59:	learn: 1.4353836	test: 3.1499046	best: 3.1499046 (59)	total: 236ms	remaining: 158ms
    60:	learn: 1.4231319	test: 3.1482671	best: 3.1482671 (60)	total: 238ms	remaining: 152ms
    61:	learn: 1.4154132	test: 3.1453782	best: 3.1453782 (61)	total: 239ms	remaining: 146ms
    62:	learn: 1.4074301	test: 3.1382315	best: 3.1382315 (62)	total: 240ms	remaining: 141ms
    63:	learn: 1.3806290	test: 3.1383818	best: 3.1382315 (62)	total: 241ms	remaining: 136ms
    64:	learn: 1.3700620	test: 3.1356546	best: 3.1356546 (64)	total: 242ms	remaining: 130ms
    65:	learn: 1.3402272	test: 3.1383333	best: 3.1356546 (64)	total: 243ms	remaining: 125ms
    66:	learn: 1.3343170	test: 3.1389653	best: 3.1356546 (64)	total: 245ms	remaining: 120ms
    67:	learn: 1.3117911	test: 3.1335491	best: 3.1335491 (67)	total: 246ms	remaining: 116ms
    68:	learn: 1.2907172	test: 3.1267977	best: 3.1267977 (68)	total: 247ms	remaining: 111ms
    69:	learn: 1.2676836	test: 3.1308278	best: 3.1267977 (68)	total: 248ms	remaining: 106ms
    70:	learn: 1.2534801	test: 3.1156640	best: 3.1156640 (70)	total: 249ms	remaining: 102ms
    71:	learn: 1.2328309	test: 3.1295827	best: 3.1156640 (70)	total: 250ms	remaining: 97.3ms
    72:	learn: 1.2113584	test: 3.1280299	best: 3.1156640 (70)	total: 251ms	remaining: 92.9ms
    73:	learn: 1.2081533	test: 3.1260282	best: 3.1156640 (70)	total: 252ms	remaining: 88.6ms
    74:	learn: 1.1911340	test: 3.1285332	best: 3.1156640 (70)	total: 253ms	remaining: 84.4ms
    75:	learn: 1.1662045	test: 3.1390849	best: 3.1156640 (70)	total: 254ms	remaining: 80.3ms
    76:	learn: 1.1488409	test: 3.1338959	best: 3.1156640 (70)	total: 255ms	remaining: 76.3ms
    77:	learn: 1.1274228	test: 3.1342280	best: 3.1156640 (70)	total: 257ms	remaining: 72.4ms
    78:	learn: 1.1168479	test: 3.1333918	best: 3.1156640 (70)	total: 258ms	remaining: 68.5ms
    79:	learn: 1.1085366	test: 3.1313997	best: 3.1156640 (70)	total: 259ms	remaining: 64.8ms
    80:	learn: 1.0879216	test: 3.1345301	best: 3.1156640 (70)	total: 260ms	remaining: 61.1ms
    81:	learn: 1.0762495	test: 3.1289270	best: 3.1156640 (70)	total: 262ms	remaining: 57.4ms
    82:	learn: 1.0621863	test: 3.1318117	best: 3.1156640 (70)	total: 263ms	remaining: 53.8ms
    83:	learn: 1.0488130	test: 3.1291659	best: 3.1156640 (70)	total: 264ms	remaining: 50.3ms
    84:	learn: 1.0342720	test: 3.1203662	best: 3.1156640 (70)	total: 265ms	remaining: 46.8ms
    85:	learn: 1.0191196	test: 3.1239201	best: 3.1156640 (70)	total: 266ms	remaining: 43.3ms
    86:	learn: 1.0119795	test: 3.1281701	best: 3.1156640 (70)	total: 267ms	remaining: 40ms
    87:	learn: 1.0064796	test: 3.1292246	best: 3.1156640 (70)	total: 269ms	remaining: 36.6ms
    88:	learn: 1.0047999	test: 3.1283472	best: 3.1156640 (70)	total: 269ms	remaining: 33.3ms
    89:	learn: 0.9921467	test: 3.1205692	best: 3.1156640 (70)	total: 271ms	remaining: 30.1ms
    90:	learn: 0.9858464	test: 3.1180941	best: 3.1156640 (70)	total: 272ms	remaining: 26.9ms
    91:	learn: 0.9726621	test: 3.1165195	best: 3.1156640 (70)	total: 273ms	remaining: 23.7ms
    92:	learn: 0.9614740	test: 3.1096483	best: 3.1096483 (92)	total: 274ms	remaining: 20.6ms
    93:	learn: 0.9481434	test: 3.1117339	best: 3.1096483 (92)	total: 275ms	remaining: 17.6ms
    94:	learn: 0.9350639	test: 3.1137102	best: 3.1096483 (92)	total: 276ms	remaining: 14.6ms
    95:	learn: 0.9243303	test: 3.1063839	best: 3.1063839 (95)	total: 278ms	remaining: 11.6ms
    96:	learn: 0.9166268	test: 3.1090978	best: 3.1063839 (95)	total: 279ms	remaining: 8.62ms
    97:	learn: 0.9095062	test: 3.1092556	best: 3.1063839 (95)	total: 280ms	remaining: 5.71ms
    98:	learn: 0.8959507	test: 3.1122477	best: 3.1063839 (95)	total: 281ms	remaining: 2.84ms
    99:	learn: 0.8863585	test: 3.1094405	best: 3.1063839 (95)	total: 282ms	remaining: 0us
    
    bestTest = 3.106383884
    bestIteration = 95
    
    Shrink model to first 96 iterations.
    Learning rate set to 0.155941
    0:	learn: 8.6347043	test: 6.9387040	best: 6.9387040 (0)	total: 1.24ms	remaining: 122ms
    1:	learn: 7.8876605	test: 6.2041751	best: 6.2041751 (1)	total: 2.72ms	remaining: 133ms
    2:	learn: 7.3475707	test: 5.6756860	best: 5.6756860 (2)	total: 4.16ms	remaining: 134ms
    3:	learn: 6.8502449	test: 5.4234014	best: 5.4234014 (3)	total: 5.69ms	remaining: 137ms
    4:	learn: 6.3216159	test: 4.9979794	best: 4.9979794 (4)	total: 6.8ms	remaining: 129ms
    5:	learn: 5.8801655	test: 4.6285681	best: 4.6285681 (5)	total: 8.09ms	remaining: 127ms
    6:	learn: 5.5551114	test: 4.3450105	best: 4.3450105 (6)	total: 9.46ms	remaining: 126ms
    7:	learn: 5.1353627	test: 4.0430786	best: 4.0430786 (7)	total: 10.7ms	remaining: 123ms
    8:	learn: 4.8772824	test: 3.7740776	best: 3.7740776 (8)	total: 11.9ms	remaining: 121ms
    9:	learn: 4.5810864	test: 3.5713944	best: 3.5713944 (9)	total: 13ms	remaining: 117ms
    10:	learn: 4.3216955	test: 3.4046171	best: 3.4046171 (10)	total: 14.4ms	remaining: 116ms
    11:	learn: 4.0608759	test: 3.2415898	best: 3.2415898 (11)	total: 15.6ms	remaining: 114ms
    12:	learn: 3.8005630	test: 3.1288091	best: 3.1288091 (12)	total: 16.8ms	remaining: 113ms
    13:	learn: 3.6037719	test: 3.0284644	best: 3.0284644 (13)	total: 18.1ms	remaining: 111ms
    14:	learn: 3.4730652	test: 2.9198367	best: 2.9198367 (14)	total: 19.3ms	remaining: 109ms
    15:	learn: 3.3365918	test: 2.8490796	best: 2.8490796 (15)	total: 20.4ms	remaining: 107ms
    16:	learn: 3.2050963	test: 2.7971941	best: 2.7971941 (16)	total: 21.6ms	remaining: 105ms
    17:	learn: 3.1121942	test: 2.7710052	best: 2.7710052 (17)	total: 22.7ms	remaining: 103ms
    18:	learn: 3.0263226	test: 2.7569077	best: 2.7569077 (18)	total: 23.8ms	remaining: 101ms
    19:	learn: 2.9475253	test: 2.7295569	best: 2.7295569 (19)	total: 25ms	remaining: 99.9ms
    20:	learn: 2.8487338	test: 2.6910333	best: 2.6910333 (20)	total: 26ms	remaining: 97.8ms
    21:	learn: 2.7528558	test: 2.6417998	best: 2.6417998 (21)	total: 27.3ms	remaining: 96.6ms
    22:	learn: 2.6916698	test: 2.6363810	best: 2.6363810 (22)	total: 28.7ms	remaining: 96.2ms
    23:	learn: 2.6100214	test: 2.5942476	best: 2.5942476 (23)	total: 29.9ms	remaining: 94.7ms
    24:	learn: 2.5549681	test: 2.5741808	best: 2.5741808 (24)	total: 31ms	remaining: 93.1ms
    25:	learn: 2.5108055	test: 2.5512677	best: 2.5512677 (25)	total: 32.2ms	remaining: 91.7ms
    26:	learn: 2.4592078	test: 2.5271606	best: 2.5271606 (26)	total: 33.4ms	remaining: 90.2ms
    27:	learn: 2.4075756	test: 2.5212673	best: 2.5212673 (27)	total: 34.6ms	remaining: 88.9ms
    28:	learn: 2.3556622	test: 2.5132911	best: 2.5132911 (28)	total: 35.7ms	remaining: 87.3ms
    29:	learn: 2.3187422	test: 2.5027868	best: 2.5027868 (29)	total: 36.8ms	remaining: 86ms
    30:	learn: 2.2762779	test: 2.4750421	best: 2.4750421 (30)	total: 38.1ms	remaining: 84.9ms
    31:	learn: 2.2252025	test: 2.4798390	best: 2.4750421 (30)	total: 39.4ms	remaining: 83.8ms
    32:	learn: 2.2024818	test: 2.4804531	best: 2.4750421 (30)	total: 40.9ms	remaining: 83.1ms
    33:	learn: 2.1813365	test: 2.4812096	best: 2.4750421 (30)	total: 42.1ms	remaining: 81.7ms
    34:	learn: 2.1620628	test: 2.4709081	best: 2.4709081 (34)	total: 43.1ms	remaining: 80ms
    35:	learn: 2.1305119	test: 2.4540978	best: 2.4540978 (35)	total: 44.1ms	remaining: 78.4ms
    36:	learn: 2.0954616	test: 2.4418285	best: 2.4418285 (36)	total: 45.2ms	remaining: 76.9ms
    37:	learn: 2.0659386	test: 2.4393771	best: 2.4393771 (37)	total: 46.2ms	remaining: 75.4ms
    38:	learn: 2.0394497	test: 2.4249234	best: 2.4249234 (38)	total: 47.2ms	remaining: 73.9ms
    39:	learn: 2.0064153	test: 2.4231736	best: 2.4231736 (39)	total: 48.3ms	remaining: 72.4ms
    40:	learn: 1.9662593	test: 2.4042495	best: 2.4042495 (40)	total: 49.3ms	remaining: 70.9ms
    41:	learn: 1.9318148	test: 2.3827303	best: 2.3827303 (41)	total: 50.3ms	remaining: 69.5ms
    42:	learn: 1.9056781	test: 2.3736052	best: 2.3736052 (42)	total: 51.3ms	remaining: 68ms
    43:	learn: 1.8847877	test: 2.3709050	best: 2.3709050 (43)	total: 52.3ms	remaining: 66.5ms
    44:	learn: 1.8497727	test: 2.3682292	best: 2.3682292 (44)	total: 53.7ms	remaining: 65.7ms
    45:	learn: 1.8040994	test: 2.3879854	best: 2.3682292 (44)	total: 54.9ms	remaining: 64.4ms
    46:	learn: 1.7963615	test: 2.3895459	best: 2.3682292 (44)	total: 56ms	remaining: 63.2ms
    47:	learn: 1.7740664	test: 2.3862187	best: 2.3682292 (44)	total: 57ms	remaining: 61.8ms
    48:	learn: 1.7551920	test: 2.3814638	best: 2.3682292 (44)	total: 58ms	remaining: 60.4ms
    49:	learn: 1.7288260	test: 2.3801313	best: 2.3682292 (44)	total: 59.1ms	remaining: 59.1ms
    50:	learn: 1.7001699	test: 2.3729015	best: 2.3682292 (44)	total: 60.1ms	remaining: 57.8ms
    51:	learn: 1.6750114	test: 2.3641206	best: 2.3641206 (51)	total: 61.1ms	remaining: 56.4ms
    52:	learn: 1.6421283	test: 2.3569281	best: 2.3569281 (52)	total: 62.1ms	remaining: 55.1ms
    53:	learn: 1.6325722	test: 2.3553384	best: 2.3553384 (53)	total: 63.1ms	remaining: 53.8ms
    54:	learn: 1.6090244	test: 2.3548386	best: 2.3548386 (54)	total: 64.3ms	remaining: 52.6ms
    55:	learn: 1.5701962	test: 2.3744270	best: 2.3548386 (54)	total: 65.3ms	remaining: 51.3ms
    56:	learn: 1.5617406	test: 2.3716232	best: 2.3548386 (54)	total: 66.3ms	remaining: 50ms
    57:	learn: 1.5486931	test: 2.3581367	best: 2.3548386 (54)	total: 67.7ms	remaining: 49ms
    58:	learn: 1.5430825	test: 2.3560075	best: 2.3548386 (54)	total: 68.9ms	remaining: 47.9ms
    59:	learn: 1.5309401	test: 2.3522971	best: 2.3522971 (59)	total: 70ms	remaining: 46.7ms
    60:	learn: 1.5108160	test: 2.3551672	best: 2.3522971 (59)	total: 71ms	remaining: 45.4ms
    61:	learn: 1.4978680	test: 2.3496973	best: 2.3496973 (61)	total: 71.9ms	remaining: 44.1ms
    62:	learn: 1.4732434	test: 2.3456606	best: 2.3456606 (62)	total: 73ms	remaining: 42.9ms
    63:	learn: 1.4499733	test: 2.3404387	best: 2.3404387 (63)	total: 74ms	remaining: 41.6ms
    64:	learn: 1.4274949	test: 2.3394534	best: 2.3394534 (64)	total: 75.1ms	remaining: 40.4ms
    65:	learn: 1.4237438	test: 2.3373925	best: 2.3373925 (65)	total: 76.1ms	remaining: 39.2ms
    66:	learn: 1.4068771	test: 2.3318249	best: 2.3318249 (66)	total: 77.1ms	remaining: 38ms
    67:	learn: 1.4027376	test: 2.3351932	best: 2.3318249 (66)	total: 78.3ms	remaining: 36.8ms
    68:	learn: 1.3835129	test: 2.3341955	best: 2.3318249 (66)	total: 79.3ms	remaining: 35.6ms
    69:	learn: 1.3630829	test: 2.3198993	best: 2.3198993 (69)	total: 80.4ms	remaining: 34.5ms
    70:	learn: 1.3495881	test: 2.3091087	best: 2.3091087 (70)	total: 81.7ms	remaining: 33.4ms
    71:	learn: 1.3417861	test: 2.3069396	best: 2.3069396 (71)	total: 82.7ms	remaining: 32.2ms
    72:	learn: 1.3350026	test: 2.3053173	best: 2.3053173 (72)	total: 83.8ms	remaining: 31ms
    73:	learn: 1.3247285	test: 2.3049885	best: 2.3049885 (73)	total: 84.8ms	remaining: 29.8ms
    74:	learn: 1.3215894	test: 2.3092888	best: 2.3049885 (73)	total: 86ms	remaining: 28.7ms
    75:	learn: 1.3003578	test: 2.3155923	best: 2.3049885 (73)	total: 87.2ms	remaining: 27.5ms
    76:	learn: 1.2794930	test: 2.3207194	best: 2.3049885 (73)	total: 88.4ms	remaining: 26.4ms
    77:	learn: 1.2755937	test: 2.3266458	best: 2.3049885 (73)	total: 89.4ms	remaining: 25.2ms
    78:	learn: 1.2586163	test: 2.3252313	best: 2.3049885 (73)	total: 90.6ms	remaining: 24.1ms
    79:	learn: 1.2376228	test: 2.3300084	best: 2.3049885 (73)	total: 91.6ms	remaining: 22.9ms
    80:	learn: 1.2258730	test: 2.3325946	best: 2.3049885 (73)	total: 92.9ms	remaining: 21.8ms
    81:	learn: 1.2231983	test: 2.3371061	best: 2.3049885 (73)	total: 94.1ms	remaining: 20.7ms
    82:	learn: 1.2045865	test: 2.3369175	best: 2.3049885 (73)	total: 95.3ms	remaining: 19.5ms
    83:	learn: 1.1967806	test: 2.3362164	best: 2.3049885 (73)	total: 96.3ms	remaining: 18.4ms
    84:	learn: 1.1870849	test: 2.3383054	best: 2.3049885 (73)	total: 97.5ms	remaining: 17.2ms
    85:	learn: 1.1680035	test: 2.3274213	best: 2.3049885 (73)	total: 98.6ms	remaining: 16ms
    86:	learn: 1.1597352	test: 2.3266348	best: 2.3049885 (73)	total: 99.6ms	remaining: 14.9ms
    87:	learn: 1.1570754	test: 2.3300377	best: 2.3049885 (73)	total: 101ms	remaining: 13.7ms
    88:	learn: 1.1539282	test: 2.3291801	best: 2.3049885 (73)	total: 102ms	remaining: 12.6ms
    89:	learn: 1.1365204	test: 2.3360712	best: 2.3049885 (73)	total: 103ms	remaining: 11.4ms
    90:	learn: 1.1339329	test: 2.3406200	best: 2.3049885 (73)	total: 104ms	remaining: 10.3ms
    91:	learn: 1.1236800	test: 2.3424095	best: 2.3049885 (73)	total: 105ms	remaining: 9.14ms
    92:	learn: 1.1217533	test: 2.3459569	best: 2.3049885 (73)	total: 107ms	remaining: 8.02ms
    93:	learn: 1.1200202	test: 2.3504721	best: 2.3049885 (73)	total: 108ms	remaining: 6.88ms
    94:	learn: 1.1104155	test: 2.3634084	best: 2.3049885 (73)	total: 109ms	remaining: 5.72ms
    95:	learn: 1.0928571	test: 2.3680257	best: 2.3049885 (73)	total: 110ms	remaining: 4.57ms
    96:	learn: 1.0905897	test: 2.3660928	best: 2.3049885 (73)	total: 111ms	remaining: 3.42ms
    97:	learn: 1.0750356	test: 2.3737700	best: 2.3049885 (73)	total: 112ms	remaining: 2.28ms
    98:	learn: 1.0649067	test: 2.3729832	best: 2.3049885 (73)	total: 113ms	remaining: 1.14ms
    99:	learn: 1.0472379	test: 2.3686984	best: 2.3049885 (73)	total: 114ms	remaining: 0us
    
    bestTest = 2.304988514
    bestIteration = 73
    
    Shrink model to first 74 iterations.
    Learning rate set to 0.155941
    0:	learn: 7.8212355	test: 10.2837822	best: 10.2837822 (0)	total: 1.25ms	remaining: 124ms
    1:	learn: 7.1455816	test: 9.5443380	best: 9.5443380 (1)	total: 2.23ms	remaining: 110ms
    2:	learn: 6.6591952	test: 9.0894346	best: 9.0894346 (2)	total: 3.21ms	remaining: 104ms
    3:	learn: 6.2046233	test: 8.8348631	best: 8.8348631 (3)	total: 4.18ms	remaining: 100ms
    4:	learn: 5.7243509	test: 8.1658461	best: 8.1658461 (4)	total: 5.32ms	remaining: 101ms
    5:	learn: 5.3407048	test: 7.7023014	best: 7.7023014 (5)	total: 6.35ms	remaining: 99.6ms
    6:	learn: 5.0577353	test: 7.3172858	best: 7.3172858 (6)	total: 7.42ms	remaining: 98.5ms
    7:	learn: 4.7532324	test: 7.0574667	best: 7.0574667 (7)	total: 8.44ms	remaining: 97ms
    8:	learn: 4.4637556	test: 6.8461675	best: 6.8461675 (8)	total: 9.54ms	remaining: 96.5ms
    9:	learn: 4.2009022	test: 6.6185360	best: 6.6185360 (9)	total: 11ms	remaining: 98.6ms
    10:	learn: 3.9549606	test: 6.2847326	best: 6.2847326 (10)	total: 12ms	remaining: 97.5ms
    11:	learn: 3.7786865	test: 6.1451544	best: 6.1451544 (11)	total: 13ms	remaining: 95.3ms
    12:	learn: 3.6224688	test: 5.9732589	best: 5.9732589 (12)	total: 13.9ms	remaining: 93.3ms
    13:	learn: 3.4826429	test: 5.8892805	best: 5.8892805 (13)	total: 15ms	remaining: 92.1ms
    14:	learn: 3.3395237	test: 5.6376823	best: 5.6376823 (14)	total: 16.2ms	remaining: 91.9ms
    15:	learn: 3.2263912	test: 5.6298801	best: 5.6298801 (15)	total: 17.6ms	remaining: 92.4ms
    16:	learn: 3.1087239	test: 5.4637266	best: 5.4637266 (16)	total: 18.6ms	remaining: 91ms
    17:	learn: 3.0274974	test: 5.3556723	best: 5.3556723 (17)	total: 19.7ms	remaining: 89.8ms
    18:	learn: 2.9342088	test: 5.2225675	best: 5.2225675 (18)	total: 20.8ms	remaining: 88.8ms
    19:	learn: 2.8522248	test: 5.1565179	best: 5.1565179 (19)	total: 21.8ms	remaining: 87.3ms
    20:	learn: 2.7772360	test: 5.1097143	best: 5.1097143 (20)	total: 22.9ms	remaining: 86.3ms
    21:	learn: 2.7227675	test: 5.0129232	best: 5.0129232 (21)	total: 24.6ms	remaining: 87.2ms
    22:	learn: 2.6718345	test: 4.9749150	best: 4.9749150 (22)	total: 25.7ms	remaining: 86.1ms
    23:	learn: 2.6194296	test: 4.9354732	best: 4.9354732 (23)	total: 27ms	remaining: 85.3ms
    24:	learn: 2.5436509	test: 4.8371857	best: 4.8371857 (24)	total: 28.1ms	remaining: 84.3ms
    25:	learn: 2.4964726	test: 4.7993462	best: 4.7993462 (25)	total: 29.2ms	remaining: 83.2ms
    26:	learn: 2.4627029	test: 4.7683266	best: 4.7683266 (26)	total: 30.4ms	remaining: 82.2ms
    27:	learn: 2.4301889	test: 4.7073859	best: 4.7073859 (27)	total: 31.7ms	remaining: 81.5ms
    28:	learn: 2.3857968	test: 4.6335171	best: 4.6335171 (28)	total: 33ms	remaining: 80.8ms
    29:	learn: 2.3431332	test: 4.5692383	best: 4.5692383 (29)	total: 34.5ms	remaining: 80.5ms
    30:	learn: 2.3140515	test: 4.5448790	best: 4.5448790 (30)	total: 35.7ms	remaining: 79.5ms
    31:	learn: 2.2690335	test: 4.4893594	best: 4.4893594 (31)	total: 37.2ms	remaining: 79.1ms
    32:	learn: 2.2432778	test: 4.4587154	best: 4.4587154 (32)	total: 38.4ms	remaining: 78ms
    33:	learn: 2.2222307	test: 4.4401502	best: 4.4401502 (33)	total: 39.6ms	remaining: 76.8ms
    34:	learn: 2.1926881	test: 4.4426302	best: 4.4401502 (33)	total: 40.7ms	remaining: 75.6ms
    35:	learn: 2.1504975	test: 4.3631353	best: 4.3631353 (35)	total: 42ms	remaining: 74.7ms
    36:	learn: 2.1338849	test: 4.3528448	best: 4.3528448 (36)	total: 43.2ms	remaining: 73.6ms
    37:	learn: 2.1045547	test: 4.3363573	best: 4.3363573 (37)	total: 44.4ms	remaining: 72.4ms
    38:	learn: 2.0639526	test: 4.2831438	best: 4.2831438 (38)	total: 45.6ms	remaining: 71.3ms
    39:	learn: 2.0409496	test: 4.2727495	best: 4.2727495 (39)	total: 46.8ms	remaining: 70.2ms
    40:	learn: 2.0022422	test: 4.2511410	best: 4.2511410 (40)	total: 47.8ms	remaining: 68.8ms
    41:	learn: 1.9786847	test: 4.2560080	best: 4.2511410 (40)	total: 48.8ms	remaining: 67.4ms
    42:	learn: 1.9657565	test: 4.2481036	best: 4.2481036 (42)	total: 50.3ms	remaining: 66.6ms
    43:	learn: 1.9261245	test: 4.2043694	best: 4.2043694 (43)	total: 51.6ms	remaining: 65.6ms
    44:	learn: 1.9153148	test: 4.2041167	best: 4.2041167 (44)	total: 52.8ms	remaining: 64.6ms
    45:	learn: 1.8772780	test: 4.1442703	best: 4.1442703 (45)	total: 53.9ms	remaining: 63.3ms
    46:	learn: 1.8512170	test: 4.1094259	best: 4.1094259 (46)	total: 54.9ms	remaining: 61.9ms
    47:	learn: 1.8353204	test: 4.1005218	best: 4.1005218 (47)	total: 56ms	remaining: 60.6ms
    48:	learn: 1.8270696	test: 4.0925137	best: 4.0925137 (48)	total: 57.2ms	remaining: 59.6ms
    49:	learn: 1.8181919	test: 4.0879757	best: 4.0879757 (49)	total: 58.2ms	remaining: 58.2ms
    50:	learn: 1.7812060	test: 4.0759924	best: 4.0759924 (50)	total: 59.2ms	remaining: 56.9ms
    51:	learn: 1.7704042	test: 4.0738224	best: 4.0738224 (51)	total: 60.2ms	remaining: 55.6ms
    52:	learn: 1.7494940	test: 4.0606047	best: 4.0606047 (52)	total: 61.2ms	remaining: 54.3ms
    53:	learn: 1.7409727	test: 4.0512249	best: 4.0512249 (53)	total: 62.6ms	remaining: 53.3ms
    54:	learn: 1.7200927	test: 4.0510605	best: 4.0510605 (54)	total: 63.9ms	remaining: 52.3ms
    55:	learn: 1.7106986	test: 4.0369022	best: 4.0369022 (55)	total: 65.2ms	remaining: 51.2ms
    56:	learn: 1.7038552	test: 4.0347742	best: 4.0347742 (56)	total: 66.2ms	remaining: 49.9ms
    57:	learn: 1.6868733	test: 4.0135583	best: 4.0135583 (57)	total: 67.6ms	remaining: 48.9ms
    58:	learn: 1.6645705	test: 4.0112382	best: 4.0112382 (58)	total: 68.6ms	remaining: 47.7ms
    59:	learn: 1.6365175	test: 4.0004424	best: 4.0004424 (59)	total: 69.5ms	remaining: 46.4ms
    60:	learn: 1.6176647	test: 3.9833901	best: 3.9833901 (60)	total: 70.6ms	remaining: 45.1ms
    61:	learn: 1.6131891	test: 3.9830389	best: 3.9830389 (61)	total: 71.6ms	remaining: 43.9ms
    62:	learn: 1.6079962	test: 3.9735012	best: 3.9735012 (62)	total: 72.7ms	remaining: 42.7ms
    63:	learn: 1.6049564	test: 3.9753258	best: 3.9735012 (62)	total: 73.8ms	remaining: 41.5ms
    64:	learn: 1.6025470	test: 3.9749080	best: 3.9735012 (62)	total: 74.8ms	remaining: 40.3ms
    65:	learn: 1.5994183	test: 3.9747024	best: 3.9735012 (62)	total: 76.2ms	remaining: 39.3ms
    66:	learn: 1.5971921	test: 3.9763371	best: 3.9735012 (62)	total: 77.8ms	remaining: 38.3ms
    67:	learn: 1.5779068	test: 3.9671655	best: 3.9671655 (67)	total: 79ms	remaining: 37.2ms
    68:	learn: 1.5753120	test: 3.9669827	best: 3.9669827 (68)	total: 80ms	remaining: 36ms
    69:	learn: 1.5428222	test: 3.9500893	best: 3.9500893 (69)	total: 81ms	remaining: 34.7ms
    70:	learn: 1.5170926	test: 3.9386391	best: 3.9386391 (70)	total: 82ms	remaining: 33.5ms
    71:	learn: 1.5132720	test: 3.9357018	best: 3.9357018 (71)	total: 83.4ms	remaining: 32.4ms
    72:	learn: 1.4921637	test: 3.9358438	best: 3.9357018 (71)	total: 84.4ms	remaining: 31.2ms
    73:	learn: 1.4898436	test: 3.9307120	best: 3.9307120 (73)	total: 85.4ms	remaining: 30ms
    74:	learn: 1.4800043	test: 3.9028266	best: 3.9028266 (74)	total: 86.5ms	remaining: 28.8ms
    75:	learn: 1.4487063	test: 3.9039886	best: 3.9028266 (74)	total: 87.5ms	remaining: 27.6ms
    76:	learn: 1.4305440	test: 3.8981658	best: 3.8981658 (76)	total: 88.8ms	remaining: 26.5ms
    77:	learn: 1.4135045	test: 3.8927829	best: 3.8927829 (77)	total: 90ms	remaining: 25.4ms
    78:	learn: 1.3972037	test: 3.8895404	best: 3.8895404 (78)	total: 91.2ms	remaining: 24.2ms
    79:	learn: 1.3752502	test: 3.8864771	best: 3.8864771 (79)	total: 92.5ms	remaining: 23.1ms
    80:	learn: 1.3532060	test: 3.8716434	best: 3.8716434 (80)	total: 93.8ms	remaining: 22ms
    81:	learn: 1.3334023	test: 3.8676500	best: 3.8676500 (81)	total: 94.9ms	remaining: 20.8ms
    82:	learn: 1.3306536	test: 3.8658545	best: 3.8658545 (82)	total: 95.9ms	remaining: 19.6ms
    83:	learn: 1.3202080	test: 3.8632022	best: 3.8632022 (83)	total: 96.9ms	remaining: 18.5ms
    84:	learn: 1.3053642	test: 3.8693822	best: 3.8632022 (83)	total: 98.2ms	remaining: 17.3ms
    85:	learn: 1.2913476	test: 3.8664203	best: 3.8632022 (83)	total: 99.3ms	remaining: 16.2ms
    86:	learn: 1.2691229	test: 3.8629461	best: 3.8629461 (86)	total: 100ms	remaining: 15ms
    87:	learn: 1.2504869	test: 3.8574556	best: 3.8574556 (87)	total: 101ms	remaining: 13.8ms
    88:	learn: 1.2361647	test: 3.8511723	best: 3.8511723 (88)	total: 103ms	remaining: 12.7ms
    89:	learn: 1.2211698	test: 3.8387968	best: 3.8387968 (89)	total: 104ms	remaining: 11.6ms
    90:	learn: 1.2177029	test: 3.8363678	best: 3.8363678 (90)	total: 105ms	remaining: 10.4ms
    91:	learn: 1.1984496	test: 3.8181906	best: 3.8181906 (91)	total: 106ms	remaining: 9.24ms
    92:	learn: 1.1889200	test: 3.8244016	best: 3.8181906 (91)	total: 107ms	remaining: 8.08ms
    93:	learn: 1.1792312	test: 3.8309826	best: 3.8181906 (91)	total: 109ms	remaining: 6.93ms
    94:	learn: 1.1589859	test: 3.8268381	best: 3.8181906 (91)	total: 110ms	remaining: 5.77ms
    95:	learn: 1.1518166	test: 3.8268043	best: 3.8181906 (91)	total: 111ms	remaining: 4.61ms
    96:	learn: 1.1355294	test: 3.8128264	best: 3.8128264 (96)	total: 112ms	remaining: 3.45ms
    97:	learn: 1.1219173	test: 3.8024263	best: 3.8024263 (97)	total: 113ms	remaining: 2.3ms
    98:	learn: 1.1075518	test: 3.7986872	best: 3.7986872 (98)	total: 114ms	remaining: 1.15ms
    99:	learn: 1.0936281	test: 3.7954393	best: 3.7954393 (99)	total: 115ms	remaining: 0us
    
    bestTest = 3.795439321
    bestIteration = 99
    
    Learning rate set to 0.155941
    0:	learn: 8.3734029	test: 8.1856523	best: 8.1856523 (0)	total: 1.24ms	remaining: 123ms
    1:	learn: 7.6614916	test: 7.4490963	best: 7.4490963 (1)	total: 2.37ms	remaining: 116ms
    2:	learn: 7.1392651	test: 7.0062065	best: 7.0062065 (2)	total: 3.49ms	remaining: 113ms
    3:	learn: 6.6364656	test: 6.4822666	best: 6.4822666 (3)	total: 4.78ms	remaining: 115ms
    4:	learn: 6.0800785	test: 6.0377576	best: 6.0377576 (4)	total: 5.92ms	remaining: 113ms
    5:	learn: 5.6387481	test: 5.6197812	best: 5.6197812 (5)	total: 7.14ms	remaining: 112ms
    6:	learn: 5.3652615	test: 5.3400647	best: 5.3400647 (6)	total: 8.14ms	remaining: 108ms
    7:	learn: 5.0347187	test: 5.0470352	best: 5.0470352 (7)	total: 9.18ms	remaining: 106ms
    8:	learn: 4.7766065	test: 4.8792992	best: 4.8792992 (8)	total: 10.2ms	remaining: 104ms
    9:	learn: 4.5045848	test: 4.7062043	best: 4.7062043 (9)	total: 11.2ms	remaining: 101ms
    10:	learn: 4.1853879	test: 4.4215418	best: 4.4215418 (10)	total: 12.2ms	remaining: 98.7ms
    11:	learn: 3.9487373	test: 4.2188327	best: 4.2188327 (11)	total: 13.3ms	remaining: 97.5ms
    12:	learn: 3.7531321	test: 4.0811673	best: 4.0811673 (12)	total: 14.3ms	remaining: 95.6ms
    13:	learn: 3.5478259	test: 3.9791251	best: 3.9791251 (13)	total: 15.3ms	remaining: 94.2ms
    14:	learn: 3.3662586	test: 3.8128824	best: 3.8128824 (14)	total: 16.4ms	remaining: 92.8ms
    15:	learn: 3.2494402	test: 3.7216182	best: 3.7216182 (15)	total: 17.7ms	remaining: 93ms
    16:	learn: 3.1405534	test: 3.6009875	best: 3.6009875 (16)	total: 18.9ms	remaining: 92.1ms
    17:	learn: 3.0476664	test: 3.5065638	best: 3.5065638 (17)	total: 20ms	remaining: 90.9ms
    18:	learn: 2.9466416	test: 3.4784981	best: 3.4784981 (18)	total: 20.9ms	remaining: 89.3ms
    19:	learn: 2.8738515	test: 3.4532413	best: 3.4532413 (19)	total: 22ms	remaining: 88.1ms
    20:	learn: 2.8011074	test: 3.3970537	best: 3.3970537 (20)	total: 23ms	remaining: 86.6ms
    21:	learn: 2.7172132	test: 3.3125533	best: 3.3125533 (21)	total: 24ms	remaining: 85.2ms
    22:	learn: 2.6458578	test: 3.2570826	best: 3.2570826 (22)	total: 25.6ms	remaining: 85.6ms
    23:	learn: 2.5780469	test: 3.2107969	best: 3.2107969 (23)	total: 26.8ms	remaining: 84.9ms
    24:	learn: 2.5074584	test: 3.1770923	best: 3.1770923 (24)	total: 28ms	remaining: 84ms
    25:	learn: 2.4692393	test: 3.1489853	best: 3.1489853 (25)	total: 29ms	remaining: 82.5ms
    26:	learn: 2.4192042	test: 3.1114222	best: 3.1114222 (26)	total: 30.5ms	remaining: 82.4ms
    27:	learn: 2.3650279	test: 3.0428160	best: 3.0428160 (27)	total: 31.7ms	remaining: 81.5ms
    28:	learn: 2.3173907	test: 3.0012292	best: 3.0012292 (28)	total: 32.8ms	remaining: 80.3ms
    29:	learn: 2.2788484	test: 2.9799536	best: 2.9799536 (29)	total: 34ms	remaining: 79.4ms
    30:	learn: 2.2406789	test: 2.9645093	best: 2.9645093 (30)	total: 35.2ms	remaining: 78.3ms
    31:	learn: 2.2099612	test: 2.9488633	best: 2.9488633 (31)	total: 36.3ms	remaining: 77.2ms
    32:	learn: 2.1675950	test: 2.9212544	best: 2.9212544 (32)	total: 37.5ms	remaining: 76.2ms
    33:	learn: 2.1388426	test: 2.8955824	best: 2.8955824 (33)	total: 38.9ms	remaining: 75.4ms
    34:	learn: 2.1100133	test: 2.8673729	best: 2.8673729 (34)	total: 39.9ms	remaining: 74.2ms
    35:	learn: 2.0859223	test: 2.8604653	best: 2.8604653 (35)	total: 41.1ms	remaining: 73ms
    36:	learn: 2.0449572	test: 2.8492152	best: 2.8492152 (36)	total: 42.1ms	remaining: 71.6ms
    37:	learn: 2.0234987	test: 2.8467093	best: 2.8467093 (37)	total: 43.5ms	remaining: 70.9ms
    38:	learn: 2.0074208	test: 2.8418010	best: 2.8418010 (38)	total: 44.6ms	remaining: 69.7ms
    39:	learn: 1.9767355	test: 2.8086279	best: 2.8086279 (39)	total: 45.6ms	remaining: 68.4ms
    40:	learn: 1.9480297	test: 2.7996332	best: 2.7996332 (40)	total: 46.6ms	remaining: 67.1ms
    41:	learn: 1.9173726	test: 2.7819704	best: 2.7819704 (41)	total: 47.6ms	remaining: 65.8ms
    42:	learn: 1.9002185	test: 2.7736219	best: 2.7736219 (42)	total: 48.6ms	remaining: 64.4ms
    43:	learn: 1.8758630	test: 2.7672627	best: 2.7672627 (43)	total: 49.7ms	remaining: 63.2ms
    44:	learn: 1.8413776	test: 2.7582186	best: 2.7582186 (44)	total: 50.8ms	remaining: 62ms
    45:	learn: 1.8055637	test: 2.7476517	best: 2.7476517 (45)	total: 51.8ms	remaining: 60.8ms
    46:	learn: 1.7743573	test: 2.7245862	best: 2.7245862 (46)	total: 52.8ms	remaining: 59.6ms
    47:	learn: 1.7422915	test: 2.7485205	best: 2.7245862 (46)	total: 54ms	remaining: 58.5ms
    48:	learn: 1.7143840	test: 2.7387247	best: 2.7245862 (46)	total: 55.1ms	remaining: 57.3ms
    49:	learn: 1.6952626	test: 2.7252511	best: 2.7245862 (46)	total: 56.2ms	remaining: 56.2ms
    50:	learn: 1.6817227	test: 2.7154535	best: 2.7154535 (50)	total: 57.4ms	remaining: 55.1ms
    51:	learn: 1.6644569	test: 2.7063723	best: 2.7063723 (51)	total: 58.4ms	remaining: 53.9ms
    52:	learn: 1.6454197	test: 2.6986973	best: 2.6986973 (52)	total: 59.4ms	remaining: 52.7ms
    53:	learn: 1.6355167	test: 2.6955233	best: 2.6955233 (53)	total: 60.4ms	remaining: 51.5ms
    54:	learn: 1.6157110	test: 2.6949089	best: 2.6949089 (54)	total: 61.5ms	remaining: 50.3ms
    55:	learn: 1.5858854	test: 2.6974701	best: 2.6949089 (54)	total: 62.5ms	remaining: 49.1ms
    56:	learn: 1.5794915	test: 2.6947552	best: 2.6947552 (56)	total: 63.5ms	remaining: 47.9ms
    57:	learn: 1.5725090	test: 2.6891428	best: 2.6891428 (57)	total: 64.4ms	remaining: 46.7ms
    58:	learn: 1.5449652	test: 2.6863153	best: 2.6863153 (58)	total: 65.5ms	remaining: 45.5ms
    59:	learn: 1.5343936	test: 2.6799073	best: 2.6799073 (59)	total: 66.6ms	remaining: 44.4ms
    60:	learn: 1.5120456	test: 2.6779568	best: 2.6779568 (60)	total: 67.6ms	remaining: 43.2ms
    61:	learn: 1.4911639	test: 2.6696612	best: 2.6696612 (61)	total: 68.6ms	remaining: 42ms
    62:	learn: 1.4823625	test: 2.6683482	best: 2.6683482 (62)	total: 69.7ms	remaining: 40.9ms
    63:	learn: 1.4619851	test: 2.6612379	best: 2.6612379 (63)	total: 71ms	remaining: 39.9ms
    64:	learn: 1.4493669	test: 2.6514391	best: 2.6514391 (64)	total: 72ms	remaining: 38.8ms
    65:	learn: 1.4279950	test: 2.6450191	best: 2.6450191 (65)	total: 73.1ms	remaining: 37.7ms
    66:	learn: 1.4040802	test: 2.6479416	best: 2.6450191 (65)	total: 74.1ms	remaining: 36.5ms
    67:	learn: 1.3855564	test: 2.6500010	best: 2.6450191 (65)	total: 75.1ms	remaining: 35.3ms
    68:	learn: 1.3639592	test: 2.6551335	best: 2.6450191 (65)	total: 76.1ms	remaining: 34.2ms
    69:	learn: 1.3433657	test: 2.6540411	best: 2.6450191 (65)	total: 77.1ms	remaining: 33.1ms
    70:	learn: 1.3311546	test: 2.6533225	best: 2.6450191 (65)	total: 78.1ms	remaining: 31.9ms
    71:	learn: 1.3095752	test: 2.6499689	best: 2.6450191 (65)	total: 79.3ms	remaining: 30.8ms
    72:	learn: 1.2925997	test: 2.6516049	best: 2.6450191 (65)	total: 80.5ms	remaining: 29.8ms
    73:	learn: 1.2859607	test: 2.6557272	best: 2.6450191 (65)	total: 81.7ms	remaining: 28.7ms
    74:	learn: 1.2819687	test: 2.6560655	best: 2.6450191 (65)	total: 83ms	remaining: 27.7ms
    75:	learn: 1.2630629	test: 2.6610887	best: 2.6450191 (65)	total: 84.3ms	remaining: 26.6ms
    76:	learn: 1.2494605	test: 2.6661682	best: 2.6450191 (65)	total: 85.4ms	remaining: 25.5ms
    77:	learn: 1.2378876	test: 2.6629842	best: 2.6450191 (65)	total: 86.4ms	remaining: 24.4ms
    78:	learn: 1.2169818	test: 2.6622002	best: 2.6450191 (65)	total: 87.4ms	remaining: 23.2ms
    79:	learn: 1.2116866	test: 2.6601755	best: 2.6450191 (65)	total: 88.4ms	remaining: 22.1ms
    80:	learn: 1.2050194	test: 2.6597828	best: 2.6450191 (65)	total: 89.3ms	remaining: 21ms
    81:	learn: 1.1878767	test: 2.6580323	best: 2.6450191 (65)	total: 90.4ms	remaining: 19.8ms
    82:	learn: 1.1703230	test: 2.6633719	best: 2.6450191 (65)	total: 91.7ms	remaining: 18.8ms
    83:	learn: 1.1611703	test: 2.6626838	best: 2.6450191 (65)	total: 92.7ms	remaining: 17.7ms
    84:	learn: 1.1472965	test: 2.6571923	best: 2.6450191 (65)	total: 93.9ms	remaining: 16.6ms
    85:	learn: 1.1327201	test: 2.6550538	best: 2.6450191 (65)	total: 94.8ms	remaining: 15.4ms
    86:	learn: 1.1264753	test: 2.6555497	best: 2.6450191 (65)	total: 95.8ms	remaining: 14.3ms
    87:	learn: 1.1153620	test: 2.6559891	best: 2.6450191 (65)	total: 97.2ms	remaining: 13.3ms
    88:	learn: 1.1073554	test: 2.6516938	best: 2.6450191 (65)	total: 98.2ms	remaining: 12.1ms
    89:	learn: 1.0873129	test: 2.6457019	best: 2.6450191 (65)	total: 99.2ms	remaining: 11ms
    90:	learn: 1.0660577	test: 2.6442240	best: 2.6442240 (90)	total: 100ms	remaining: 9.91ms
    91:	learn: 1.0599863	test: 2.6407551	best: 2.6407551 (91)	total: 101ms	remaining: 8.8ms
    92:	learn: 1.0434908	test: 2.6430624	best: 2.6407551 (91)	total: 102ms	remaining: 7.69ms
    93:	learn: 1.0278178	test: 2.6459430	best: 2.6407551 (91)	total: 103ms	remaining: 6.59ms
    94:	learn: 1.0171837	test: 2.6452223	best: 2.6407551 (91)	total: 104ms	remaining: 5.49ms
    95:	learn: 1.0146722	test: 2.6430009	best: 2.6407551 (91)	total: 105ms	remaining: 4.39ms
    96:	learn: 1.0049220	test: 2.6395795	best: 2.6395795 (96)	total: 107ms	remaining: 3.3ms
    97:	learn: 0.9953567	test: 2.6373564	best: 2.6373564 (97)	total: 108ms	remaining: 2.2ms
    98:	learn: 0.9850713	test: 2.6275466	best: 2.6275466 (98)	total: 109ms	remaining: 1.1ms
    99:	learn: 0.9748952	test: 2.6284603	best: 2.6275466 (98)	total: 110ms	remaining: 0us
    
    bestTest = 2.627546562
    bestIteration = 98
    
    Shrink model to first 99 iterations.
    Learning rate set to 0.155941
    0:	learn: 8.4333678	test: 7.8515174	best: 7.8515174 (0)	total: 1.21ms	remaining: 120ms
    1:	learn: 7.6731009	test: 7.2458578	best: 7.2458578 (1)	total: 2.23ms	remaining: 109ms
    2:	learn: 7.1292557	test: 6.9182152	best: 6.9182152 (2)	total: 3.24ms	remaining: 105ms
    3:	learn: 6.6147246	test: 6.5822497	best: 6.5822497 (3)	total: 4.23ms	remaining: 102ms
    4:	learn: 6.0802804	test: 6.2601489	best: 6.2601489 (4)	total: 5.4ms	remaining: 103ms
    5:	learn: 5.6843731	test: 6.0064290	best: 6.0064290 (5)	total: 6.46ms	remaining: 101ms
    6:	learn: 5.3664034	test: 5.7372563	best: 5.7372563 (6)	total: 7.54ms	remaining: 100ms
    7:	learn: 5.0001771	test: 5.5090252	best: 5.5090252 (7)	total: 8.54ms	remaining: 98.3ms
    8:	learn: 4.6792280	test: 5.3697140	best: 5.3697140 (8)	total: 9.59ms	remaining: 97ms
    9:	learn: 4.4136814	test: 5.2135001	best: 5.2135001 (9)	total: 10.6ms	remaining: 95.6ms
    10:	learn: 4.1409593	test: 4.9708617	best: 4.9708617 (10)	total: 11.7ms	remaining: 94.8ms
    11:	learn: 3.9181166	test: 4.8675239	best: 4.8675239 (11)	total: 12.7ms	remaining: 93.2ms
    12:	learn: 3.7418786	test: 4.6732045	best: 4.6732045 (12)	total: 13.8ms	remaining: 92ms
    13:	learn: 3.6074063	test: 4.5828546	best: 4.5828546 (13)	total: 14.8ms	remaining: 90.7ms
    14:	learn: 3.4467865	test: 4.4322613	best: 4.4322613 (14)	total: 15.8ms	remaining: 89.4ms
    15:	learn: 3.3234062	test: 4.3442932	best: 4.3442932 (15)	total: 16.8ms	remaining: 88.3ms
    16:	learn: 3.1954922	test: 4.2630430	best: 4.2630430 (16)	total: 17.8ms	remaining: 87.1ms
    17:	learn: 3.0965742	test: 4.2388168	best: 4.2388168 (17)	total: 19.2ms	remaining: 87.6ms
    18:	learn: 2.9991072	test: 4.1877256	best: 4.1877256 (18)	total: 20.3ms	remaining: 86.5ms
    19:	learn: 2.9203781	test: 4.1700065	best: 4.1700065 (19)	total: 21.3ms	remaining: 85.3ms
    20:	learn: 2.8551612	test: 4.1439107	best: 4.1439107 (20)	total: 22.4ms	remaining: 84.2ms
    21:	learn: 2.7645008	test: 4.0998949	best: 4.0998949 (21)	total: 23.4ms	remaining: 82.9ms
    22:	learn: 2.6913884	test: 4.0898862	best: 4.0898862 (22)	total: 24.4ms	remaining: 81.8ms
    23:	learn: 2.6276841	test: 4.0816147	best: 4.0816147 (23)	total: 25.5ms	remaining: 80.7ms
    24:	learn: 2.5716639	test: 4.0567413	best: 4.0567413 (24)	total: 26.4ms	remaining: 79.3ms
    25:	learn: 2.5329513	test: 4.0170641	best: 4.0170641 (25)	total: 27.4ms	remaining: 78.1ms
    26:	learn: 2.5201588	test: 3.9868768	best: 3.9868768 (26)	total: 28.5ms	remaining: 77ms
    27:	learn: 2.4559987	test: 3.9403178	best: 3.9403178 (27)	total: 29.5ms	remaining: 75.9ms
    28:	learn: 2.3917150	test: 3.9456839	best: 3.9403178 (27)	total: 30.5ms	remaining: 74.7ms
    29:	learn: 2.3485406	test: 3.8967932	best: 3.8967932 (29)	total: 31.5ms	remaining: 73.5ms
    30:	learn: 2.3015346	test: 3.8423890	best: 3.8423890 (30)	total: 33ms	remaining: 73.4ms
    31:	learn: 2.2578405	test: 3.8249421	best: 3.8249421 (31)	total: 34.1ms	remaining: 72.5ms
    32:	learn: 2.2323248	test: 3.8304150	best: 3.8249421 (31)	total: 35.2ms	remaining: 71.5ms
    33:	learn: 2.2116737	test: 3.8257741	best: 3.8249421 (31)	total: 36.2ms	remaining: 70.3ms
    34:	learn: 2.1897204	test: 3.7966408	best: 3.7966408 (34)	total: 37.3ms	remaining: 69.4ms
    35:	learn: 2.1505170	test: 3.7614878	best: 3.7614878 (35)	total: 38.7ms	remaining: 68.8ms
    36:	learn: 2.1195713	test: 3.7451538	best: 3.7451538 (36)	total: 39.8ms	remaining: 67.8ms
    37:	learn: 2.0834830	test: 3.7316540	best: 3.7316540 (37)	total: 41ms	remaining: 66.8ms
    38:	learn: 2.0722034	test: 3.7084441	best: 3.7084441 (38)	total: 42.1ms	remaining: 65.9ms
    39:	learn: 2.0332811	test: 3.6947853	best: 3.6947853 (39)	total: 43.2ms	remaining: 64.8ms
    40:	learn: 2.0103660	test: 3.6652446	best: 3.6652446 (40)	total: 44.5ms	remaining: 64.1ms
    41:	learn: 1.9806869	test: 3.6575713	best: 3.6575713 (41)	total: 46.1ms	remaining: 63.7ms
    42:	learn: 1.9715917	test: 3.6440331	best: 3.6440331 (42)	total: 47.3ms	remaining: 62.7ms
    43:	learn: 1.9395715	test: 3.6109398	best: 3.6109398 (43)	total: 48.4ms	remaining: 61.7ms
    44:	learn: 1.9227446	test: 3.6035540	best: 3.6035540 (44)	total: 49.7ms	remaining: 60.8ms
    45:	learn: 1.8988229	test: 3.5970539	best: 3.5970539 (45)	total: 51ms	remaining: 59.8ms
    46:	learn: 1.8900840	test: 3.5867002	best: 3.5867002 (46)	total: 52.2ms	remaining: 58.8ms
    47:	learn: 1.8793617	test: 3.5869486	best: 3.5867002 (46)	total: 53.4ms	remaining: 57.8ms
    48:	learn: 1.8646181	test: 3.5673599	best: 3.5673599 (48)	total: 54.7ms	remaining: 56.9ms
    49:	learn: 1.8390802	test: 3.5400992	best: 3.5400992 (49)	total: 56ms	remaining: 56ms
    50:	learn: 1.8242453	test: 3.5261640	best: 3.5261640 (50)	total: 57.2ms	remaining: 55ms
    51:	learn: 1.7945320	test: 3.5236057	best: 3.5236057 (51)	total: 59ms	remaining: 54.5ms
    52:	learn: 1.7531788	test: 3.5096903	best: 3.5096903 (52)	total: 60.2ms	remaining: 53.4ms
    53:	learn: 1.7378541	test: 3.4972236	best: 3.4972236 (53)	total: 61.3ms	remaining: 52.2ms
    54:	learn: 1.7081705	test: 3.4916839	best: 3.4916839 (54)	total: 62.4ms	remaining: 51.1ms
    55:	learn: 1.6998347	test: 3.4946514	best: 3.4916839 (54)	total: 63.4ms	remaining: 49.8ms
    56:	learn: 1.6928310	test: 3.4866447	best: 3.4866447 (56)	total: 64.3ms	remaining: 48.5ms
    57:	learn: 1.6633983	test: 3.4809980	best: 3.4809980 (57)	total: 65.4ms	remaining: 47.3ms
    58:	learn: 1.6447065	test: 3.4830322	best: 3.4809980 (57)	total: 66.4ms	remaining: 46.2ms
    59:	learn: 1.6364712	test: 3.4716685	best: 3.4716685 (59)	total: 67.4ms	remaining: 45ms
    60:	learn: 1.6297667	test: 3.4608522	best: 3.4608522 (60)	total: 68.5ms	remaining: 43.8ms
    61:	learn: 1.6057678	test: 3.4579768	best: 3.4579768 (61)	total: 69.8ms	remaining: 42.8ms
    62:	learn: 1.5971392	test: 3.4484439	best: 3.4484439 (62)	total: 71ms	remaining: 41.7ms
    63:	learn: 1.5917546	test: 3.4390830	best: 3.4390830 (63)	total: 72.6ms	remaining: 40.9ms
    64:	learn: 1.5659873	test: 3.4352768	best: 3.4352768 (64)	total: 74.6ms	remaining: 40.2ms
    65:	learn: 1.5609367	test: 3.4242088	best: 3.4242088 (65)	total: 76.2ms	remaining: 39.3ms
    66:	learn: 1.5570047	test: 3.4188364	best: 3.4188364 (66)	total: 77.4ms	remaining: 38.1ms
    67:	learn: 1.5271238	test: 3.4151254	best: 3.4151254 (67)	total: 78.8ms	remaining: 37.1ms
    68:	learn: 1.5227400	test: 3.4071228	best: 3.4071228 (68)	total: 79.9ms	remaining: 35.9ms
    69:	learn: 1.5174258	test: 3.3978196	best: 3.3978196 (69)	total: 81ms	remaining: 34.7ms
    70:	learn: 1.4917578	test: 3.3891057	best: 3.3891057 (70)	total: 82.3ms	remaining: 33.6ms
    71:	learn: 1.4889689	test: 3.3840754	best: 3.3840754 (71)	total: 83.4ms	remaining: 32.4ms
    72:	learn: 1.4695334	test: 3.3818907	best: 3.3818907 (72)	total: 84.6ms	remaining: 31.3ms
    73:	learn: 1.4626118	test: 3.3747740	best: 3.3747740 (73)	total: 86ms	remaining: 30.2ms
    74:	learn: 1.4593711	test: 3.3665211	best: 3.3665211 (74)	total: 87.3ms	remaining: 29.1ms
    75:	learn: 1.4366302	test: 3.3594751	best: 3.3594751 (75)	total: 88.4ms	remaining: 27.9ms
    76:	learn: 1.4241004	test: 3.3568586	best: 3.3568586 (76)	total: 89.6ms	remaining: 26.8ms
    77:	learn: 1.4086568	test: 3.3522465	best: 3.3522465 (77)	total: 90.7ms	remaining: 25.6ms
    78:	learn: 1.3865016	test: 3.3509410	best: 3.3509410 (78)	total: 91.9ms	remaining: 24.4ms
    79:	learn: 1.3656521	test: 3.3570333	best: 3.3509410 (78)	total: 93ms	remaining: 23.3ms
    80:	learn: 1.3460045	test: 3.3531646	best: 3.3509410 (78)	total: 94.2ms	remaining: 22.1ms
    81:	learn: 1.3352566	test: 3.3551432	best: 3.3509410 (78)	total: 95.4ms	remaining: 20.9ms
    82:	learn: 1.3306059	test: 3.3475682	best: 3.3475682 (82)	total: 96.5ms	remaining: 19.8ms
    83:	learn: 1.3232719	test: 3.3549173	best: 3.3475682 (82)	total: 97.8ms	remaining: 18.6ms
    84:	learn: 1.3207063	test: 3.3530047	best: 3.3475682 (82)	total: 99.2ms	remaining: 17.5ms
    85:	learn: 1.3001349	test: 3.3452308	best: 3.3452308 (85)	total: 100ms	remaining: 16.3ms
    86:	learn: 1.2835262	test: 3.3488615	best: 3.3452308 (85)	total: 102ms	remaining: 15.2ms
    87:	learn: 1.2729235	test: 3.3437179	best: 3.3437179 (87)	total: 103ms	remaining: 14ms
    88:	learn: 1.2547002	test: 3.3388591	best: 3.3388591 (88)	total: 104ms	remaining: 12.9ms
    89:	learn: 1.2418457	test: 3.3335543	best: 3.3335543 (89)	total: 106ms	remaining: 11.7ms
    90:	learn: 1.2354081	test: 3.3296213	best: 3.3296213 (90)	total: 107ms	remaining: 10.6ms
    91:	learn: 1.2158897	test: 3.3248071	best: 3.3248071 (91)	total: 108ms	remaining: 9.39ms
    92:	learn: 1.2027252	test: 3.3236051	best: 3.3236051 (92)	total: 109ms	remaining: 8.22ms
    93:	learn: 1.1862192	test: 3.3268962	best: 3.3236051 (92)	total: 110ms	remaining: 7.04ms
    94:	learn: 1.1828928	test: 3.3223289	best: 3.3223289 (94)	total: 112ms	remaining: 5.91ms
    95:	learn: 1.1800463	test: 3.3182213	best: 3.3182213 (95)	total: 114ms	remaining: 4.73ms
    96:	learn: 1.1624960	test: 3.3196857	best: 3.3182213 (95)	total: 115ms	remaining: 3.56ms
    97:	learn: 1.1606573	test: 3.3154823	best: 3.3154823 (97)	total: 116ms	remaining: 2.38ms
    98:	learn: 1.1586399	test: 3.3132432	best: 3.3132432 (98)	total: 118ms	remaining: 1.19ms
    99:	learn: 1.1433343	test: 3.3177311	best: 3.3132432 (98)	total: 119ms	remaining: 0us
    
    bestTest = 3.313243227
    bestIteration = 98
    
    Shrink model to first 99 iterations.
    


```python
sub_cat = pd.DataFrame()
sub_cat['target'] = sub_preds_catboost
```

## LGB


```python
# lgm 이용
print("lightgbm")
sub_preds_lgm = np.zeros(test.shape[0])
oof_preds_lgm = np.zeros(train.shape[0])

for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)) :
    trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]
    val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]
    params = {'learning_rate': 0.1, 
              'boosting': 'gbdt', 
              'objective': 'regression', 
              'metric': 'rmse', 
              'seed':2018}

    train_T = lgb.Dataset(trn_x, label=trn_y)
    val_T = lgb.Dataset(val_x, label=val_y)
    
    model1 = lgb.train(params, train_T, 1000, val_T, verbose_eval=100, early_stopping_rounds=100)
    
    sub_preds_lgm += model1.predict(x_test) / folds.n_splits
    oof_preds_lgm[val_idx] = model1.predict(val_x)
```

    lightgbm
    Training until validation scores don't improve for 100 rounds
    [100]	valid_0's rmse: 2.96728
    [200]	valid_0's rmse: 2.99043
    Early stopping, best iteration is:
    [125]	valid_0's rmse: 2.93686
    Training until validation scores don't improve for 100 rounds
    [100]	valid_0's rmse: 3.02056
    Early stopping, best iteration is:
    [23]	valid_0's rmse: 2.93885
    Training until validation scores don't improve for 100 rounds
    [100]	valid_0's rmse: 3.83144
    [200]	valid_0's rmse: 3.521
    [300]	valid_0's rmse: 3.47405
    [400]	valid_0's rmse: 3.43671
    [500]	valid_0's rmse: 3.42742
    [600]	valid_0's rmse: 3.43325
    Early stopping, best iteration is:
    [549]	valid_0's rmse: 3.41832
    Training until validation scores don't improve for 100 rounds
    [100]	valid_0's rmse: 2.8655
    [200]	valid_0's rmse: 2.86034
    [300]	valid_0's rmse: 2.85072
    Early stopping, best iteration is:
    [287]	valid_0's rmse: 2.84571
    Training until validation scores don't improve for 100 rounds
    [100]	valid_0's rmse: 4.29462
    [200]	valid_0's rmse: 4.08035
    [300]	valid_0's rmse: 4.0098
    [400]	valid_0's rmse: 3.98759
    [500]	valid_0's rmse: 3.97659
    [600]	valid_0's rmse: 3.97302
    [700]	valid_0's rmse: 3.96509
    [800]	valid_0's rmse: 3.95226
    [900]	valid_0's rmse: 3.9436
    [1000]	valid_0's rmse: 3.93562
    Did not meet early stopping. Best iteration is:
    [1000]	valid_0's rmse: 3.93562
    


```python
sub_lgm = pd.DataFrame()
sub_lgm['target'] = sub_preds_lgm
```

## XGB


```python
# xgboost
print("xgboost")

sub_preds_xgb = np.zeros(test.shape[0])
oof_preds_xgb = np.zeros(train.shape[0])

for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)) :
    trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]
    val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]
    
    params = {'objective': 'reg:squarederror', 
          'eval_metric': 'rmse',
          'eta': 0.005,
          'random_state': 42,}

    tr_data = xgb.DMatrix(trn_x, trn_y)
    va_data = xgb.DMatrix(val_x, val_y)
    
    watchlist = [(tr_data, 'train'), (va_data, 'valid')]
    
    model_xgb = xgb.train(params, tr_data, 2100, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)

    test_data = xgb.DMatrix(x_test)
    
    sub_preds_xgb += model_xgb.predict(test_data) / folds.n_splits
    oof_preds_xgb[val_idx] = model_xgb.predict(va_data)
    
```

    xgboost
    [0]	train-rmse:25.62559	valid-rmse:24.53293
    Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.
    
    Will train until valid-rmse hasn't improved in 100 rounds.
    [100]	train-rmse:15.97046	valid-rmse:15.37678
    [200]	train-rmse:10.09333	valid-rmse:9.90591
    [300]	train-rmse:6.48550	valid-rmse:6.75688
    [400]	train-rmse:4.26069	valid-rmse:4.95043
    [500]	train-rmse:2.88187	valid-rmse:4.00059
    [600]	train-rmse:2.03611	valid-rmse:3.48958
    [700]	train-rmse:1.51692	valid-rmse:3.20968
    [800]	train-rmse:1.19590	valid-rmse:3.05873
    [900]	train-rmse:0.98532	valid-rmse:2.96147
    [1000]	train-rmse:0.85880	valid-rmse:2.92281
    [1100]	train-rmse:0.75335	valid-rmse:2.89973
    [1200]	train-rmse:0.66702	valid-rmse:2.88654
    [1300]	train-rmse:0.60336	valid-rmse:2.87538
    [1400]	train-rmse:0.55235	valid-rmse:2.86186
    [1500]	train-rmse:0.50164	valid-rmse:2.85670
    [1600]	train-rmse:0.46641	valid-rmse:2.85230
    [1700]	train-rmse:0.42941	valid-rmse:2.85010
    [1800]	train-rmse:0.40253	valid-rmse:2.84578
    [1900]	train-rmse:0.37200	valid-rmse:2.84301
    [2000]	train-rmse:0.34574	valid-rmse:2.83660
    Stopping. Best iteration:
    [1976]	train-rmse:0.35340	valid-rmse:2.83568
    
    [0]	train-rmse:25.72884	valid-rmse:24.09380
    Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.
    
    Will train until valid-rmse hasn't improved in 100 rounds.
    [100]	train-rmse:16.04939	valid-rmse:14.87285
    [200]	train-rmse:10.15260	valid-rmse:9.32030
    [300]	train-rmse:6.53107	valid-rmse:6.01757
    [400]	train-rmse:4.28364	valid-rmse:4.12958
    [500]	train-rmse:2.89359	valid-rmse:3.13605
    [600]	train-rmse:2.03626	valid-rmse:2.67704
    [700]	train-rmse:1.50299	valid-rmse:2.49754
    [800]	train-rmse:1.17196	valid-rmse:2.43197
    [900]	train-rmse:0.97504	valid-rmse:2.42319
    [1000]	train-rmse:0.83620	valid-rmse:2.42060
    Stopping. Best iteration:
    [967]	train-rmse:0.87687	valid-rmse:2.41930
    
    [0]	train-rmse:24.83555	valid-rmse:27.59550
    Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.
    
    Will train until valid-rmse hasn't improved in 100 rounds.
    [100]	train-rmse:15.51037	valid-rmse:17.75444
    [200]	train-rmse:9.84336	valid-rmse:11.84530
    [300]	train-rmse:6.36821	valid-rmse:8.37494
    [400]	train-rmse:4.22326	valid-rmse:6.49130
    [500]	train-rmse:2.88244	valid-rmse:5.45230
    [600]	train-rmse:2.07979	valid-rmse:4.93177
    [700]	train-rmse:1.56658	valid-rmse:4.56008
    [800]	train-rmse:1.23600	valid-rmse:4.34263
    [900]	train-rmse:1.02492	valid-rmse:4.21472
    [1000]	train-rmse:0.88714	valid-rmse:4.12284
    [1100]	train-rmse:0.78417	valid-rmse:4.06146
    [1200]	train-rmse:0.70511	valid-rmse:4.03553
    [1300]	train-rmse:0.64462	valid-rmse:4.02313
    [1400]	train-rmse:0.59662	valid-rmse:4.00488
    [1500]	train-rmse:0.55313	valid-rmse:3.99189
    [1600]	train-rmse:0.51869	valid-rmse:3.98544
    [1700]	train-rmse:0.47966	valid-rmse:3.98242
    [1800]	train-rmse:0.44028	valid-rmse:3.97688
    [1900]	train-rmse:0.40209	valid-rmse:3.97171
    [2000]	train-rmse:0.36487	valid-rmse:3.97428
    Stopping. Best iteration:
    [1928]	train-rmse:0.39092	valid-rmse:3.97093
    
    [0]	train-rmse:25.39125	valid-rmse:25.48413
    Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.
    
    Will train until valid-rmse hasn't improved in 100 rounds.
    [100]	train-rmse:15.84253	valid-rmse:15.80519
    [200]	train-rmse:10.01553	valid-rmse:10.05498
    [300]	train-rmse:6.43286	valid-rmse:6.65008
    [400]	train-rmse:4.22064	valid-rmse:4.75744
    [500]	train-rmse:2.85402	valid-rmse:3.71064
    [600]	train-rmse:2.00232	valid-rmse:3.17857
    [700]	train-rmse:1.47965	valid-rmse:2.93764
    [800]	train-rmse:1.13805	valid-rmse:2.84571
    [900]	train-rmse:0.92953	valid-rmse:2.81928
    [1000]	train-rmse:0.79547	valid-rmse:2.81174
    [1100]	train-rmse:0.69870	valid-rmse:2.79501
    [1200]	train-rmse:0.62061	valid-rmse:2.77773
    [1300]	train-rmse:0.55606	valid-rmse:2.76884
    [1400]	train-rmse:0.49406	valid-rmse:2.76247
    [1500]	train-rmse:0.44954	valid-rmse:2.75486
    [1600]	train-rmse:0.39850	valid-rmse:2.74965
    [1700]	train-rmse:0.35482	valid-rmse:2.74507
    [1800]	train-rmse:0.32178	valid-rmse:2.74147
    [1900]	train-rmse:0.29745	valid-rmse:2.73722
    [2000]	train-rmse:0.27485	valid-rmse:2.73393
    [2099]	train-rmse:0.25204	valid-rmse:2.73060
    [0]	train-rmse:25.46225	valid-rmse:25.20177
    Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.
    
    Will train until valid-rmse hasn't improved in 100 rounds.
    [100]	train-rmse:15.87931	valid-rmse:15.65378
    [200]	train-rmse:10.04362	valid-rmse:9.98427
    [300]	train-rmse:6.47166	valid-rmse:6.73372
    [400]	train-rmse:4.26296	valid-rmse:5.08463
    [500]	train-rmse:2.90593	valid-rmse:4.31514
    [600]	train-rmse:2.07038	valid-rmse:3.94839
    [700]	train-rmse:1.55961	valid-rmse:3.81635
    [800]	train-rmse:1.23850	valid-rmse:3.74325
    [900]	train-rmse:1.03724	valid-rmse:3.72340
    [1000]	train-rmse:0.90288	valid-rmse:3.72539
    Stopping. Best iteration:
    [947]	train-rmse:0.96859	valid-rmse:3.72098
    
    


```python
sub_xgb = pd.DataFrame()
sub_xgb['target'] = sub_preds_xgb
```

## Stacking

- 3개의 예측치와 실제 예측치의 Linear regression 을 통해서 대~충 어느 정도의 계수를 가질때에 3개의 조합이 잘 맞느냐를 근사
- 이때 상수항은 없이! 하는것이 중요. Stacking 이 어쩃든 Weighted mean 이므로


```python
# stacking 준비
regr = linear_model.LinearRegression(fit_intercept = False)
```


```python
model_oof_preds= pd.DataFrame({'xgb':oof_preds_xgb,
                               'cat':oof_preds_catboost,
                               'lgm':oof_preds_lgm,
                               'True':y_train})
```


```python
import statsmodels.api as sm
```


```python
model_oof_preds[['xgb','lgm','cat']]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>xgb</th>
      <th>lgm</th>
      <th>cat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28.923031</td>
      <td>28.467960</td>
      <td>27.993408</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21.566641</td>
      <td>21.229759</td>
      <td>22.893387</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32.977371</td>
      <td>37.727515</td>
      <td>37.306006</td>
    </tr>
    <tr>
      <th>3</th>
      <td>34.244316</td>
      <td>35.668266</td>
      <td>37.523434</td>
    </tr>
    <tr>
      <th>4</th>
      <td>34.203175</td>
      <td>35.927976</td>
      <td>33.303277</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>395</th>
      <td>14.624663</td>
      <td>12.870272</td>
      <td>14.537821</td>
    </tr>
    <tr>
      <th>396</th>
      <td>15.784380</td>
      <td>10.807714</td>
      <td>14.005389</td>
    </tr>
    <tr>
      <th>397</th>
      <td>12.173995</td>
      <td>11.151352</td>
      <td>12.899440</td>
    </tr>
    <tr>
      <th>398</th>
      <td>10.109540</td>
      <td>12.587345</td>
      <td>8.796838</td>
    </tr>
    <tr>
      <th>399</th>
      <td>12.287602</td>
      <td>7.567070</td>
      <td>15.494076</td>
    </tr>
  </tbody>
</table>
<p>400 rows × 3 columns</p>
</div>




```python
stackedmodel = sm.OLS(model_oof_preds[['True']],model_oof_preds[['xgb','lgm','cat']])
```


```python
result = stackedmodel.fit()
```


```python
print(result.summary())
```

                                     OLS Regression Results                                
    =======================================================================================
    Dep. Variable:                   True   R-squared (uncentered):                   0.987
    Model:                            OLS   Adj. R-squared (uncentered):              0.987
    Method:                 Least Squares   F-statistic:                          1.024e+04
    Date:                Sun, 07 Feb 2021   Prob (F-statistic):                        0.00
    Time:                        01:36:03   Log-Likelihood:                         -998.52
    No. Observations:                 400   AIC:                                      2003.
    Df Residuals:                     397   BIC:                                      2015.
    Df Model:                           3                                                  
    Covariance Type:            nonrobust                                                  
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    xgb            0.2617      0.088      2.962      0.003       0.088       0.435
    lgm            0.2706      0.079      3.414      0.001       0.115       0.426
    cat            0.4766      0.091      5.217      0.000       0.297       0.656
    ==============================================================================
    Omnibus:                       64.888   Durbin-Watson:                   1.574
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              632.077
    Skew:                           0.264   Prob(JB):                    5.58e-138
    Kurtosis:                       9.136   Cond. No.                         34.4
    ==============================================================================
    
    Notes:
    [1] R² is computed without centering (uncentered) since the model does not contain a constant.
    [2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

## Result


```python
model_oof_submit=pd.DataFrame({'xgb':sub_preds_xgb,
                               'cat':sub_preds_catboost,
                               'lgm':sub_preds_lgm})
```


```python
submission = pd.DataFrame()
submission['Target'] = model_oof_submit['xgb'] * 0.25 + model_oof_submit['cat'] * 0.50 + model_oof_submit['lgm'] * 0.25 
```


```python
submission['Target']
```




    0      10.162903
    1      11.194272
    2      12.301294
    3      10.166376
    4       9.479463
             ...    
    101    24.366129
    102    23.325401
    103    29.013309
    104    27.675369
    105    22.838501
    Name: Target, Length: 106, dtype: float64



# Ref

- https://swlock.blogspot.com/2019/01/scikit-learn-cross-validation-iterators.html?m=1
- https://scikit-learn.org/stable/modules/cross_validation.html
- hands on machine learning (2nd)
