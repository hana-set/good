---
title:  "2.Bayes Optimization"
excerpt: "ë² ì´ì¦ˆ ìµœì í™”ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤."
categories:
  - Py_Analysis
tags:
  - ê¸°ì´ˆ
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
---

# How to Optimize

1. ìš°ì„  ê° hyperparameter ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•œë‹¤. 

2. regularization ê´€ë ¨ / learning rate ê´€ë ¨ / tree ìˆ˜ ê´€ë ¨ ë“±ì˜ ë³€ìˆ˜ë¥¼ êµ¬ë¶„ì§“ëŠ”ë‹¤.

3. default ì„¤ì •ì—ì„œì˜ loss ë¥¼ ê´€ì°°í•˜ê³  ê·¸ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.

4. ê° ê°’ë“¤ì„ ë„£ì–´ë³´ë©´ì„œ default ë³´ë‹¤ ì¢‹ì€ì„¤ì •ì´ ì–¸ì œ ë‚˜ì˜¤ëŠ”ì§€ ê´€ì°°í•œë‹¤. ì´ë•Œì— í° ë²”ì£¼ë¡œ ë‚˜ëˆˆ ê°’ë“¤ì€ ê°™ì´ ë³€ê²½í•˜ë„ë¡ í•œë‹¤.

    ex) regulerization ì„ ë„£ì–´ë³´ê³ ì‹¶ì€ ê²½ìš° ê´€ë ¨ ë³€ìˆ˜ë¥¼ ê°™ì´ ì¡°ì ˆí•˜ê³ , ë¹„êµê°€ ë˜ëŠ” ëª¨ë¸ì—ì„œëŠ” regularization ì„ ì“°ì§€ ì•ŠëŠ”ë‹¤. 

    ex) reg ë¥¼ ë„£ì—ˆì„ë–„ ì¢‹ì€ ê°’ì´ ë‚˜ì™”ë‹¤ë©´ ê·¸ ê²½ìš° reg ê°€ ë¶€ì¡±í•œê²ƒì´ë¯€ë¡œ reg ë¥¼ ë” ì˜¬ë¦¬ë©´ì„œ í™•ì¸í•´ë³¸ë‹¤..... ì²˜ëŸ¼ ì˜ë¯¸ê°€ ë¹„ìŠ·í•œê±°ë¼ë¦¬ëŠ” ê°™ì´ ì˜í–¥ì´ ìƒì‡„ë˜ì§€ ì•Šê²Œ ì¡°ì ˆí•´ì¤ì‹œë‹¤.

5. 4ë²ˆì—ì„œ ì–´ëŠì •ë„ ì•ˆì •í™”ê°€ ë˜ë©´, ê·¸ ê°’ë“¤ì„ ë²”ìœ„ë¥¼ ì„¤ì •í•˜ê³ , ë² ì´ì§€ì•ˆ optimization ì„ ì‹¤í–‰í•´ë³¸ë‹¤. 


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics # ëª¨ë¸í‰ê°€ì‹œ ì´ìš©
```


```python
from sklearn.datasets import fetch_california_housing
import pandas as pd
california = fetch_california_housing()
X = pd.DataFrame(california.data, columns=california.feature_names)
y = pd.DataFrame(california.target,columns=["Target"])
df = pd.concat([X, y], axis=1)
df.tail()
y = california.target
```


```python
# dataset train/test set ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```

# Random Forest (íšŒê·€)

## íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •


```python
bayes_params = {
    'min_samples_split' : (2,100),
    'min_samples_leaf' : (1,100),
    'max_depth': (1, 10),}
```


```python
from sklearn.ensemble import RandomForestRegressor
rf_model=RandomForestRegressor()
```

## í‰ê°€ í•¨ìˆ˜ ì •ì˜


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
import numpy as np

# ìš°ì„  ìš°ë¦¬ê°€ tuning í•˜ê³  ì‹¶ì€ ê°’ë“¤ì„ ë°›ê³ , ê·¸ì—ë”°ë¼ì„œ tuning ì´ ë˜ê²Œ ì¡°ì ˆí•´ë´…ì‹œë‹¤.
# ìš°ì„  max_depth, min_samples_split, min_samples_leaf ë¥¼ ì´ìš©í•œë‹¤ê³  í•´ ë´…ì‹œë‹¤.
def ran_mse_eval(max_depth,
                min_samples_split,
                min_samples_leaf,):
    # ì—¬ê¸°ì—ì„œëŠ” ìš°ë¦¬ê°€ ì¡°ì ˆí•´ì•¼í•  íŒŒë¼ë¯¸í„°ë“¤ì„ dic í˜•íƒœë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    params = {
        "n_estimators": 100 , # êµ³ì´ parameter grid ë¡œ ì°¾ê³ ì‹¶ì§€ëŠ” ì•Šì§€ë§Œ ,default ê°’ì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ê°’ì„ ì£¼ê³ ì‹¶ì„ë–„ ì´ë ‡ê²Œ 200ìœ¼ë¡œ ê³ ì •í•´ì„œ í•˜ê³ ì‹¶ë‹¤ê³  í•©ì‹œë‹¤!
        'max_depth': int(round(max_depth)), # ì´ ë•Œì— ì •ìˆ˜ê°’ì´ ë“¤ì–´ê°€ì•¼í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°ìš”, ê·¸ ë–„ì—ëŠ” int(round ë¡œ ì²˜ë¦¬ë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
        'min_samples_split' : int(round(min_samples_split)),
        'min_samples_leaf' : int(round(min_samples_leaf))}
    print("params:", params)  # ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€ print í•˜ê²Œ í•´ì„œ í•™ìŠµê³¼ì •ì„ ì§€ì¼œë³´ë„ë¡ í•´ìš”~
    rf_model = RandomForestRegressor(**params) # ìš°ë¦¬ì˜ ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    
    # ì—¬ê¸°ì„œ result ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤! 
    
    # 1. y_val/ x_val ì„ ë‚˜ëˆ„ì–´ì„œ í‰ê°€í•˜ëŠ” ê²½ìš°
    #rf_model.fit(X_train, y_train) # ë¨¼ì € train ì— í•™ìŠµì‹œí‚¨ í›„
    #valid_proba = rf_model.predict(X_val) # X_val ì— ëŒ€í•´ì„œ ì˜ˆì¸¡í•˜ê³ 
    #result = -1 * mean_squared_error(y_val, valid_proba) # ê·¸ì— ë”°ë¥¸ mse ìŠ¤ì½”ì–´ë¥¼ êµ¬í•©ë‹ˆë‹¤.
    
    # í•˜ì§€ë§Œ ìœ„ ê°™ì€ ê²½ìš°.. ë­”ê°€ ê²°ê³¼ê°€ y_val/ x_val ì— ê³¼ì í•©ë  ìˆ˜ë„ ìˆì„ê±°ê°™ì•„ì„œ 
    # 2. ê·¸ëƒ¥ X_train/ y_train ì˜ cv ê°’ì„ í•˜ê²Œë˜ë©´ ë” ì¢‹ì•„ë³´ì¸ë‹¤! 
    cv_value = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error') # ì´ë–„ scoring ì€ ì£¼ìµœì¸¡ì—ì„œ ì •í•œ ê°’ì„ ë„£ì–´ì•¼ ê²Œì—£ì£ ì˜¤?
    result = np.mean(cv_value) # cv_value ëŠ” list í˜•íƒœë¡œ ë‚˜ì˜¤ê²Œ ë˜ë‹ˆê¹Œ!
    return result # ì´ result ê°’ì´ ì»¤ì§€ê²Œ ë² ì´ì§€ì•ˆoptimization ì´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

    print('mse:', result)  # ê·¸ ê°’ì„ ë„ì¶œ print í•´ì„œ ì˜ í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ (ì¤„ì—¬ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ) ì•Œ ì•„ ë³´ì•„ìš”~
```

## Optimization

BayesianOptimization ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
ì´ë•Œ ìƒì„± ì¸ìë¡œ ì•ì—ì„œ ë§Œë“  í‰ê°€í•¨ìˆ˜ lgb_roc_eval í•¨ìˆ˜ì™€ íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ê°’ì„ ì„¤ì •í•œ ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ì¸ bayes_paramsë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.


```python
from bayes_opt import BayesianOptimization
# ê°ì²´ë¥¼ í˜•ì„±í•œë‹¤
BO_rf = BayesianOptimization(ran_mse_eval, bayes_params, random_state=0)
```

ì´ì œ ì…ë ¥ë°›ì€ í‰ê°€í•¨ìˆ˜ì— íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ë°˜ë³µì ìœ¼ë¡œ ì…ë ¥í•˜ì—¬ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 
BayesianOptimizationê°ì²´ì—ì„œ maximize()ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 


```python
BO_rf.maximize(init_points=1, n_iter=3) 
# ìš°ë¦¬ì˜ object function ì„ maximize! í•˜ë ¤ í•œë‹¤. ê·¸ë˜ì„œ ìœ„ì—ì„œ return ì„ negative mse ë¥¼ ì“´ ê²ƒì´ë‹¤.
# ìµœì  íŒŒë¼ë¯¸í„° ë„ì¶œ ì‘ì—…ì„ n_iter ë§Œí¼ ë°˜ë³µí•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤!
# init_points = ì²« ì‹œì‘ì§€ì ì˜ score ë¥¼ 5ê°œ ëŒë ¤ì„œ ì•Œì•„ë³´ëŠ”ê²ƒ.
```

    |   iter    |  target   | max_depth | min_sa... | min_sa... |
    -------------------------------------------------------------
    params: {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 54, 'min_samples_leaf': 79}
    | [0m 3       [0m | [0m-0.5063  [0m | [0m 4.451   [0m | [0m 79.38   [0m | [0m 53.83   [0m |
    params: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 93}
    | [0m 4       [0m | [0m-0.4211  [0m | [0m 6.112   [0m | [0m 92.63   [0m | [0m 8.962   [0m |
    params: {'n_estimators': 100, 'max_depth': 2, 'min_samples_split': 84, 'min_samples_leaf': 3}
    | [0m 5       [0m | [0m-0.7096  [0m | [0m 1.784   [0m | [0m 3.002   [0m | [0m 83.6    [0m |
    params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 79, 'min_samples_leaf': 61}
    | [95m 6       [0m | [95m-0.3624  [0m | [95m 10.0    [0m | [95m 60.51   [0m | [95m 78.68   [0m |
    params: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 40, 'min_samples_leaf': 96}
    | [0m 7       [0m | [0m-0.5817  [0m | [0m 3.073   [0m | [0m 95.79   [0m | [0m 39.56   [0m |
    params: {'n_estimators': 100, 'max_depth': 1, 'min_samples_split': 100, 'min_samples_leaf': 85}
    | [0m 8       [0m | [0m-0.8864  [0m | [0m 1.0     [0m | [0m 84.85   [0m | [0m 100.0   [0m |
    =============================================================
    


```python
# BayesianOptimization ê°ì²´ì˜ res ì†ì„±ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•˜ëŠ” ê³¼ì •ì—ì„œì˜ metric ê°’ê³¼ ê·¸ë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ. 
```


```python
#BO_rf.res 
```

BayesianOptimization ê°ì²´ì˜ max ì†ì„±ì€ ìµœê³  ë†’ì€ ì„±ëŠ¥ Metricë¥¼ ê°€ì§ˆë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ.


```python
BO_rf.max
# ì´ë–„ì— ìš°ë¦¬ëŠ” ì•„ë˜ ê°’ë“¤ì„ ë°˜ì˜¬ë¦¼ í•´ì„œ(int(round)) ì‚¬ìš©í–ˆìŒì„ ê¸°ì–µí•˜ì„¸ìš”!
# ì¦‰ max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 ì´ ë©ë‹ˆë‹¤. # ì´ëŠ” ì‹¤í–‰ë§ˆë‹¤ ë‹¬ë¼ì§€ë‹ˆê¹Œ ì£¼ì˜í•˜ì„¸ìš©
```




    {'target': -0.3624136969982552,
     'params': {'max_depth': 10.0,
      'min_samples_leaf': 60.506031279081775,
      'min_samples_split': 78.68476690496574}}



## ìµœì¢… í‰ê°€!


```python
from sklearn.metrics import mean_squared_error

max_params = BO_rf.max['params']
max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))
max_params['min_samples_split'] = int(round(max_params['min_samples_split']))
model_rf = RandomForestRegressor(n_estimators=100, **max_params)
model_rf.fit(X_train, y_train)
y_pred = model_rf.predict(X_test)
score = mean_squared_error(y_pred,y_test)

print('mse score : {0:.4f}'.format(score))
```

    mse score : 0.3655
    

## ë¹„êµ ëª¨ë¸

- ì¼ë°˜ì ìœ¼ë¡œ ê·¸ëƒ¥ search ë³´ë‹¤ëŠ” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤
- ë‹¤ë§Œ! ì—¬ê¸°ì—ì„œëŠ” ê·¸ëƒ¥ iter ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶”ì–´ì„œ randomize ê°€ ì´ê¸´ê²ƒì²˜ëŸ¼ ë³´ì´ë‚˜ ì›ë˜ëŠ” ë² ì´ì¦ˆê°€ ì´ê¹ë‹ˆë‹¤.
- Colab ì—ì„œ ëŒë¦¬ëŠ”ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.


```python
from sklearn.ensemble import RandomForestRegressor
model =  RandomForestRegressor(n_estimators=100)
# scoring ì€ default ì´ë¯€ë¡œ model ì˜ ìì²´ scoring ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. 
# n_estimator = 500 í´ìˆ˜ë¡ ì¢‹ìœ¼ë‚˜ ë‚´ ì»´í“¨í„°ê°€ ë²„í‹°ì§ˆ ëª»í• ë“¯.
model.fit(X_train,y_train)
```




    RandomForestRegressor()




```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 0.26241141407302704
    R_squared : 0.7987572932106815
    

# XGBboost (íšŒê·€)

2-1. ì¼ë°˜ íŒŒë¼ë©”í„°


(ì¤‘ìš”)booster: ì–´ë–¤ ë¶€ìŠ¤í„° êµ¬ì¡°ë¥¼ ì“¸ì§€ ê²°ì •í•œë‹¤. ì´ê²ƒì€ gbtree, gblinear, dartê°€ ìˆë‹¤.

(ì¤‘ìš”)n_eastimator : This is how many subtrees h will be trained.

nthread: ëª‡ ê°œì˜ ì“°ë ˆë“œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ë„ë¡ í• ì§€ ê²°ì •í•œë‹¤. ë””í´íŠ¸ëŠ” â€œê°€ëŠ¥í•œ í•œ ë§ì´â€.

num_feature: feature ì°¨ì›ì˜ ìˆ«ìë¥¼ ì •í•´ì•¼ í•˜ëŠ” ê²½ìš° ì˜µì…˜ì„ ì„¸íŒ…í•œë‹¤. ë””í´íŠ¸ëŠ” â€œê°€ëŠ¥í•œ í•œ ë§ì´.â€


2-2. ë¶€ìŠ¤íŒ… íŒŒë¼ë©”í„°


(ì¤‘ìš”)learning_rate: learning rateë‹¤. íŠ¸ë¦¬ì— ê°€ì§€ê°€ ë§ì„ìˆ˜ë¡ ê³¼ì í•©í•˜ê¸° ì‰½ë‹¤. ê° íŠ¸ë¦¬ë§ˆë‹¤ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ë¶€ìŠ¤íŒ… ê³¼ì •ì— ê³¼ì í•©ì´ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡ í•œë‹¤

gamma: ì •ë³´ íšë“(Information Gain)ì—ì„œ -rë¡œ í‘œí˜„í•œ ë°” ìˆë‹¤. ì´ê²ƒì´ ì»¤ì§€ë©´, íŠ¸ë¦¬ ê¹Šì´ê°€ ì¤„ì–´ë“¤ì–´ ë³´ìˆ˜ì ì¸ ëª¨ë¸ì´ ëœë‹¤. ë””í´íŠ¸ ê°’ì€ 0ì´ë‹¤

(ì¤‘ìš”)max_depth: í•œ íŠ¸ë¦¬ì˜ maximum depth. ìˆ«ìë¥¼ í‚¤ìš¸ìˆ˜ë¡ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì»¤ì§„ë‹¤. ì‘ì„ìˆ˜ë¡ ê³¼ì í•© ë°©ì§€ . ë””í´íŠ¸ëŠ” 6. 

lambda(L2 reg-form): L2 Regularization Formì— ë‹¬ë¦¬ëŠ” weightsì´ë‹¤. ìˆ«ìê°€ í´ìˆ˜ë¡ ë³´ìˆ˜ì ì¸ ëª¨ë¸ì´ ëœë‹¤

(ì¤‘ìš”)alpha(L1 reg-form): L1 Regularization Form weightsë‹¤. ìˆ«ìê°€ í´ìˆ˜ë¡ ê³¼ì í•© ë°©ì§€


2-3. í•™ìŠµ ê³¼ì • íŒŒë¼ë©”í„°

objective: ëª©ì  í•¨ìˆ˜ë‹¤. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) ë“± ë‹¤ì–‘í•˜ë‹¤

eval_metric: ëª¨ë¸ì˜ í‰ê°€ í•¨ìˆ˜ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ë‹¤. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) ë“±, í•´ë‹¹ ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ê²Œ í‰ê°€ í•¨ìˆ˜ë¥¼ ì¡°ì •í•œë‹¤



2-4. ì»¤ë§¨ë“œ ë¼ì¸ íŒŒë¼ë©”í„°
num_rounds: boosting ë¼ìš´ë“œë¥¼ ê²°ì •í•œë‹¤. ëœë¤ í•˜ê²Œ ìƒì„±ë˜ëŠ” ëª¨ë¸ì´ë‹ˆë§Œí¼ ì´ ìˆ˜ê°€ ì ë‹¹íˆ í° ê²Œ ì¢‹ë‹¤. epoch ì˜µì…˜ê³¼ ë™ì¼í•˜ë‹¤

ë‚´ ìƒê°ì— tuning í• ë•Œì—ëŠ” ì¤‘ìš” í•˜ë‹¤ê³  ìƒê°ë˜ëŠ” ê²ƒë“¤ì„ ì‹ ê²½ì“°ëŠ”ê²Œ ì¢‹ì•„ë³´ì¸ë‹¤.
- 1. booster : ë¶€ìŠ¤í„° êµ¬ì¡°ë¥¼ ì˜ ê³¨ë¼ì„œ ì˜ ë‚˜ì˜¤ëŠ”ê²ƒì„ ìš°ì„  ì„ ë³„í•œë‹¤.
- 2. n_eastimator : ì´ ê²½ìš°ë„ í´ìˆ˜ë¡ tree ê°€ ë§ì•„ì ¸ì„œ í•™ìŠµì´ ëŠë ¤ì§„ë‹¤. ë¯¸ë¦¬ ì„ ë³„í•´ì„œ ê¸°ì–µí•˜ëŠ”ê²Œ ì¢‹ì„ë“¯í•˜ë‹¤.
- 3. learning rate(íŠ¸ë¦¬ê°„ ê°€ì¤‘ì¹˜) / maxdepth(íŠ¸ë¦¬ì˜ êµ¬ì¡°) / alpha(íŠ¸ë¦¬ì˜ leaf ì œí•œ) ì„¸ê°œë¥¼ ì ì ˆíˆ ì¨ì„œ ì˜ ì¡°ì ˆí•œë‹¤.
- 4. ì´ì œ ì–´ëŠì •ë„ í™•ì •ëœ ê°’ì„ ê°€ì§€ê³  ì–´ë–»ê²Œ ì¡°ì ˆí• ì§€ ê°ì„ ì¡ì€ í›„ì— colab ìœ¼ë¡œ Baysian ì„ ëŒë¦¬ë©´ ë ê±°ê°™ë‹¤!


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
XGBRegressor()
```




    XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,
                 colsample_bynode=None, colsample_bytree=None, gamma=None,
                 gpu_id=None, importance_type='gain', interaction_constraints=None,
                 learning_rate=None, max_delta_step=None, max_depth=None,
                 min_child_weight=None, missing=nan, monotone_constraints=None,
                 n_estimators=100, n_jobs=None, num_parallel_tree=None,
                 random_state=None, reg_alpha=None, reg_lambda=None,
                 scale_pos_weight=None, subsample=None, tree_method=None,
                 validate_parameters=None, verbosity=None)



## íŒŒë¼ë¯¸í„° ë²”ìœ„ì„¤ì •


```python
bayes_params = {'n_estimators' : (100,300), # íŠ¸ë¦¬ì˜ ê°¯ìˆ˜
                'learning_rate' : (0.05,0.15), # learning rate
                'max_depth': (9, 14), # ì»¤ì§ˆìˆ˜ë¡ ë³µì¡í•œ ëª¨ë¸ (ê³¼ì í•© ë°©ì§€)
                'reg_alpha': (0, 0.2), # leaves ì— ëŒ€í•œ l1 ê·œì œì˜ ê³„ìˆ˜ì´ë¯€ë¡œ í´ìˆ˜ë¡ ê·œì œ
                'colsample_bytree' :(0.85, 1.0)} # tree ì— ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ê°¯ìˆ˜ ì§€ì •. 
```

## í‰ê°€ custom loss ì •ì˜


```python
# ì–´ë–¤ ê²½ìš°ëŠ” ìš°ë¦¬ê°€ í‰ê°€í•˜ê³ ì í•˜ëŠ” loss ê°€ ì—†ì„ ìˆ˜ ìˆë‹¤.
```


```python
from sklearn.metrics import make_scorer
import numpy as np

def mape(y_true, y_predict):
    # Note this blows up if y_true = 0
    # Ignore for demo -- in some sense an unsolvable
    # problem with MAPE as an error metric 
    # í•˜ì§€ë§Œ y_true ê°€ 0 ì¸ê²½ìš°ëŠ” ì—†ìœ¼ë¯€ë¡œ ì•ˆì‹¬í•˜ë¼êµ¬!
    y_true = np.array(y_true)
    y_predict = np.array(y_predict)
    return np.abs((y_true - y_predict)/y_true).mean()

mape_scorer = make_scorer(mape, greater_is_better=False) 
# greater is better ì´ false ì´ë¯€ë¡œ neg ê°’ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. 
```

## í‰ê°€ í•¨ìˆ˜ ì •ì˜


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# ìš°ì„  ìš°ë¦¬ê°€ tuning í•˜ê³  ì‹¶ì€ ê°’ë“¤ì„ ë°›ê³ , ê·¸ì—ë”°ë¼ì„œ tuning ì´ ë˜ê²Œ ì¡°ì ˆí•´ë´…ì‹œë‹¤.
# ìš°ì„  max_depth, min_samples_split, min_samples_leaf ë¥¼ ì´ìš©í•œë‹¤ê³  í•´ ë´…ì‹œë‹¤.
def ran_mape_eval(n_estimators,
                  learning_rate,
                 max_depth,
                 reg_alpha,
                 colsample_bytree):
    # ì—¬ê¸°ì—ì„œëŠ” ìš°ë¦¬ê°€ ì¡°ì ˆí•´ì•¼í•  íŒŒë¼ë¯¸í„°ë“¤ì„ dic í˜•íƒœë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    params = {
        "n_estimators": int(round(n_estimators)) ,
        'learning_rate': learning_rate,
        'max_depth': int(round(max_depth)), # ì´ ë•Œì— ì •ìˆ˜ê°’ì´ ë“¤ì–´ê°€ì•¼í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°ìš”, ê·¸ ë–„ì—ëŠ” int(round ë¡œ ì²˜ë¦¬ë¥¼ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
        'reg_alpha' : reg_alpha,
        'colsample_bytree' : colsample_bytree}
    print("params:", params)  # ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€ print í•˜ê²Œ í•´ì„œ í•™ìŠµê³¼ì •ì„ ì§€ì¼œë³´ë„ë¡ í•´ìš”~
    xgb_model = XGBRegressor(**params) # ìš°ë¦¬ì˜ ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    
    # ì—¬ê¸°ì„œ result ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤! 
    
    # 1. y_val/ x_val ì„ ë‚˜ëˆ„ì–´ì„œ í‰ê°€í•˜ëŠ” ê²½ìš°
    #rf_model.fit(X_train, y_train) # ë¨¼ì € train ì— í•™ìŠµì‹œí‚¨ í›„
    #valid_proba = rf_model.predict(X_val) # X_val ì— ëŒ€í•´ì„œ ì˜ˆì¸¡í•˜ê³ 
    #result = -1 * mean_squared_error(y_val, valid_proba) # ê·¸ì— ë”°ë¥¸ mse ìŠ¤ì½”ì–´ë¥¼ êµ¬í•©ë‹ˆë‹¤.
    
    # í•˜ì§€ë§Œ ìœ„ ê°™ì€ ê²½ìš°.. ë­”ê°€ ê²°ê³¼ê°€ y_val/ x_val ì— ê³¼ì í•©ë  ìˆ˜ë„ ìˆì„ê±°ê°™ì•„ì„œ 
    # 2. ê·¸ëƒ¥ X_train/ y_train ì˜ cv ê°’ì„ í•˜ê²Œë˜ë©´ ë” ì¢‹ì•„ë³´ì¸ë‹¤! 
    cv_value = cross_val_score(xgb_model, X_train, y_train, cv=3, scoring = mape_scorer) # ì´ë–„ scoring ì€ ìš°ë¦¬ê°€ ìœ„ì—ì„œ ì •ì˜í•œ í‰ê°€í•¨ìˆ˜ë¥¼ ì»ë‹¤.
    result = np.mean(cv_value) # cv_value ëŠ” list í˜•íƒœë¡œ ë‚˜ì˜¤ê²Œ ë˜ë‹ˆê¹Œ!
    return result # ì´ result ê°’ì´ ì»¤ì§€ê²Œ ë² ì´ì§€ì•ˆoptimization ì´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

    print('mape:', result)  # ê·¸ ê°’ì„ ë„ì¶œ print í•´ì„œ ì˜ í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ (ì¤„ì—¬ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ) ì•Œ ì•„ ë³´ì•„ìš”~
```

## Optimization

BayesianOptimization ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
ì´ë•Œ ìƒì„± ì¸ìë¡œ ì•ì—ì„œ ë§Œë“  í‰ê°€í•¨ìˆ˜ lgb_roc_eval í•¨ìˆ˜ì™€ íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ê°’ì„ ì„¤ì •í•œ ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ì¸ bayes_paramsë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.


```python
from bayes_opt import BayesianOptimization
# ê°ì²´ë¥¼ í˜•ì„±í•œë‹¤
BO_rf = BayesianOptimization(ran_mape_eval, bayes_params, random_state=0)
```

ì´ì œ ì…ë ¥ë°›ì€ í‰ê°€í•¨ìˆ˜ì— íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ë°˜ë³µì ìœ¼ë¡œ ì…ë ¥í•˜ì—¬ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 
BayesianOptimizationê°ì²´ì—ì„œ maximize()ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 


```python
BO_rf.maximize(init_points=2, n_iter=3) 
# ìš°ë¦¬ì˜ object function ì„ maximize! í•˜ë ¤ í•œë‹¤. ê·¸ë˜ì„œ ìœ„ì—ì„œ return ì„ negative mse ë¥¼ ì“´ ê²ƒì´ë‹¤.
# ìµœì  íŒŒë¼ë¯¸í„° ë„ì¶œ ì‘ì—…ì„ n_iter ë§Œí¼ ë°˜ë³µí•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤!
# init_points = ì²« ì‹œì‘ì§€ì ì˜ score ë¥¼ 5ê°œ ëŒë ¤ì„œ ì•Œì•„ë³´ëŠ”ê²ƒ.
```

    |   iter    |  target   | colsam... | learni... | max_depth | n_esti... | reg_alpha |
    -------------------------------------------------------------------------------------
    params: {'n_estimators': 209, 'learning_rate': 0.12151893663724195, 'max_depth': 12, 'reg_alpha': 0.08473095986778095, 'colsample_bytree': 0.9323220255890987}
    | [0m 1       [0m | [0m-0.1702  [0m | [0m 0.9323  [0m | [0m 0.1215  [0m | [0m 12.01   [0m | [0m 209.0   [0m | [0m 0.08473 [0m |
    params: {'n_estimators': 293, 'learning_rate': 0.09375872112626925, 'max_depth': 13, 'reg_alpha': 0.07668830376515555, 'colsample_bytree': 0.9468841169599984}
    | [95m 2       [0m | [95m-0.1696  [0m | [95m 0.9469  [0m | [95m 0.09376 [0m | [95m 13.46   [0m | [95m 292.7   [0m | [95m 0.07669 [0m |
    params: {'n_estimators': 291, 'learning_rate': 0.09738089297908531, 'max_depth': 13, 'reg_alpha': 0.05391212288356184, 'colsample_bytree': 0.8568444223026506}
    | [95m 3       [0m | [95m-0.1695  [0m | [95m 0.8568  [0m | [95m 0.09738 [0m | [95m 13.31   [0m | [95m 291.4   [0m | [95m 0.05391 [0m |
    params: {'n_estimators': 285, 'learning_rate': 0.10203465161914638, 'max_depth': 13, 'reg_alpha': 0.07549368916672952, 'colsample_bytree': 0.9450782992887754}
    | [0m 4       [0m | [0m-0.1707  [0m | [0m 0.9451  [0m | [0m 0.102   [0m | [0m 12.79   [0m | [0m 285.0   [0m | [0m 0.07549 [0m |
    params: {'n_estimators': 292, 'learning_rate': 0.05323332238814628, 'max_depth': 10, 'reg_alpha': 0.17997766062089499, 'colsample_bytree': 0.8757474751378177}
    | [95m 5       [0m | [95m-0.1667  [0m | [95m 0.8757  [0m | [95m 0.05323 [0m | [95m 9.582   [0m | [95m 292.0   [0m | [95m 0.18    [0m |
    =====================================================================================
    


```python
# BayesianOptimization ê°ì²´ì˜ res ì†ì„±ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•˜ëŠ” ê³¼ì •ì—ì„œì˜ metric ê°’ê³¼ ê·¸ë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ. 
```


```python
#BO_rf.res 
```

BayesianOptimization ê°ì²´ì˜ max ì†ì„±ì€ ìµœê³  ë†’ì€ ì„±ëŠ¥ Metricë¥¼ ê°€ì§ˆë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ.


```python
BO_rf.max
# ì´ë–„ì— ìš°ë¦¬ëŠ” ì•„ë˜ ê°’ë“¤ì„ ë°˜ì˜¬ë¦¼ í•´ì„œ(int(round)) ì‚¬ìš©í–ˆìŒì„ ê¸°ì–µí•˜ì„¸ìš”!
# ì¦‰ max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 ì´ ë©ë‹ˆë‹¤. # ì´ëŠ” ì‹¤í–‰ë§ˆë‹¤ ë‹¬ë¼ì§€ë‹ˆê¹Œ ì£¼ì˜í•˜ì„¸ìš©
```




    {'target': -0.16671083283960844,
     'params': {'colsample_bytree': 0.8757474751378177,
      'learning_rate': 0.05323332238814628,
      'max_depth': 9.582347007327694,
      'n_estimators': 291.95980137914466,
      'reg_alpha': 0.17997766062089499}}



## ìµœì¢… í‰ê°€!


```python
max_params = BO_rf.max['params']
max_params['max_depth'] = int(round(max_params['max_depth']))
max_params['n_estimators'] = int(round(max_params['n_estimators']))

model_xgb = XGBRegressor(**max_params)
model_xgb.fit(X_train, y_train)
y_pred = model_xgb.predict(X_test)
score = mape(y_pred,y_test)

print('mape score : {0:.4f}'.format(score))
```

    mape score : 0.1466
    

## ë¹„êµëª¨ë¸


```python
from sklearn.ensemble import RandomForestRegressor
model = XGBRegressor()
model.fit(X_train,y_train)
```




    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                 colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
                 importance_type='gain', interaction_constraints='',
                 learning_rate=0.300000012, max_delta_step=0, max_depth=6,
                 min_child_weight=1, missing=nan, monotone_constraints='()',
                 n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
                 tree_method='exact', validate_parameters=1, verbosity=None)




```python
y_pred = model.predict(X_test)
print ("MAPE :", mape(y_test, y_pred))
```

    MAPE : 0.17614545366526785
    

# Light LGB(íšŒê·€)

## íŒŒë¼ë¯¸í„° ë²”ìœ„ì„¤ì •


```python
bayes_params ={ 
    'learning_rate': (0.05,0.2),
    'n_estimators' : (50,250),
    'max_depth': (12,24), # íŠ¸ë¦¬ì˜ ê¹Šì´
    'num_leaves': (30,40), 
    'feature_fraction': (0.7,0.99), # íŠ¸ë¦¬ë¥¼ ë§Œë“¤ë–„ ì–¼ë§ˆë‚˜ feaure ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí• ì§€
    'bagging_fraction': (0.6,0.9), # ë°°ê¹…ì˜ ë¹„ìœ¨
    'bagging_freq':(1,3)} # ë² ê¹…ì„ ì–¼ë§ˆë‚˜ ì§„í–‰í• ì§€ 
```

## í‰ê°€ custom loss ì •ì˜


```python
# ì–´ë–¤ ê²½ìš°ëŠ” ìš°ë¦¬ê°€ í‰ê°€í•˜ê³ ì í•˜ëŠ” loss ê°€ ì—†ì„ ìˆ˜ ìˆë‹¤.
```


```python
from sklearn.metrics import make_scorer
import numpy as np

def mape(y_true, y_predict):
    # Note this blows up if y_true = 0
    # Ignore for demo -- in some sense an unsolvable
    # problem with MAPE as an error metric 
    # í•˜ì§€ë§Œ y_true ê°€ 0 ì¸ê²½ìš°ëŠ” ì—†ìœ¼ë¯€ë¡œ ì•ˆì‹¬í•˜ë¼êµ¬!
    y_true = np.array(y_true)
    y_predict = np.array(y_predict)
    return np.abs((y_true - y_predict)/y_true).mean()

mape_scorer = make_scorer(mape, greater_is_better=False) 
# greater is better ì´ false ì´ë¯€ë¡œ neg ê°’ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. 
# ì¦‰ MAPE SCORE ëŠ” -mape ê°€ ë‚˜ì™€ìš”~
```

## í‰ê°€ í•¨ìˆ˜ ì •ì˜


```python
#ë§Œì¼ shuffle ì´ ì œëŒ€ë¡œ ì•ˆë˜ì–´ìˆì–´ì„œ ê±±ì •ëœë‹¤ë©´ ì´ë ‡ê²Œ k fold ë¡œ shuffle ì‹œì¼œì£¼ì„¸ìš” ^^
#from sklearn.model_selection import KFold
#kfold = KFold(n_splits=5, shuffle=True, random_state=2)
#cross_val_score(lgb_model, X_train, y_train, cv=kfold, scoring = mape_scorer)
```


```python
from lightgbm import LGBMRegressor
import lightgbm as lgb
from sklearn.model_selection import cross_val_score

def lgb_mape_eval(learning_rate,
                  n_estimators,
                 max_depth,
                 num_leaves,
                 feature_fraction,
                 bagging_fraction,
                 bagging_freq,
                 ):
    # í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ê¸° ìœ„í•´ , ì´ê²Œ ì œëŒ€ë¡œ! í•™ìŠµì´ ë˜ê³ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ í•™ìŠµ/ í‰ê°€í•˜ê³  ì´ì— ë”°ë¥¸ í‰ê°€ ì§€í‘œë¥¼ ë°˜í™˜í•˜ëŠ” í˜•ì‹ì´ ëœë‹¤.
    params = {
        'learning_rate': learning_rate,
        'max_depth': int(round(max_depth)), # íŠ¸ë¦¬ì˜ ê¹Šì´
        'num_leaves': int(round(num_leaves)), 
        'feature_fraction': feature_fraction, # íŠ¸ë¦¬ë¥¼ ë§Œë“¤ë–„ ì–¼ë§ˆë‚˜ feaure ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí• ì§€
        'bagging_fraction': bagging_fraction, # ë°°ê¹…ì˜ ë¹„ìœ¨
        'bagging_freq': int(round(bagging_freq)), # ë² ê¹…ì„ ì–¼ë§ˆë‚˜ ì§„í–‰í• ì§€
        'n_estimators' : int(round(n_estimators))
    }    

    print("params:", params)  # ì–´ ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€
    lgb_model = LGBMRegressor(**params) # ëª¨ë¸! 
    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring = mape_scorer)
    
    result = np.mean(cv_value) 
    # cv_value ëŠ” list í˜•íƒœë¡œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. (ê° cv ë§ˆë‹¤ì˜ score ê°’.)
    return result 
    # ì´ result ê°’ì´ ì»¤ì§€ê²Œ ë² ì´ì§€ì•ˆoptimization ì´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

    print('accuracy :', result)  # ê·¸ ê°’ì„ ë„ì¶œ print í•´ì„œ ì˜ í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ (ì¤„ì—¬ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ) ì•Œ ì•„ ë³´ì•„ìš”~    return roc_preds
```

## Optimization

BayesianOptimization ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
ì´ë•Œ ìƒì„± ì¸ìë¡œ ì•ì—ì„œ ë§Œë“  í‰ê°€í•¨ìˆ˜ lgb_roc_eval í•¨ìˆ˜ì™€ íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ê°’ì„ ì„¤ì •í•œ ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ì¸ bayes_paramsë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.


```python
from bayes_opt import BayesianOptimization
# ê°ì²´ë¥¼ í˜•ì„±í•œë‹¤
BO_rf = BayesianOptimization(lgb_mape_eval, bayes_params, random_state=0)
```

ì´ì œ ì…ë ¥ë°›ì€ í‰ê°€í•¨ìˆ˜ì— íŠœë‹í•  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ë°˜ë³µì ìœ¼ë¡œ ì…ë ¥í•˜ì—¬ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 
BayesianOptimizationê°ì²´ì—ì„œ maximize()ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 


```python
BO_rf.maximize(init_points=5, n_iter=10) 
# ìš°ë¦¬ì˜ object function ì„ maximize! í•˜ë ¤ í•œë‹¤. ê·¸ë˜ì„œ ìœ„ì—ì„œ return ì„ negative mse ë¥¼ ì“´ ê²ƒì´ë‹¤.
# ìµœì  íŒŒë¼ë¯¸í„° ë„ì¶œ ì‘ì—…ì„ n_iter ë§Œí¼ ë°˜ë³µí•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤!
# init_points = ì²« ì‹œì‘ì§€ì ì˜ score ë¥¼ 5ê°œ ëŒë ¤ì„œ ì•Œì•„ë³´ëŠ”ê²ƒ.
```

    |   iter    |  target   | baggin... | baggin... | featur... | learni... | max_depth | n_esti... | num_le... |
    -------------------------------------------------------------------------------------------------------------
    params: {'learning_rate': 0.13173247744953454, 'max_depth': 17, 'num_leaves': 34, 'feature_fraction': 0.8748013790607767, 'bagging_fraction': 0.7646440511781974, 'bagging_freq': 2, 'n_estimators': 179}
    | [0m 1       [0m | [0m-0.1706  [0m | [0m 0.7646  [0m | [0m 2.43    [0m | [0m 0.8748  [0m | [0m 0.1317  [0m | [0m 17.08   [0m | [0m 179.2   [0m | [0m 34.38   [0m |
    params: {'learning_rate': 0.1687587557123997, 'max_depth': 18, 'num_leaves': 39, 'feature_fraction': 0.8111980404594755, 'bagging_fraction': 0.867531900234624, 'bagging_freq': 3, 'n_estimators': 164}
    | [95m 2       [0m | [95m-0.1704  [0m | [95m 0.8675  [0m | [95m 2.927   [0m | [95m 0.8112  [0m | [95m 0.1688  [0m | [95m 18.35   [0m | [95m 163.6   [0m | [95m 39.26   [0m |
    params: {'learning_rate': 0.17489297683219074, 'max_depth': 21, 'num_leaves': 40, 'feature_fraction': 0.7058633352576944, 'bagging_fraction': 0.6213108174593661, 'bagging_freq': 1, 'n_estimators': 224}
    | [0m 3       [0m | [0m-0.1775  [0m | [0m 0.6213  [0m | [0m 1.174   [0m | [0m 0.7059  [0m | [0m 0.1749  [0m | [0m 21.34   [0m | [0m 224.0   [0m | [0m 39.79   [0m |
    params: {'learning_rate': 0.06774116388033999, 'max_depth': 20, 'num_leaves': 39, 'feature_fraction': 0.926353461123072, 'bagging_fraction': 0.8397475692650171, 'bagging_freq': 2, 'n_estimators': 79}
    | [0m 4       [0m | [0m-0.1822  [0m | [0m 0.8397  [0m | [0m 1.923   [0m | [0m 0.9264  [0m | [0m 0.06774 [0m | [0m 19.68   [0m | [0m 78.67   [0m | [0m 39.45   [0m |
    params: {'learning_rate': 0.16613505341513252, 'max_depth': 17, 'num_leaves': 30, 'feature_fraction': 0.7767211275103418, 'bagging_fraction': 0.7565544965250215, 'bagging_freq': 2, 'n_estimators': 164}
    | [0m 5       [0m | [0m-0.173   [0m | [0m 0.7566  [0m | [0m 1.829   [0m | [0m 0.7767  [0m | [0m 0.1661  [0m | [0m 17.47   [0m | [0m 163.7   [0m | [0m 30.19   [0m |
    params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.99, 'bagging_fraction': 0.6, 'bagging_freq': 3, 'n_estimators': 135}
    | [0m 6       [0m | [0m-0.1795  [0m | [0m 0.6     [0m | [0m 3.0     [0m | [0m 0.99    [0m | [0m 0.05    [0m | [0m 24.0    [0m | [0m 134.6   [0m | [0m 40.0    [0m |
    params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'bagging_freq': 3, 'n_estimators': 193}
    | [0m 7       [0m | [0m-0.1712  [0m | [0m 0.9     [0m | [0m 3.0     [0m | [0m 0.7     [0m | [0m 0.05    [0m | [0m 24.0    [0m | [0m 192.6   [0m | [0m 40.0    [0m |
    params: {'learning_rate': 0.12472102385409382, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7720763961650281, 'bagging_fraction': 0.8314376373392335, 'bagging_freq': 3, 'n_estimators': 173}
    | [95m 8       [0m | [95m-0.1695  [0m | [95m 0.8314  [0m | [95m 2.981   [0m | [95m 0.7721  [0m | [95m 0.1247  [0m | [95m 23.5    [0m | [95m 173.0   [0m | [95m 39.77   [0m |
    params: {'learning_rate': 0.09375052437923304, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.9371506430319936, 'bagging_fraction': 0.8925919669634644, 'bagging_freq': 3, 'n_estimators': 174}
    | [95m 9       [0m | [95m-0.1687  [0m | [95m 0.8926  [0m | [95m 2.578   [0m | [95m 0.9372  [0m | [95m 0.09375 [0m | [95m 23.46   [0m | [95m 173.6   [0m | [95m 38.76   [0m |
    params: {'learning_rate': 0.05595332997717015, 'max_depth': 24, 'num_leaves': 30, 'feature_fraction': 0.7732915429010659, 'bagging_fraction': 0.818689843828635, 'bagging_freq': 1, 'n_estimators': 184}
    | [0m 10      [0m | [0m-0.1753  [0m | [0m 0.8187  [0m | [0m 1.12    [0m | [0m 0.7733  [0m | [0m 0.05595 [0m | [0m 23.97   [0m | [0m 184.4   [0m | [0m 30.21   [0m |
    params: {'learning_rate': 0.050459114886174466, 'max_depth': 12, 'num_leaves': 39, 'feature_fraction': 0.9736076245608813, 'bagging_fraction': 0.813305748333056, 'bagging_freq': 1, 'n_estimators': 172}
    | [0m 11      [0m | [0m-0.1752  [0m | [0m 0.8133  [0m | [0m 1.39    [0m | [0m 0.9736  [0m | [0m 0.05046 [0m | [0m 12.07   [0m | [0m 172.0   [0m | [0m 38.9    [0m |
    params: {'learning_rate': 0.06257526977942529, 'max_depth': 24, 'num_leaves': 33, 'feature_fraction': 0.7878086998962229, 'bagging_fraction': 0.677221679288176, 'bagging_freq': 2, 'n_estimators': 171}
    | [0m 12      [0m | [0m-0.1744  [0m | [0m 0.6772  [0m | [0m 2.395   [0m | [0m 0.7878  [0m | [0m 0.06258 [0m | [0m 23.75   [0m | [0m 170.8   [0m | [0m 32.61   [0m |
    params: {'learning_rate': 0.1046953001301889, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.844141337224133, 'bagging_fraction': 0.7062916336088501, 'bagging_freq': 1, 'n_estimators': 180}
    | [0m 13      [0m | [0m-0.1715  [0m | [0m 0.7063  [0m | [0m 1.177   [0m | [0m 0.8441  [0m | [0m 0.1047  [0m | [0m 22.76   [0m | [0m 179.5   [0m | [0m 39.23   [0m |
    params: {'learning_rate': 0.06719992961699134, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.9506562232174386, 'bagging_fraction': 0.7142613973761445, 'bagging_freq': 2, 'n_estimators': 156}
    | [0m 14      [0m | [0m-0.1738  [0m | [0m 0.7143  [0m | [0m 1.943   [0m | [0m 0.9507  [0m | [0m 0.0672  [0m | [0m 19.44   [0m | [0m 156.0   [0m | [0m 38.87   [0m |
    params: {'learning_rate': 0.052247126952164546, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.7991887891001287, 'bagging_fraction': 0.800485989461228, 'bagging_freq': 1, 'n_estimators': 175}
    | [0m 15      [0m | [0m-0.1732  [0m | [0m 0.8005  [0m | [0m 1.022   [0m | [0m 0.7992  [0m | [0m 0.05225 [0m | [0m 19.49   [0m | [0m 175.2   [0m | [0m 38.61   [0m |
    =============================================================================================================
    


```python
# BayesianOptimization ê°ì²´ì˜ res ì†ì„±ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•˜ëŠ” ê³¼ì •ì—ì„œì˜ metric ê°’ê³¼ ê·¸ë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ. 
```


```python
#BO_rf.res 
```

BayesianOptimization ê°ì²´ì˜ max ì†ì„±ì€ ìµœê³  ë†’ì€ ì„±ëŠ¥ Metricë¥¼ ê°€ì§ˆë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ.


```python
BO_rf.max
# ì´ë–„ì— ìš°ë¦¬ëŠ” ì•„ë˜ ê°’ë“¤ì„ ë°˜ì˜¬ë¦¼ í•´ì„œ(int(round)) ì‚¬ìš©í–ˆìŒì„ ê¸°ì–µí•˜ì„¸ìš”!
# ì¦‰ max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 ì´ ë©ë‹ˆë‹¤. # ì´ëŠ” ì‹¤í–‰ë§ˆë‹¤ ë‹¬ë¼ì§€ë‹ˆê¹Œ ì£¼ì˜í•˜ì„¸ìš©
```




    {'target': -0.16873401068694988,
     'params': {'bagging_fraction': 0.8925919669634644,
      'bagging_freq': 2.5776214962985264,
      'feature_fraction': 0.9371506430319936,
      'learning_rate': 0.09375052437923304,
      'max_depth': 23.46023058263993,
      'n_estimators': 173.5539900082939,
      'num_leaves': 38.75618804607675}}



## ìµœì¢… í‰ê°€!


```python
max_params = BO_rf.max['params']
max_params['max_depth'] = int(round(max_params['max_depth']))
max_params['num_leaves'] = int(round(max_params['num_leaves']))
max_params['bagging_freq']=int(round(max_params['bagging_freq']))
max_params['n_estimators']=int(round(max_params['n_estimators']))
```


```python
lgb_model = LGBMRegressor(**max_params)
lgb_model.fit(X_train, y_train)
y_pred = lgb_model.predict(X_test)
score = mape(y_test,y_pred)

print('mape : {0:.4f}'.format(score))
```

    mape : 0.1692
    

## ë¹„êµëª¨ë¸


```python
lgb_model = LGBMRegressor()
lgb_model.fit(X_train, y_train) 
y_pred = lgb_model.predict(X_test)
score = mape(y_pred,y_test)

print('mape : {0:.4f}'.format(score))

# ì¡°ì •ê°’ì´ ë§ì•„ì„œ ê± DEFAULT ê°€ ì´ê¸´ëª¨ìŠµ... 
# ì˜ì™¸ë¡œ DEFAULT ì´ê¸°ê¸° ì‰½ì§€ì•Šì•„ìš”
```

    mape : 0.1600
    

# Light LGB ( ë¶„ë¥˜ )


```python
from sklearn.datasets import fetch_covtype
covtype = fetch_covtype()
```


```python
X = pd.DataFrame(covtype.data, 
                  columns=["x{:02d}".format(i + 1) for i in range(covtype.data.shape[1])],
                  dtype=int)
y = covtype.target
```


```python
X
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x01</th>
      <th>x02</th>
      <th>x03</th>
      <th>x04</th>
      <th>x05</th>
      <th>x06</th>
      <th>x07</th>
      <th>x08</th>
      <th>x09</th>
      <th>x10</th>
      <th>...</th>
      <th>x45</th>
      <th>x46</th>
      <th>x47</th>
      <th>x48</th>
      <th>x49</th>
      <th>x50</th>
      <th>x51</th>
      <th>x52</th>
      <th>x53</th>
      <th>x54</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2596</td>
      <td>51</td>
      <td>3</td>
      <td>258</td>
      <td>0</td>
      <td>510</td>
      <td>221</td>
      <td>232</td>
      <td>148</td>
      <td>6279</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2590</td>
      <td>56</td>
      <td>2</td>
      <td>212</td>
      <td>-6</td>
      <td>390</td>
      <td>220</td>
      <td>235</td>
      <td>151</td>
      <td>6225</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2804</td>
      <td>139</td>
      <td>9</td>
      <td>268</td>
      <td>65</td>
      <td>3180</td>
      <td>234</td>
      <td>238</td>
      <td>135</td>
      <td>6121</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2785</td>
      <td>155</td>
      <td>18</td>
      <td>242</td>
      <td>118</td>
      <td>3090</td>
      <td>238</td>
      <td>238</td>
      <td>122</td>
      <td>6211</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2595</td>
      <td>45</td>
      <td>2</td>
      <td>153</td>
      <td>-1</td>
      <td>391</td>
      <td>220</td>
      <td>234</td>
      <td>150</td>
      <td>6172</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>581007</th>
      <td>2396</td>
      <td>153</td>
      <td>20</td>
      <td>85</td>
      <td>17</td>
      <td>108</td>
      <td>240</td>
      <td>237</td>
      <td>118</td>
      <td>837</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581008</th>
      <td>2391</td>
      <td>152</td>
      <td>19</td>
      <td>67</td>
      <td>12</td>
      <td>95</td>
      <td>240</td>
      <td>237</td>
      <td>119</td>
      <td>845</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581009</th>
      <td>2386</td>
      <td>159</td>
      <td>17</td>
      <td>60</td>
      <td>7</td>
      <td>90</td>
      <td>236</td>
      <td>241</td>
      <td>130</td>
      <td>854</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581010</th>
      <td>2384</td>
      <td>170</td>
      <td>15</td>
      <td>60</td>
      <td>5</td>
      <td>90</td>
      <td>230</td>
      <td>245</td>
      <td>143</td>
      <td>864</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581011</th>
      <td>2383</td>
      <td>165</td>
      <td>13</td>
      <td>60</td>
      <td>4</td>
      <td>67</td>
      <td>231</td>
      <td>244</td>
      <td>141</td>
      <td>875</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>581012 rows Ã— 54 columns</p>
</div>




```python
X = X.iloc[0:5000,:]
y = y[0:5000]
```


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)
```

## íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •


```python
bayes_params = {
    'num_leaves': (24, 45), # ë²”ìœ„ê°’ìœ¼ë¡œ ì¸ì‹í•˜ê²Œ ëœë‹¤!
    'colsample_bytree':(0.5, 1),  
    'subsample': (0.5, 1),
    'max_depth': (4, 12),
    'reg_alpha': (0, 0.5),
    'reg_lambda': (0, 0.5), 
    'min_split_gain': (0.001, 0.1),
    'min_child_weight':(5, 50)
}
```

í…ŒìŠ¤íŠ¸ í•´ë³¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ ê°’ì„ ì„¤ì •í•˜ì˜€ìœ¼ë©´ BaysianOptimizationì—ì„œ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.

í•´ë‹¹ í•¨ìˆ˜ëŠ” BaysianOptimizationì—ì„œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ê¸° ìœ„í•´ í˜¸ì¶œë˜ë©´ ì œëŒ€ë¡œ íŠœë‹ì´ ë˜ê³  ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œ ëª¨ë¸ì„ í•™ìŠµ/í‰ê°€í•˜ê³  ì´ì— ë”°ë¥¸ í‰ê°€ ì§€í‘œë¥¼ ë°˜í™˜í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ì´ í‰ê°€ í•¨ìˆ˜ëŠ” BayesianOptimization ê°ì²´ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ë©´ì„œ í˜¸ì¶œë˜ë¯€ë¡œ í•¨ìˆ˜ì˜ ì¸ìë¡œ ì•ì—ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •ëœ íŒŒë¼ë¯¸í„°ë“¤ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.  

## í‰ê°€ í•¨ìˆ˜ ì •ì˜


```python
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

def lgb_acc_eval(num_leaves, 
                 colsample_bytree,
                 subsample, 
                 max_depth, 
                 reg_alpha, 
                 reg_lambda, 
                 min_split_gain, 
                 min_child_weight):
    # í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ê¸° ìœ„í•´ , ì´ê²Œ ì œëŒ€ë¡œ! í•™ìŠµì´ ë˜ê³ ìˆëŠ”ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ í•™ìŠµ/ í‰ê°€í•˜ê³  ì´ì— ë”°ë¥¸ í‰ê°€ ì§€í‘œë¥¼ ë°˜í™˜í•˜ëŠ” í˜•ì‹ì´ ëœë‹¤.
    params = {
        "n_estimator":200,
        "learning_rate":0.02,
        'num_leaves': int(round(num_leaves)), # ì´ ê°’ì€ ì •ìˆ˜í˜•ì„ return ë°›ì•„ì•¼ í•˜ë¯€ë¡œ! round/ int ë¥¼ ì°¨ë¡€ë¡œ ë°›ëŠ”ë‹¤.
        'colsample_bytree': colsample_bytree, 
        'subsample': subsample,
        'max_depth': int(round(max_depth)),
        'reg_alpha': reg_alpha,
        'reg_lambda': reg_lambda, 
        'min_split_gain': min_split_gain,
        'min_child_weight': min_child_weight,
        'verbosity': -1
    }
    print("params:", params)  # ì–´ ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€
    lgb_model = LGBMClassifier(**params) # ëª¨ë¸! 
    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='accuracy')
    
    result = np.mean(cv_value) # cv_value ëŠ” list í˜•íƒœë¡œ ë‚˜ì˜¤ê²Œ ë˜ë‹ˆê¹Œ!
    return result # ì´ result ê°’ì´ ì»¤ì§€ê²Œ ë² ì´ì§€ì•ˆoptimization ì´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

    print('accuracy :', result)  # ê·¸ ê°’ì„ ë„ì¶œ print í•´ì„œ ì˜ í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ (ì¤„ì—¬ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ) ì•Œ ì•„ ë³´ì•„ìš”~    return roc_preds
```

## Optimization


```python
from bayes_opt import BayesianOptimization
# ê°ì²´ë¥¼ í˜•ì„±í•œë‹¤
BO_lgb = BayesianOptimization(lgb_acc_eval, bayes_params, random_state=0)
```


```python
BO_lgb.maximize(init_points=5, n_iter=10)
```

    |   iter    |  target   | colsam... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |
    -------------------------------------------------------------------------------------------------------------------------
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 33, 'colsample_bytree': 0.7744067519636624, 'subsample': 0.9458865003910399, 'max_depth': 10, 'reg_alpha': 0.32294705653332806, 'reg_lambda': 0.21879360563134626, 'min_split_gain': 0.05494343511669279, 'min_child_weight': 32.12435192322397, 'verbosity': -1}
    | [0m 1       [0m | [0m 0.7729  [0m | [0m 0.7744  [0m | [0m 9.722   [0m | [0m 32.12   [0m | [0m 0.05494 [0m | [0m 32.9    [0m | [0m 0.3229  [0m | [0m 0.2188  [0m | [0m 0.9459  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 36, 'colsample_bytree': 0.9818313802505146, 'subsample': 0.5435646498507704, 'max_depth': 7, 'reg_alpha': 0.4627983191463305, 'reg_lambda': 0.03551802909894347, 'min_split_gain': 0.05336059705553755, 'min_child_weight': 40.627626713719906, 'verbosity': -1}
    | [0m 2       [0m | [0m 0.7649  [0m | [0m 0.9818  [0m | [0m 7.068   [0m | [0m 40.63   [0m | [0m 0.05336 [0m | [0m 35.93   [0m | [0m 0.4628  [0m | [0m 0.03552 [0m | [0m 0.5436  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5101091987201629, 'subsample': 0.8902645881432277, 'max_depth': 11, 'reg_alpha': 0.3995792821083618, 'reg_lambda': 0.23073968112646592, 'min_split_gain': 0.08713120267643511, 'min_child_weight': 40.01705379274327, 'verbosity': -1}
    | [0m 3       [0m | [0m 0.7594  [0m | [0m 0.5101  [0m | [0m 10.66   [0m | [0m 40.02   [0m | [0m 0.08713 [0m | [0m 44.55   [0m | [0m 0.3996  [0m | [0m 0.2307  [0m | [0m 0.8903  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 35, 'colsample_bytree': 0.5591372129344666, 'subsample': 0.8871168447171083, 'max_depth': 9, 'reg_alpha': 0.2073309699952618, 'reg_lambda': 0.13227780605231348, 'min_split_gain': 0.09452222278790881, 'min_child_weight': 11.450897933407088, 'verbosity': -1}
    | [95m 4       [0m | [95m 0.8066  [0m | [95m 0.5591  [0m | [95m 9.119   [0m | [95m 11.45   [0m | [95m 0.09452 [0m | [95m 34.96   [0m | [95m 0.2073  [0m | [95m 0.1323  [0m | [95m 0.8871  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.7280751661082743, 'subsample': 0.8409101495517417, 'max_depth': 9, 'reg_alpha': 0.30846699843737846, 'reg_lambda': 0.4718740392573121, 'min_split_gain': 0.062145914210511834, 'min_child_weight': 5.845541019635982, 'verbosity': -1}
    | [95m 5       [0m | [95m 0.8169  [0m | [95m 0.7281  [0m | [95m 8.547   [0m | [95m 5.846   [0m | [95m 0.06215 [0m | [95m 36.85   [0m | [95m 0.3085  [0m | [95m 0.4719  [0m | [95m 0.8409  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}
    | [0m 6       [0m | [0m 0.7671  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 5.0     [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.5     [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 6.153818666923531, 'verbosity': -1}
    | [95m 7       [0m | [95m 0.8274  [0m | [95m 0.5     [0m | [95m 12.0    [0m | [95m 6.154   [0m | [95m 0.1     [0m | [95m 45.0    [0m | [95m 0.0     [0m | [95m 0.0     [0m | [95m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}
    | [0m 8       [0m | [0m 0.7671  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 5.0     [0m | [0m 0.001   [0m | [0m 45.0    [0m | [0m 0.5     [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 41, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 9.282792883879118, 'verbosity': -1}
    | [0m 9       [0m | [0m 0.8174  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 9.283   [0m | [0m 0.1     [0m | [0m 40.95   [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 16.37016837765438, 'verbosity': -1}
    | [0m 10      [0m | [0m 0.8006  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 16.37   [0m | [0m 0.1     [0m | [0m 45.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 31, 'colsample_bytree': 0.9963415847423831, 'subsample': 0.8576257010583855, 'max_depth': 12, 'reg_alpha': 0.009330048227644716, 'reg_lambda': 0.019604314288185376, 'min_split_gain': 0.04155948070314624, 'min_child_weight': 5.033169891354441, 'verbosity': -1}
    | [0m 11      [0m | [0m 0.8266  [0m | [0m 0.9963  [0m | [0m 11.82   [0m | [0m 5.033   [0m | [0m 0.04156 [0m | [0m 31.37   [0m | [0m 0.00933 [0m | [0m 0.0196  [0m | [0m 0.8576  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.28278185068333334, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 50.0, 'verbosity': -1}
    | [0m 12      [0m | [0m 0.7357  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 50.0    [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.2828  [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.001, 'min_child_weight': 18.488156702160605, 'verbosity': -1}
    | [0m 13      [0m | [0m 0.7926  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 18.49   [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.8958351898111097, 'subsample': 0.8153907933202891, 'max_depth': 12, 'reg_alpha': 0.019824024962597386, 'reg_lambda': 0.27977114529274616, 'min_split_gain': 0.02168998981760603, 'min_child_weight': 5.111223910488313, 'verbosity': -1}
    | [0m 14      [0m | [0m 0.8226  [0m | [0m 0.8958  [0m | [0m 11.91   [0m | [0m 5.111   [0m | [0m 0.02169 [0m | [0m 36.83   [0m | [0m 0.01982 [0m | [0m 0.2798  [0m | [0m 0.8154  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 1.0, 'max_depth': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 26.679596508578747, 'verbosity': -1}
    | [0m 15      [0m | [0m 0.7477  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 26.68   [0m | [0m 0.1     [0m | [0m 24.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    =========================================================================================================================
    


```python
# BayesianOptimization ê°ì²´ì˜ res ì†ì„±ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•˜ëŠ” ê³¼ì •ì—ì„œì˜ metric ê°’ê³¼ ê·¸ë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ. 
```


```python
BO_lgb.res
```




    [{'target': 0.7728571428571429,
      'params': {'colsample_bytree': 0.7744067519636624,
       'max_depth': 9.721514930979357,
       'min_child_weight': 32.12435192322397,
       'min_split_gain': 0.05494343511669279,
       'num_leaves': 32.896750786116996,
       'reg_alpha': 0.32294705653332806,
       'reg_lambda': 0.21879360563134626,
       'subsample': 0.9458865003910399}},
     {'target': 0.7648571428571429,
      'params': {'colsample_bytree': 0.9818313802505146,
       'max_depth': 7.067532150606222,
       'min_child_weight': 40.627626713719906,
       'min_split_gain': 0.05336059705553755,
       'num_leaves': 35.92893578297258,
       'reg_alpha': 0.4627983191463305,
       'reg_lambda': 0.03551802909894347,
       'subsample': 0.5435646498507704}},
     {'target': 0.7594285714285715,
      'params': {'colsample_bytree': 0.5101091987201629,
       'max_depth': 10.660958764383505,
       'min_child_weight': 40.01705379274327,
       'min_split_gain': 0.08713120267643511,
       'num_leaves': 44.55098518688804,
       'reg_alpha': 0.3995792821083618,
       'reg_lambda': 0.23073968112646592,
       'subsample': 0.8902645881432277}},
     {'target': 0.8065714285714286,
      'params': {'colsample_bytree': 0.5591372129344666,
       'max_depth': 9.119368170620191,
       'min_child_weight': 11.450897933407088,
       'min_split_gain': 0.09452222278790881,
       'num_leaves': 34.95881475675151,
       'reg_alpha': 0.2073309699952618,
       'reg_lambda': 0.13227780605231348,
       'subsample': 0.8871168447171083}},
     {'target': 0.816857142857143,
      'params': {'colsample_bytree': 0.7280751661082743,
       'max_depth': 8.547471590949188,
       'min_child_weight': 5.845541019635982,
       'min_split_gain': 0.062145914210511834,
       'num_leaves': 36.85401017717085,
       'reg_alpha': 0.30846699843737846,
       'reg_lambda': 0.4718740392573121,
       'subsample': 0.8409101495517417}},
     {'target': 0.7671428571428571,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 5.0,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.5,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.8274285714285714,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 6.153818666923531,
       'min_split_gain': 0.1,
       'num_leaves': 45.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.7671428571428571,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 5.0,
       'min_split_gain': 0.001,
       'num_leaves': 45.0,
       'reg_alpha': 0.5,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.8174285714285714,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 9.282792883879118,
       'min_split_gain': 0.1,
       'num_leaves': 40.95446342173249,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.8005714285714285,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 16.37016837765438,
       'min_split_gain': 0.1,
       'num_leaves': 45.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.8265714285714285,
      'params': {'colsample_bytree': 0.9963415847423831,
       'max_depth': 11.819684742565805,
       'min_child_weight': 5.033169891354441,
       'min_split_gain': 0.04155948070314624,
       'num_leaves': 31.371377406180006,
       'reg_alpha': 0.009330048227644716,
       'reg_lambda': 0.019604314288185376,
       'subsample': 0.8576257010583855}},
     {'target': 0.7357142857142858,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 50.0,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.28278185068333334,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.7925714285714286,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 18.488156702160605,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 0.5}},
     {'target': 0.8225714285714286,
      'params': {'colsample_bytree': 0.8958351898111097,
       'max_depth': 11.912363935718016,
       'min_child_weight': 5.111223910488313,
       'min_split_gain': 0.02168998981760603,
       'num_leaves': 36.82863912155457,
       'reg_alpha': 0.019824024962597386,
       'reg_lambda': 0.27977114529274616,
       'subsample': 0.8153907933202891}},
     {'target': 0.7477142857142857,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 26.679596508578747,
       'min_split_gain': 0.1,
       'num_leaves': 24.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}}]



BayesianOptimization ê°ì²´ì˜ max ì†ì„±ì€ ìµœê³  ë†’ì€ ì„±ëŠ¥ Metricë¥¼ ê°€ì§ˆë•Œì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ ê°€ì§€ê³  ìˆìŒ.


```python
BO_lgb.max
```




    {'target': 0.8274285714285714,
     'params': {'colsample_bytree': 0.5,
      'max_depth': 12.0,
      'min_child_weight': 6.153818666923531,
      'min_split_gain': 0.1,
      'num_leaves': 45.0,
      'reg_alpha': 0.0,
      'reg_lambda': 0.0,
      'subsample': 1.0}}



## ìµœì¢… í‰ê°€


```python
from sklearn.metrics import accuracy_score

max_params = BO_rf.max['params']
max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))
max_params['min_samples_split'] = int(round(max_params['min_samples_split']))
max_params['max_depth'] = int(max_params['max_depth'])
lgb_model = LGBMClassifier(n_estimators=200,learning_rate=0.2, **max_params)
lgb_model.fit(X_train, y_train) # train ì„ ì „ì²´ë¡œ í•´ì„œ ë” ë†’ì€ê°€ë°”..
y_pred = lgb_model.predict(X_test)
score = accuracy_score(y_pred,y_test)

print('accuracy : {0:.4f}'.format(score))
```

    mse score : 0.8713
    

# Catboost
