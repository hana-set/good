---
title:  "2.Bayes Optimization"
excerpt: "베이즈 최적화에 대해서 알아봅시다."
categories:
  - Py_Analysis
tags:
  - 기초
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
---

# How to Optimize

1. 우선 각 hyperparameter 의 의미를 파악한다. 

2. regularization 관련 / learning rate 관련 / tree 수 관련 등의 변수를 구분짓는다.

3. default 설정에서의 loss 를 관찰하고 그것을 기준으로 삼는다.

4. 각 값들을 넣어보면서 default 보다 좋은설정이 언제 나오는지 관찰한다. 이때에 큰 범주로 나눈 값들은 같이 변경하도록 한다.

    ex) regulerization 을 넣어보고싶은 경우 관련 변수를 같이 조절하고, 비교가 되는 모델에서는 regularization 을 쓰지 않는다. 

    ex) reg 를 넣었을떄 좋은 값이 나왔다면 그 경우 reg 가 부족한것이므로 reg 를 더 올리면서 확인해본다..... 처럼 의미가 비슷한거끼리는 같이 영향이 상쇄되지 않게 조절해줍시다.

5. 4번에서 어느정도 안정화가 되면, 그 값들을 범위를 설정하고, 베이지안 optimization 을 실행해본다. 


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics # 모델평가시 이용
```


```python
from sklearn.datasets import fetch_california_housing
import pandas as pd
california = fetch_california_housing()
X = pd.DataFrame(california.data, columns=california.feature_names)
y = pd.DataFrame(california.target,columns=["Target"])
df = pd.concat([X, y], axis=1)
df.tail()
y = california.target
```


```python
# dataset train/test set 으로 나누기
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```

# Random Forest (회귀)

## 파라미터 범위 설정


```python
bayes_params = {
    'min_samples_split' : (2,100),
    'min_samples_leaf' : (1,100),
    'max_depth': (1, 10),}
```


```python
from sklearn.ensemble import RandomForestRegressor
rf_model=RandomForestRegressor()
```

## 평가 함수 정의


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
import numpy as np

# 우선 우리가 tuning 하고 싶은 값들을 받고, 그에따라서 tuning 이 되게 조절해봅시다.
# 우선 max_depth, min_samples_split, min_samples_leaf 를 이용한다고 해 봅시다.
def ran_mse_eval(max_depth,
                min_samples_split,
                min_samples_leaf,):
    # 여기에서는 우리가 조절해야할 파라미터들을 dic 형태로 정의합니다.
    params = {
        "n_estimators": 100 , # 굳이 parameter grid 로 찾고싶지는 않지만 ,default 값이 아니라 다른값을 주고싶을떄 이렇게 200으로 고정해서 하고싶다고 합시다!
        'max_depth': int(round(max_depth)), # 이 때에 정수값이 들어가야하는 경우가 있는데요, 그 떄에는 int(round 로 처리를 해주어야 합니다.
        'min_samples_split' : int(round(min_samples_split)),
        'min_samples_leaf' : int(round(min_samples_leaf))}
    print("params:", params)  # 어떤 파라미터를 사용하였는지 print 하게 해서 학습과정을 지켜보도록 해요~
    rf_model = RandomForestRegressor(**params) # 우리의 모델을 정의합니다.
    
    # 여기서 result 를 뽑아내기 위해서는 2가지 경우가 있습니다! 
    
    # 1. y_val/ x_val 을 나누어서 평가하는 경우
    #rf_model.fit(X_train, y_train) # 먼저 train 에 학습시킨 후
    #valid_proba = rf_model.predict(X_val) # X_val 에 대해서 예측하고
    #result = -1 * mean_squared_error(y_val, valid_proba) # 그에 따른 mse 스코어를 구합니다.
    
    # 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서 
    # 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다! 
    cv_value = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error') # 이떄 scoring 은 주최측에서 정한 값을 넣어야 게엣죠오?
    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!
    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.

    print('mse:', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~
```

## Optimization

BayesianOptimization 객체를 생성합니다. 
이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다.


```python
from bayes_opt import BayesianOptimization
# 객체를 형성한다
BO_rf = BayesianOptimization(ran_mse_eval, bayes_params, random_state=0)
```

이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. 
BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. 


```python
BO_rf.maximize(init_points=1, n_iter=3) 
# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.
# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!
# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것.
```

    |   iter    |  target   | max_depth | min_sa... | min_sa... |
    -------------------------------------------------------------
    params: {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 54, 'min_samples_leaf': 79}
    | [0m 3       [0m | [0m-0.5063  [0m | [0m 4.451   [0m | [0m 79.38   [0m | [0m 53.83   [0m |
    params: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 93}
    | [0m 4       [0m | [0m-0.4211  [0m | [0m 6.112   [0m | [0m 92.63   [0m | [0m 8.962   [0m |
    params: {'n_estimators': 100, 'max_depth': 2, 'min_samples_split': 84, 'min_samples_leaf': 3}
    | [0m 5       [0m | [0m-0.7096  [0m | [0m 1.784   [0m | [0m 3.002   [0m | [0m 83.6    [0m |
    params: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 79, 'min_samples_leaf': 61}
    | [95m 6       [0m | [95m-0.3624  [0m | [95m 10.0    [0m | [95m 60.51   [0m | [95m 78.68   [0m |
    params: {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 40, 'min_samples_leaf': 96}
    | [0m 7       [0m | [0m-0.5817  [0m | [0m 3.073   [0m | [0m 95.79   [0m | [0m 39.56   [0m |
    params: {'n_estimators': 100, 'max_depth': 1, 'min_samples_split': 100, 'min_samples_leaf': 85}
    | [0m 8       [0m | [0m-0.8864  [0m | [0m 1.0     [0m | [0m 84.85   [0m | [0m 100.0   [0m |
    =============================================================
    


```python
# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. 
```


```python
#BO_rf.res 
```

BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음.


```python
BO_rf.max
# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!
# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용
```




    {'target': -0.3624136969982552,
     'params': {'max_depth': 10.0,
      'min_samples_leaf': 60.506031279081775,
      'min_samples_split': 78.68476690496574}}



## 최종 평가!


```python
from sklearn.metrics import mean_squared_error

max_params = BO_rf.max['params']
max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))
max_params['min_samples_split'] = int(round(max_params['min_samples_split']))
model_rf = RandomForestRegressor(n_estimators=100, **max_params)
model_rf.fit(X_train, y_train)
y_pred = model_rf.predict(X_test)
score = mean_squared_error(y_pred,y_test)

print('mse score : {0:.4f}'.format(score))
```

    mse score : 0.3655
    

## 비교 모델

- 일반적으로 그냥 search 보다는 우수한 성능을 보여줍니다
- 다만! 여기에서는 그냥 iter 을 극단적으로 낮추어서 randomize 가 이긴것처럼 보이나 원래는 베이즈가 이깁니다.
- Colab 에서 돌리는것을 추천드립니다.


```python
from sklearn.ensemble import RandomForestRegressor
model =  RandomForestRegressor(n_estimators=100)
# scoring 은 default 이므로 model 의 자체 scoring 으로 들어간다. 
# n_estimator = 500 클수록 좋으나 내 컴퓨터가 버티질 못할듯.
model.fit(X_train,y_train)
```




    RandomForestRegressor()




```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 0.26241141407302704
    R_squared : 0.7987572932106815
    

# XGBboost (회귀)

2-1. 일반 파라메터


(중요)booster: 어떤 부스터 구조를 쓸지 결정한다. 이것은 gbtree, gblinear, dart가 있다.

(중요)n_eastimator : This is how many subtrees h will be trained.

nthread: 몇 개의 쓰레드를 동시에 처리하도록 할지 결정한다. 디폴트는 “가능한 한 많이”.

num_feature: feature 차원의 숫자를 정해야 하는 경우 옵션을 세팅한다. 디폴트는 “가능한 한 많이.”


2-2. 부스팅 파라메터


(중요)learning_rate: learning rate다. 트리에 가지가 많을수록 과적합하기 쉽다. 각 트리마다 가중치를 주어 부스팅 과정에 과적합이 일어나지 않도록 한다

gamma: 정보 획득(Information Gain)에서 -r로 표현한 바 있다. 이것이 커지면, 트리 깊이가 줄어들어 보수적인 모델이 된다. 디폴트 값은 0이다

(중요)max_depth: 한 트리의 maximum depth. 숫자를 키울수록 모델의 복잡도가 커진다. 작을수록 과적합 방지 . 디폴트는 6. 

lambda(L2 reg-form): L2 Regularization Form에 달리는 weights이다. 숫자가 클수록 보수적인 모델이 된다

(중요)alpha(L1 reg-form): L1 Regularization Form weights다. 숫자가 클수록 과적합 방지


2-3. 학습 과정 파라메터

objective: 목적 함수다. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) 등 다양하다

eval_metric: 모델의 평가 함수를 조정하는 함수다. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) 등, 해당 데이터의 특성에 맞게 평가 함수를 조정한다



2-4. 커맨드 라인 파라메터
num_rounds: boosting 라운드를 결정한다. 랜덤 하게 생성되는 모델이니만큼 이 수가 적당히 큰 게 좋다. epoch 옵션과 동일하다

내 생각에 tuning 할때에는 중요 하다고 생각되는 것들을 신경쓰는게 좋아보인다.
- 1. booster : 부스터 구조를 잘 골라서 잘 나오는것을 우선 선별한다.
- 2. n_eastimator : 이 경우도 클수록 tree 가 많아져서 학습이 느려진다. 미리 선별해서 기억하는게 좋을듯하다.
- 3. learning rate(트리간 가중치) / maxdepth(트리의 구조) / alpha(트리의 leaf 제한) 세개를 적절히 써서 잘 조절한다.
- 4. 이제 어느정도 확정된 값을 가지고 어떻게 조절할지 감을 잡은 후에 colab 으로 Baysian 을 돌리면 될거같다!


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
XGBRegressor()
```




    XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,
                 colsample_bynode=None, colsample_bytree=None, gamma=None,
                 gpu_id=None, importance_type='gain', interaction_constraints=None,
                 learning_rate=None, max_delta_step=None, max_depth=None,
                 min_child_weight=None, missing=nan, monotone_constraints=None,
                 n_estimators=100, n_jobs=None, num_parallel_tree=None,
                 random_state=None, reg_alpha=None, reg_lambda=None,
                 scale_pos_weight=None, subsample=None, tree_method=None,
                 validate_parameters=None, verbosity=None)



## 파라미터 범위설정


```python
bayes_params = {'n_estimators' : (100,300), # 트리의 갯수
                'learning_rate' : (0.05,0.15), # learning rate
                'max_depth': (9, 14), # 커질수록 복잡한 모델 (과적합 방지)
                'reg_alpha': (0, 0.2), # leaves 에 대한 l1 규제의 계수이므로 클수록 규제
                'colsample_bytree' :(0.85, 1.0)} # tree 에 사용되는 변수갯수 지정. 
```

## 평가 custom loss 정의


```python
# 어떤 경우는 우리가 평가하고자 하는 loss 가 없을 수 있다.
```


```python
from sklearn.metrics import make_scorer
import numpy as np

def mape(y_true, y_predict):
    # Note this blows up if y_true = 0
    # Ignore for demo -- in some sense an unsolvable
    # problem with MAPE as an error metric 
    # 하지만 y_true 가 0 인경우는 없으므로 안심하라구!
    y_true = np.array(y_true)
    y_predict = np.array(y_predict)
    return np.abs((y_true - y_predict)/y_true).mean()

mape_scorer = make_scorer(mape, greater_is_better=False) 
# greater is better 이 false 이므로 neg 값이 나오게 된다. 
```

## 평가 함수 정의


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# 우선 우리가 tuning 하고 싶은 값들을 받고, 그에따라서 tuning 이 되게 조절해봅시다.
# 우선 max_depth, min_samples_split, min_samples_leaf 를 이용한다고 해 봅시다.
def ran_mape_eval(n_estimators,
                  learning_rate,
                 max_depth,
                 reg_alpha,
                 colsample_bytree):
    # 여기에서는 우리가 조절해야할 파라미터들을 dic 형태로 정의합니다.
    params = {
        "n_estimators": int(round(n_estimators)) ,
        'learning_rate': learning_rate,
        'max_depth': int(round(max_depth)), # 이 때에 정수값이 들어가야하는 경우가 있는데요, 그 떄에는 int(round 로 처리를 해주어야 합니다.
        'reg_alpha' : reg_alpha,
        'colsample_bytree' : colsample_bytree}
    print("params:", params)  # 어떤 파라미터를 사용하였는지 print 하게 해서 학습과정을 지켜보도록 해요~
    xgb_model = XGBRegressor(**params) # 우리의 모델을 정의합니다.
    
    # 여기서 result 를 뽑아내기 위해서는 2가지 경우가 있습니다! 
    
    # 1. y_val/ x_val 을 나누어서 평가하는 경우
    #rf_model.fit(X_train, y_train) # 먼저 train 에 학습시킨 후
    #valid_proba = rf_model.predict(X_val) # X_val 에 대해서 예측하고
    #result = -1 * mean_squared_error(y_val, valid_proba) # 그에 따른 mse 스코어를 구합니다.
    
    # 하지만 위 같은 경우.. 뭔가 결과가 y_val/ x_val 에 과적합될 수도 있을거같아서 
    # 2. 그냥 X_train/ y_train 의 cv 값을 하게되면 더 좋아보인다! 
    cv_value = cross_val_score(xgb_model, X_train, y_train, cv=3, scoring = mape_scorer) # 이떄 scoring 은 우리가 위에서 정의한 평가함수를 썻다.
    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!
    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.

    print('mape:', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~
```

## Optimization

BayesianOptimization 객체를 생성합니다. 
이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다.


```python
from bayes_opt import BayesianOptimization
# 객체를 형성한다
BO_rf = BayesianOptimization(ran_mape_eval, bayes_params, random_state=0)
```

이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. 
BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. 


```python
BO_rf.maximize(init_points=2, n_iter=3) 
# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.
# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!
# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것.
```

    |   iter    |  target   | colsam... | learni... | max_depth | n_esti... | reg_alpha |
    -------------------------------------------------------------------------------------
    params: {'n_estimators': 209, 'learning_rate': 0.12151893663724195, 'max_depth': 12, 'reg_alpha': 0.08473095986778095, 'colsample_bytree': 0.9323220255890987}
    | [0m 1       [0m | [0m-0.1702  [0m | [0m 0.9323  [0m | [0m 0.1215  [0m | [0m 12.01   [0m | [0m 209.0   [0m | [0m 0.08473 [0m |
    params: {'n_estimators': 293, 'learning_rate': 0.09375872112626925, 'max_depth': 13, 'reg_alpha': 0.07668830376515555, 'colsample_bytree': 0.9468841169599984}
    | [95m 2       [0m | [95m-0.1696  [0m | [95m 0.9469  [0m | [95m 0.09376 [0m | [95m 13.46   [0m | [95m 292.7   [0m | [95m 0.07669 [0m |
    params: {'n_estimators': 291, 'learning_rate': 0.09738089297908531, 'max_depth': 13, 'reg_alpha': 0.05391212288356184, 'colsample_bytree': 0.8568444223026506}
    | [95m 3       [0m | [95m-0.1695  [0m | [95m 0.8568  [0m | [95m 0.09738 [0m | [95m 13.31   [0m | [95m 291.4   [0m | [95m 0.05391 [0m |
    params: {'n_estimators': 285, 'learning_rate': 0.10203465161914638, 'max_depth': 13, 'reg_alpha': 0.07549368916672952, 'colsample_bytree': 0.9450782992887754}
    | [0m 4       [0m | [0m-0.1707  [0m | [0m 0.9451  [0m | [0m 0.102   [0m | [0m 12.79   [0m | [0m 285.0   [0m | [0m 0.07549 [0m |
    params: {'n_estimators': 292, 'learning_rate': 0.05323332238814628, 'max_depth': 10, 'reg_alpha': 0.17997766062089499, 'colsample_bytree': 0.8757474751378177}
    | [95m 5       [0m | [95m-0.1667  [0m | [95m 0.8757  [0m | [95m 0.05323 [0m | [95m 9.582   [0m | [95m 292.0   [0m | [95m 0.18    [0m |
    =====================================================================================
    


```python
# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. 
```


```python
#BO_rf.res 
```

BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음.


```python
BO_rf.max
# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!
# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용
```




    {'target': -0.16671083283960844,
     'params': {'colsample_bytree': 0.8757474751378177,
      'learning_rate': 0.05323332238814628,
      'max_depth': 9.582347007327694,
      'n_estimators': 291.95980137914466,
      'reg_alpha': 0.17997766062089499}}



## 최종 평가!


```python
max_params = BO_rf.max['params']
max_params['max_depth'] = int(round(max_params['max_depth']))
max_params['n_estimators'] = int(round(max_params['n_estimators']))

model_xgb = XGBRegressor(**max_params)
model_xgb.fit(X_train, y_train)
y_pred = model_xgb.predict(X_test)
score = mape(y_pred,y_test)

print('mape score : {0:.4f}'.format(score))
```

    mape score : 0.1466
    

## 비교모델


```python
from sklearn.ensemble import RandomForestRegressor
model = XGBRegressor()
model.fit(X_train,y_train)
```




    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                 colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
                 importance_type='gain', interaction_constraints='',
                 learning_rate=0.300000012, max_delta_step=0, max_depth=6,
                 min_child_weight=1, missing=nan, monotone_constraints='()',
                 n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,
                 tree_method='exact', validate_parameters=1, verbosity=None)




```python
y_pred = model.predict(X_test)
print ("MAPE :", mape(y_test, y_pred))
```

    MAPE : 0.17614545366526785
    

# Light LGB(회귀)

## 파라미터 범위설정


```python
bayes_params ={ 
    'learning_rate': (0.05,0.2),
    'n_estimators' : (50,250),
    'max_depth': (12,24), # 트리의 깊이
    'num_leaves': (30,40), 
    'feature_fraction': (0.7,0.99), # 트리를 만들떄 얼마나 feaure 를 랜덤으로 선택할지
    'bagging_fraction': (0.6,0.9), # 배깅의 비율
    'bagging_freq':(1,3)} # 베깅을 얼마나 진행할지 
```

## 평가 custom loss 정의


```python
# 어떤 경우는 우리가 평가하고자 하는 loss 가 없을 수 있다.
```


```python
from sklearn.metrics import make_scorer
import numpy as np

def mape(y_true, y_predict):
    # Note this blows up if y_true = 0
    # Ignore for demo -- in some sense an unsolvable
    # problem with MAPE as an error metric 
    # 하지만 y_true 가 0 인경우는 없으므로 안심하라구!
    y_true = np.array(y_true)
    y_predict = np.array(y_predict)
    return np.abs((y_true - y_predict)/y_true).mean()

mape_scorer = make_scorer(mape, greater_is_better=False) 
# greater is better 이 false 이므로 neg 값이 나오게 된다. 
# 즉 MAPE SCORE 는 -mape 가 나와요~
```

## 평가 함수 정의


```python
#만일 shuffle 이 제대로 안되어있어서 걱정된다면 이렇게 k fold 로 shuffle 시켜주세요 ^^
#from sklearn.model_selection import KFold
#kfold = KFold(n_splits=5, shuffle=True, random_state=2)
#cross_val_score(lgb_model, X_train, y_train, cv=kfold, scoring = mape_scorer)
```


```python
from lightgbm import LGBMRegressor
import lightgbm as lgb
from sklearn.model_selection import cross_val_score

def lgb_mape_eval(learning_rate,
                  n_estimators,
                 max_depth,
                 num_leaves,
                 feature_fraction,
                 bagging_fraction,
                 bagging_freq,
                 ):
    # 하이퍼 파라미터를 튜닝하기 위해 , 이게 제대로! 학습이 되고있는지 판단하기 위해 모델을 학습/ 평가하고 이에 따른 평가 지표를 반환하는 형식이 된다.
    params = {
        'learning_rate': learning_rate,
        'max_depth': int(round(max_depth)), # 트리의 깊이
        'num_leaves': int(round(num_leaves)), 
        'feature_fraction': feature_fraction, # 트리를 만들떄 얼마나 feaure 를 랜덤으로 선택할지
        'bagging_fraction': bagging_fraction, # 배깅의 비율
        'bagging_freq': int(round(bagging_freq)), # 베깅을 얼마나 진행할지
        'n_estimators' : int(round(n_estimators))
    }    

    print("params:", params)  # 어 떤 파라미터를 사용하였는지
    lgb_model = LGBMRegressor(**params) # 모델! 
    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring = mape_scorer)
    
    result = np.mean(cv_value) 
    # cv_value 는 list 형태로 나오게 됩니다. (각 cv 마다의 score 값.)
    return result 
    # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.

    print('accuracy :', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~    return roc_preds
```

## Optimization

BayesianOptimization 객체를 생성합니다. 
이때 생성 인자로 앞에서 만든 평가함수 lgb_roc_eval 함수와 튜닝할 하이퍼 파라미터의 범위값을 설정한 딕셔너리 변수인 bayes_params를 입력합니다.


```python
from bayes_opt import BayesianOptimization
# 객체를 형성한다
BO_rf = BayesianOptimization(lgb_mape_eval, bayes_params, random_state=0)
```

이제 입력받은 평가함수에 튜닝할 하이퍼 파라미터의 값을 반복적으로 입력하여 최적 하이퍼 파라미터를 튜닝할 준비가 되었습니다. 
BayesianOptimization객체에서 maximize()메소드를 호출하면 이를 수행할 수 있습니다. 


```python
BO_rf.maximize(init_points=5, n_iter=10) 
# 우리의 object function 을 maximize! 하려 한다. 그래서 위에서 return 을 negative mse 를 쓴 것이다.
# 최적 파라미터 도출 작업을 n_iter 만큼 반복하여 수행합니다!
# init_points = 첫 시작지점의 score 를 5개 돌려서 알아보는것.
```

    |   iter    |  target   | baggin... | baggin... | featur... | learni... | max_depth | n_esti... | num_le... |
    -------------------------------------------------------------------------------------------------------------
    params: {'learning_rate': 0.13173247744953454, 'max_depth': 17, 'num_leaves': 34, 'feature_fraction': 0.8748013790607767, 'bagging_fraction': 0.7646440511781974, 'bagging_freq': 2, 'n_estimators': 179}
    | [0m 1       [0m | [0m-0.1706  [0m | [0m 0.7646  [0m | [0m 2.43    [0m | [0m 0.8748  [0m | [0m 0.1317  [0m | [0m 17.08   [0m | [0m 179.2   [0m | [0m 34.38   [0m |
    params: {'learning_rate': 0.1687587557123997, 'max_depth': 18, 'num_leaves': 39, 'feature_fraction': 0.8111980404594755, 'bagging_fraction': 0.867531900234624, 'bagging_freq': 3, 'n_estimators': 164}
    | [95m 2       [0m | [95m-0.1704  [0m | [95m 0.8675  [0m | [95m 2.927   [0m | [95m 0.8112  [0m | [95m 0.1688  [0m | [95m 18.35   [0m | [95m 163.6   [0m | [95m 39.26   [0m |
    params: {'learning_rate': 0.17489297683219074, 'max_depth': 21, 'num_leaves': 40, 'feature_fraction': 0.7058633352576944, 'bagging_fraction': 0.6213108174593661, 'bagging_freq': 1, 'n_estimators': 224}
    | [0m 3       [0m | [0m-0.1775  [0m | [0m 0.6213  [0m | [0m 1.174   [0m | [0m 0.7059  [0m | [0m 0.1749  [0m | [0m 21.34   [0m | [0m 224.0   [0m | [0m 39.79   [0m |
    params: {'learning_rate': 0.06774116388033999, 'max_depth': 20, 'num_leaves': 39, 'feature_fraction': 0.926353461123072, 'bagging_fraction': 0.8397475692650171, 'bagging_freq': 2, 'n_estimators': 79}
    | [0m 4       [0m | [0m-0.1822  [0m | [0m 0.8397  [0m | [0m 1.923   [0m | [0m 0.9264  [0m | [0m 0.06774 [0m | [0m 19.68   [0m | [0m 78.67   [0m | [0m 39.45   [0m |
    params: {'learning_rate': 0.16613505341513252, 'max_depth': 17, 'num_leaves': 30, 'feature_fraction': 0.7767211275103418, 'bagging_fraction': 0.7565544965250215, 'bagging_freq': 2, 'n_estimators': 164}
    | [0m 5       [0m | [0m-0.173   [0m | [0m 0.7566  [0m | [0m 1.829   [0m | [0m 0.7767  [0m | [0m 0.1661  [0m | [0m 17.47   [0m | [0m 163.7   [0m | [0m 30.19   [0m |
    params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.99, 'bagging_fraction': 0.6, 'bagging_freq': 3, 'n_estimators': 135}
    | [0m 6       [0m | [0m-0.1795  [0m | [0m 0.6     [0m | [0m 3.0     [0m | [0m 0.99    [0m | [0m 0.05    [0m | [0m 24.0    [0m | [0m 134.6   [0m | [0m 40.0    [0m |
    params: {'learning_rate': 0.05, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'bagging_freq': 3, 'n_estimators': 193}
    | [0m 7       [0m | [0m-0.1712  [0m | [0m 0.9     [0m | [0m 3.0     [0m | [0m 0.7     [0m | [0m 0.05    [0m | [0m 24.0    [0m | [0m 192.6   [0m | [0m 40.0    [0m |
    params: {'learning_rate': 0.12472102385409382, 'max_depth': 24, 'num_leaves': 40, 'feature_fraction': 0.7720763961650281, 'bagging_fraction': 0.8314376373392335, 'bagging_freq': 3, 'n_estimators': 173}
    | [95m 8       [0m | [95m-0.1695  [0m | [95m 0.8314  [0m | [95m 2.981   [0m | [95m 0.7721  [0m | [95m 0.1247  [0m | [95m 23.5    [0m | [95m 173.0   [0m | [95m 39.77   [0m |
    params: {'learning_rate': 0.09375052437923304, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.9371506430319936, 'bagging_fraction': 0.8925919669634644, 'bagging_freq': 3, 'n_estimators': 174}
    | [95m 9       [0m | [95m-0.1687  [0m | [95m 0.8926  [0m | [95m 2.578   [0m | [95m 0.9372  [0m | [95m 0.09375 [0m | [95m 23.46   [0m | [95m 173.6   [0m | [95m 38.76   [0m |
    params: {'learning_rate': 0.05595332997717015, 'max_depth': 24, 'num_leaves': 30, 'feature_fraction': 0.7732915429010659, 'bagging_fraction': 0.818689843828635, 'bagging_freq': 1, 'n_estimators': 184}
    | [0m 10      [0m | [0m-0.1753  [0m | [0m 0.8187  [0m | [0m 1.12    [0m | [0m 0.7733  [0m | [0m 0.05595 [0m | [0m 23.97   [0m | [0m 184.4   [0m | [0m 30.21   [0m |
    params: {'learning_rate': 0.050459114886174466, 'max_depth': 12, 'num_leaves': 39, 'feature_fraction': 0.9736076245608813, 'bagging_fraction': 0.813305748333056, 'bagging_freq': 1, 'n_estimators': 172}
    | [0m 11      [0m | [0m-0.1752  [0m | [0m 0.8133  [0m | [0m 1.39    [0m | [0m 0.9736  [0m | [0m 0.05046 [0m | [0m 12.07   [0m | [0m 172.0   [0m | [0m 38.9    [0m |
    params: {'learning_rate': 0.06257526977942529, 'max_depth': 24, 'num_leaves': 33, 'feature_fraction': 0.7878086998962229, 'bagging_fraction': 0.677221679288176, 'bagging_freq': 2, 'n_estimators': 171}
    | [0m 12      [0m | [0m-0.1744  [0m | [0m 0.6772  [0m | [0m 2.395   [0m | [0m 0.7878  [0m | [0m 0.06258 [0m | [0m 23.75   [0m | [0m 170.8   [0m | [0m 32.61   [0m |
    params: {'learning_rate': 0.1046953001301889, 'max_depth': 23, 'num_leaves': 39, 'feature_fraction': 0.844141337224133, 'bagging_fraction': 0.7062916336088501, 'bagging_freq': 1, 'n_estimators': 180}
    | [0m 13      [0m | [0m-0.1715  [0m | [0m 0.7063  [0m | [0m 1.177   [0m | [0m 0.8441  [0m | [0m 0.1047  [0m | [0m 22.76   [0m | [0m 179.5   [0m | [0m 39.23   [0m |
    params: {'learning_rate': 0.06719992961699134, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.9506562232174386, 'bagging_fraction': 0.7142613973761445, 'bagging_freq': 2, 'n_estimators': 156}
    | [0m 14      [0m | [0m-0.1738  [0m | [0m 0.7143  [0m | [0m 1.943   [0m | [0m 0.9507  [0m | [0m 0.0672  [0m | [0m 19.44   [0m | [0m 156.0   [0m | [0m 38.87   [0m |
    params: {'learning_rate': 0.052247126952164546, 'max_depth': 19, 'num_leaves': 39, 'feature_fraction': 0.7991887891001287, 'bagging_fraction': 0.800485989461228, 'bagging_freq': 1, 'n_estimators': 175}
    | [0m 15      [0m | [0m-0.1732  [0m | [0m 0.8005  [0m | [0m 1.022   [0m | [0m 0.7992  [0m | [0m 0.05225 [0m | [0m 19.49   [0m | [0m 175.2   [0m | [0m 38.61   [0m |
    =============================================================================================================
    


```python
# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. 
```


```python
#BO_rf.res 
```

BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음.


```python
BO_rf.max
# 이떄에 우리는 아래 값들을 반올림 해서(int(round)) 사용했음을 기억하세요!
# 즉 max depth = 5 , min_sample_leaf = 3 , min_samples_split = 6 이 됩니다. # 이는 실행마다 달라지니까 주의하세용
```




    {'target': -0.16873401068694988,
     'params': {'bagging_fraction': 0.8925919669634644,
      'bagging_freq': 2.5776214962985264,
      'feature_fraction': 0.9371506430319936,
      'learning_rate': 0.09375052437923304,
      'max_depth': 23.46023058263993,
      'n_estimators': 173.5539900082939,
      'num_leaves': 38.75618804607675}}



## 최종 평가!


```python
max_params = BO_rf.max['params']
max_params['max_depth'] = int(round(max_params['max_depth']))
max_params['num_leaves'] = int(round(max_params['num_leaves']))
max_params['bagging_freq']=int(round(max_params['bagging_freq']))
max_params['n_estimators']=int(round(max_params['n_estimators']))
```


```python
lgb_model = LGBMRegressor(**max_params)
lgb_model.fit(X_train, y_train)
y_pred = lgb_model.predict(X_test)
score = mape(y_test,y_pred)

print('mape : {0:.4f}'.format(score))
```

    mape : 0.1692
    

## 비교모델


```python
lgb_model = LGBMRegressor()
lgb_model.fit(X_train, y_train) 
y_pred = lgb_model.predict(X_test)
score = mape(y_pred,y_test)

print('mape : {0:.4f}'.format(score))

# 조정값이 많아서 걍 DEFAULT 가 이긴모습... 
# 의외로 DEFAULT 이기기 쉽지않아요
```

    mape : 0.1600
    

# Light LGB ( 분류 )


```python
from sklearn.datasets import fetch_covtype
covtype = fetch_covtype()
```


```python
X = pd.DataFrame(covtype.data, 
                  columns=["x{:02d}".format(i + 1) for i in range(covtype.data.shape[1])],
                  dtype=int)
y = covtype.target
```


```python
X
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x01</th>
      <th>x02</th>
      <th>x03</th>
      <th>x04</th>
      <th>x05</th>
      <th>x06</th>
      <th>x07</th>
      <th>x08</th>
      <th>x09</th>
      <th>x10</th>
      <th>...</th>
      <th>x45</th>
      <th>x46</th>
      <th>x47</th>
      <th>x48</th>
      <th>x49</th>
      <th>x50</th>
      <th>x51</th>
      <th>x52</th>
      <th>x53</th>
      <th>x54</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2596</td>
      <td>51</td>
      <td>3</td>
      <td>258</td>
      <td>0</td>
      <td>510</td>
      <td>221</td>
      <td>232</td>
      <td>148</td>
      <td>6279</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2590</td>
      <td>56</td>
      <td>2</td>
      <td>212</td>
      <td>-6</td>
      <td>390</td>
      <td>220</td>
      <td>235</td>
      <td>151</td>
      <td>6225</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2804</td>
      <td>139</td>
      <td>9</td>
      <td>268</td>
      <td>65</td>
      <td>3180</td>
      <td>234</td>
      <td>238</td>
      <td>135</td>
      <td>6121</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2785</td>
      <td>155</td>
      <td>18</td>
      <td>242</td>
      <td>118</td>
      <td>3090</td>
      <td>238</td>
      <td>238</td>
      <td>122</td>
      <td>6211</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2595</td>
      <td>45</td>
      <td>2</td>
      <td>153</td>
      <td>-1</td>
      <td>391</td>
      <td>220</td>
      <td>234</td>
      <td>150</td>
      <td>6172</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>581007</th>
      <td>2396</td>
      <td>153</td>
      <td>20</td>
      <td>85</td>
      <td>17</td>
      <td>108</td>
      <td>240</td>
      <td>237</td>
      <td>118</td>
      <td>837</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581008</th>
      <td>2391</td>
      <td>152</td>
      <td>19</td>
      <td>67</td>
      <td>12</td>
      <td>95</td>
      <td>240</td>
      <td>237</td>
      <td>119</td>
      <td>845</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581009</th>
      <td>2386</td>
      <td>159</td>
      <td>17</td>
      <td>60</td>
      <td>7</td>
      <td>90</td>
      <td>236</td>
      <td>241</td>
      <td>130</td>
      <td>854</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581010</th>
      <td>2384</td>
      <td>170</td>
      <td>15</td>
      <td>60</td>
      <td>5</td>
      <td>90</td>
      <td>230</td>
      <td>245</td>
      <td>143</td>
      <td>864</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>581011</th>
      <td>2383</td>
      <td>165</td>
      <td>13</td>
      <td>60</td>
      <td>4</td>
      <td>67</td>
      <td>231</td>
      <td>244</td>
      <td>141</td>
      <td>875</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>581012 rows × 54 columns</p>
</div>




```python
X = X.iloc[0:5000,:]
y = y[0:5000]
```


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)
```

## 파라미터 범위 설정


```python
bayes_params = {
    'num_leaves': (24, 45), # 범위값으로 인식하게 된다!
    'colsample_bytree':(0.5, 1),  
    'subsample': (0.5, 1),
    'max_depth': (4, 12),
    'reg_alpha': (0, 0.5),
    'reg_lambda': (0, 0.5), 
    'min_split_gain': (0.001, 0.1),
    'min_child_weight':(5, 50)
}
```

테스트 해볼 하이퍼 파라미터의 범위 값을 설정하였으면 BaysianOptimization에서 호출하여 모델을 최적화하는 함수를 만들어 보겠습니다.

해당 함수는 BaysianOptimization에서 하이퍼 파라미터를 튜닝하기 위해 호출되면 제대로 튜닝이 되고 있는지를 판단하기 위해서 모델을 학습/평가하고 이에 따른 평가 지표를 반환하는 형식으로 만들어집니다. 이 평가 함수는 BayesianOptimization 객체에서 파라미터를 변경하면서 호출되므로 함수의 인자로 앞에서 딕셔너리로 설정된 파라미터들을 가지게 됩니다.  

## 평가 함수 정의


```python
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

def lgb_acc_eval(num_leaves, 
                 colsample_bytree,
                 subsample, 
                 max_depth, 
                 reg_alpha, 
                 reg_lambda, 
                 min_split_gain, 
                 min_child_weight):
    # 하이퍼 파라미터를 튜닝하기 위해 , 이게 제대로! 학습이 되고있는지 판단하기 위해 모델을 학습/ 평가하고 이에 따른 평가 지표를 반환하는 형식이 된다.
    params = {
        "n_estimator":200,
        "learning_rate":0.02,
        'num_leaves': int(round(num_leaves)), # 이 값은 정수형을 return 받아야 하므로! round/ int 를 차례로 받는다.
        'colsample_bytree': colsample_bytree, 
        'subsample': subsample,
        'max_depth': int(round(max_depth)),
        'reg_alpha': reg_alpha,
        'reg_lambda': reg_lambda, 
        'min_split_gain': min_split_gain,
        'min_child_weight': min_child_weight,
        'verbosity': -1
    }
    print("params:", params)  # 어 떤 파라미터를 사용하였는지
    lgb_model = LGBMClassifier(**params) # 모델! 
    cv_value = cross_val_score(lgb_model, X_train, y_train, cv=5, scoring='accuracy')
    
    result = np.mean(cv_value) # cv_value 는 list 형태로 나오게 되니까!
    return result # 이 result 값이 커지게 베이지안optimization 이 학습하게 됩니다.

    print('accuracy :', result)  # 그 값을 도출 print 해서 잘 학습하고 있는지 (줄여지는 방향으로) 알 아 보아요~    return roc_preds
```

## Optimization


```python
from bayes_opt import BayesianOptimization
# 객체를 형성한다
BO_lgb = BayesianOptimization(lgb_acc_eval, bayes_params, random_state=0)
```


```python
BO_lgb.maximize(init_points=5, n_iter=10)
```

    |   iter    |  target   | colsam... | max_depth | min_ch... | min_sp... | num_le... | reg_alpha | reg_la... | subsample |
    -------------------------------------------------------------------------------------------------------------------------
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 33, 'colsample_bytree': 0.7744067519636624, 'subsample': 0.9458865003910399, 'max_depth': 10, 'reg_alpha': 0.32294705653332806, 'reg_lambda': 0.21879360563134626, 'min_split_gain': 0.05494343511669279, 'min_child_weight': 32.12435192322397, 'verbosity': -1}
    | [0m 1       [0m | [0m 0.7729  [0m | [0m 0.7744  [0m | [0m 9.722   [0m | [0m 32.12   [0m | [0m 0.05494 [0m | [0m 32.9    [0m | [0m 0.3229  [0m | [0m 0.2188  [0m | [0m 0.9459  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 36, 'colsample_bytree': 0.9818313802505146, 'subsample': 0.5435646498507704, 'max_depth': 7, 'reg_alpha': 0.4627983191463305, 'reg_lambda': 0.03551802909894347, 'min_split_gain': 0.05336059705553755, 'min_child_weight': 40.627626713719906, 'verbosity': -1}
    | [0m 2       [0m | [0m 0.7649  [0m | [0m 0.9818  [0m | [0m 7.068   [0m | [0m 40.63   [0m | [0m 0.05336 [0m | [0m 35.93   [0m | [0m 0.4628  [0m | [0m 0.03552 [0m | [0m 0.5436  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5101091987201629, 'subsample': 0.8902645881432277, 'max_depth': 11, 'reg_alpha': 0.3995792821083618, 'reg_lambda': 0.23073968112646592, 'min_split_gain': 0.08713120267643511, 'min_child_weight': 40.01705379274327, 'verbosity': -1}
    | [0m 3       [0m | [0m 0.7594  [0m | [0m 0.5101  [0m | [0m 10.66   [0m | [0m 40.02   [0m | [0m 0.08713 [0m | [0m 44.55   [0m | [0m 0.3996  [0m | [0m 0.2307  [0m | [0m 0.8903  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 35, 'colsample_bytree': 0.5591372129344666, 'subsample': 0.8871168447171083, 'max_depth': 9, 'reg_alpha': 0.2073309699952618, 'reg_lambda': 0.13227780605231348, 'min_split_gain': 0.09452222278790881, 'min_child_weight': 11.450897933407088, 'verbosity': -1}
    | [95m 4       [0m | [95m 0.8066  [0m | [95m 0.5591  [0m | [95m 9.119   [0m | [95m 11.45   [0m | [95m 0.09452 [0m | [95m 34.96   [0m | [95m 0.2073  [0m | [95m 0.1323  [0m | [95m 0.8871  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.7280751661082743, 'subsample': 0.8409101495517417, 'max_depth': 9, 'reg_alpha': 0.30846699843737846, 'reg_lambda': 0.4718740392573121, 'min_split_gain': 0.062145914210511834, 'min_child_weight': 5.845541019635982, 'verbosity': -1}
    | [95m 5       [0m | [95m 0.8169  [0m | [95m 0.7281  [0m | [95m 8.547   [0m | [95m 5.846   [0m | [95m 0.06215 [0m | [95m 36.85   [0m | [95m 0.3085  [0m | [95m 0.4719  [0m | [95m 0.8409  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}
    | [0m 6       [0m | [0m 0.7671  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 5.0     [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.5     [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 6.153818666923531, 'verbosity': -1}
    | [95m 7       [0m | [95m 0.8274  [0m | [95m 0.5     [0m | [95m 12.0    [0m | [95m 6.154   [0m | [95m 0.1     [0m | [95m 45.0    [0m | [95m 0.0     [0m | [95m 0.0     [0m | [95m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 1.0, 'subsample': 0.5, 'max_depth': 4, 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 5.0, 'verbosity': -1}
    | [0m 8       [0m | [0m 0.7671  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 5.0     [0m | [0m 0.001   [0m | [0m 45.0    [0m | [0m 0.5     [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 41, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 9.282792883879118, 'verbosity': -1}
    | [0m 9       [0m | [0m 0.8174  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 9.283   [0m | [0m 0.1     [0m | [0m 40.95   [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 45, 'colsample_bytree': 0.5, 'subsample': 1.0, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 16.37016837765438, 'verbosity': -1}
    | [0m 10      [0m | [0m 0.8006  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 16.37   [0m | [0m 0.1     [0m | [0m 45.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 31, 'colsample_bytree': 0.9963415847423831, 'subsample': 0.8576257010583855, 'max_depth': 12, 'reg_alpha': 0.009330048227644716, 'reg_lambda': 0.019604314288185376, 'min_split_gain': 0.04155948070314624, 'min_child_weight': 5.033169891354441, 'verbosity': -1}
    | [0m 11      [0m | [0m 0.8266  [0m | [0m 0.9963  [0m | [0m 11.82   [0m | [0m 5.033   [0m | [0m 0.04156 [0m | [0m 31.37   [0m | [0m 0.00933 [0m | [0m 0.0196  [0m | [0m 0.8576  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.28278185068333334, 'reg_lambda': 0.5, 'min_split_gain': 0.001, 'min_child_weight': 50.0, 'verbosity': -1}
    | [0m 12      [0m | [0m 0.7357  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 50.0    [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.2828  [0m | [0m 0.5     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 0.5, 'subsample': 0.5, 'max_depth': 12, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.001, 'min_child_weight': 18.488156702160605, 'verbosity': -1}
    | [0m 13      [0m | [0m 0.7926  [0m | [0m 0.5     [0m | [0m 12.0    [0m | [0m 18.49   [0m | [0m 0.001   [0m | [0m 24.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 0.5     [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 37, 'colsample_bytree': 0.8958351898111097, 'subsample': 0.8153907933202891, 'max_depth': 12, 'reg_alpha': 0.019824024962597386, 'reg_lambda': 0.27977114529274616, 'min_split_gain': 0.02168998981760603, 'min_child_weight': 5.111223910488313, 'verbosity': -1}
    | [0m 14      [0m | [0m 0.8226  [0m | [0m 0.8958  [0m | [0m 11.91   [0m | [0m 5.111   [0m | [0m 0.02169 [0m | [0m 36.83   [0m | [0m 0.01982 [0m | [0m 0.2798  [0m | [0m 0.8154  [0m |
    params: {'n_estimator': 200, 'learning_rate': 0.02, 'num_leaves': 24, 'colsample_bytree': 1.0, 'subsample': 1.0, 'max_depth': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'min_split_gain': 0.1, 'min_child_weight': 26.679596508578747, 'verbosity': -1}
    | [0m 15      [0m | [0m 0.7477  [0m | [0m 1.0     [0m | [0m 4.0     [0m | [0m 26.68   [0m | [0m 0.1     [0m | [0m 24.0    [0m | [0m 0.0     [0m | [0m 0.0     [0m | [0m 1.0     [0m |
    =========================================================================================================================
    


```python
# BayesianOptimization 객체의 res 속성은 하이퍼 파라미터 튜닝을 하는 과정에서의 metric 값과 그때의 하이퍼 파라미터 값을 가지고 있음. 
```


```python
BO_lgb.res
```




    [{'target': 0.7728571428571429,
      'params': {'colsample_bytree': 0.7744067519636624,
       'max_depth': 9.721514930979357,
       'min_child_weight': 32.12435192322397,
       'min_split_gain': 0.05494343511669279,
       'num_leaves': 32.896750786116996,
       'reg_alpha': 0.32294705653332806,
       'reg_lambda': 0.21879360563134626,
       'subsample': 0.9458865003910399}},
     {'target': 0.7648571428571429,
      'params': {'colsample_bytree': 0.9818313802505146,
       'max_depth': 7.067532150606222,
       'min_child_weight': 40.627626713719906,
       'min_split_gain': 0.05336059705553755,
       'num_leaves': 35.92893578297258,
       'reg_alpha': 0.4627983191463305,
       'reg_lambda': 0.03551802909894347,
       'subsample': 0.5435646498507704}},
     {'target': 0.7594285714285715,
      'params': {'colsample_bytree': 0.5101091987201629,
       'max_depth': 10.660958764383505,
       'min_child_weight': 40.01705379274327,
       'min_split_gain': 0.08713120267643511,
       'num_leaves': 44.55098518688804,
       'reg_alpha': 0.3995792821083618,
       'reg_lambda': 0.23073968112646592,
       'subsample': 0.8902645881432277}},
     {'target': 0.8065714285714286,
      'params': {'colsample_bytree': 0.5591372129344666,
       'max_depth': 9.119368170620191,
       'min_child_weight': 11.450897933407088,
       'min_split_gain': 0.09452222278790881,
       'num_leaves': 34.95881475675151,
       'reg_alpha': 0.2073309699952618,
       'reg_lambda': 0.13227780605231348,
       'subsample': 0.8871168447171083}},
     {'target': 0.816857142857143,
      'params': {'colsample_bytree': 0.7280751661082743,
       'max_depth': 8.547471590949188,
       'min_child_weight': 5.845541019635982,
       'min_split_gain': 0.062145914210511834,
       'num_leaves': 36.85401017717085,
       'reg_alpha': 0.30846699843737846,
       'reg_lambda': 0.4718740392573121,
       'subsample': 0.8409101495517417}},
     {'target': 0.7671428571428571,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 5.0,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.5,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.8274285714285714,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 6.153818666923531,
       'min_split_gain': 0.1,
       'num_leaves': 45.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.7671428571428571,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 5.0,
       'min_split_gain': 0.001,
       'num_leaves': 45.0,
       'reg_alpha': 0.5,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.8174285714285714,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 9.282792883879118,
       'min_split_gain': 0.1,
       'num_leaves': 40.95446342173249,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.8005714285714285,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 16.37016837765438,
       'min_split_gain': 0.1,
       'num_leaves': 45.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}},
     {'target': 0.8265714285714285,
      'params': {'colsample_bytree': 0.9963415847423831,
       'max_depth': 11.819684742565805,
       'min_child_weight': 5.033169891354441,
       'min_split_gain': 0.04155948070314624,
       'num_leaves': 31.371377406180006,
       'reg_alpha': 0.009330048227644716,
       'reg_lambda': 0.019604314288185376,
       'subsample': 0.8576257010583855}},
     {'target': 0.7357142857142858,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 50.0,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.28278185068333334,
       'reg_lambda': 0.5,
       'subsample': 0.5}},
     {'target': 0.7925714285714286,
      'params': {'colsample_bytree': 0.5,
       'max_depth': 12.0,
       'min_child_weight': 18.488156702160605,
       'min_split_gain': 0.001,
       'num_leaves': 24.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 0.5}},
     {'target': 0.8225714285714286,
      'params': {'colsample_bytree': 0.8958351898111097,
       'max_depth': 11.912363935718016,
       'min_child_weight': 5.111223910488313,
       'min_split_gain': 0.02168998981760603,
       'num_leaves': 36.82863912155457,
       'reg_alpha': 0.019824024962597386,
       'reg_lambda': 0.27977114529274616,
       'subsample': 0.8153907933202891}},
     {'target': 0.7477142857142857,
      'params': {'colsample_bytree': 1.0,
       'max_depth': 4.0,
       'min_child_weight': 26.679596508578747,
       'min_split_gain': 0.1,
       'num_leaves': 24.0,
       'reg_alpha': 0.0,
       'reg_lambda': 0.0,
       'subsample': 1.0}}]



BayesianOptimization 객체의 max 속성은 최고 높은 성능 Metric를 가질때의 하이퍼 파라미터 값을 가지고 있음.


```python
BO_lgb.max
```




    {'target': 0.8274285714285714,
     'params': {'colsample_bytree': 0.5,
      'max_depth': 12.0,
      'min_child_weight': 6.153818666923531,
      'min_split_gain': 0.1,
      'num_leaves': 45.0,
      'reg_alpha': 0.0,
      'reg_lambda': 0.0,
      'subsample': 1.0}}



## 최종 평가


```python
from sklearn.metrics import accuracy_score

max_params = BO_rf.max['params']
max_params['min_samples_leaf'] = int(round(max_params['min_samples_leaf']))
max_params['min_samples_split'] = int(round(max_params['min_samples_split']))
max_params['max_depth'] = int(max_params['max_depth'])
lgb_model = LGBMClassifier(n_estimators=200,learning_rate=0.2, **max_params)
lgb_model.fit(X_train, y_train) # train 을 전체로 해서 더 높은가바..
y_pred = lgb_model.predict(X_test)
score = accuracy_score(y_pred,y_test)

print('accuracy : {0:.4f}'.format(score))
```

    mse score : 0.8713
    

# Catboost
