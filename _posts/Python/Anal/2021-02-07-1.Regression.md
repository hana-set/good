---
title:  "1.Regression"
excerpt: "회귀 방법론에 대해 알아봅시다."
categories:
  - Py_Analysis
tags:
  - 기초
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"

use_math: true
---

```python
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.datasets import make_regression
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn import metrics # 모델평가시 이용
```


```python
# dataset 형성하기
X, y = make_regression(n_samples=2000, # sample 의 수는 2000개
                       n_features=20, # x 변수는 20개
                       n_informative=4, # 그중 y 변수와 관련되는 변수는 4개  
                       noise = 3       # nosie 
                      ,effective_rank=15 #  x 변수중 서로 독립인 수 (5개는 서로 관련)
                      ,tail_strength=0.3 # 관련된 변수의 관련성
                      ,random_state=0) # random state 고정
X = pd.DataFrame(X)
X[[1,4,6,10,13]]=X[[1,4,6,10,13]]*-1
```


```python
# dataset train/test set 으로 나누기
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```


```python
y
```




    array([ 4.7989583 ,  2.60992309,  3.15534149, ..., -0.88908075,
           -5.57811736,  3.9804761 ])



# Multi Linear Regression

## 모델 세우기


```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```




    LinearRegression()



## 모델 평가


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test,y_pred))
print("R squared :", metrics.r2_score(y_test,y_pred))
```

    MSE : 8.611762902052247
    R squared : 0.20108198497901186
    

# Polynomial regression

변수가 적다면, 데이터 분석시 부정확 할 수 있다. 그래서 polynomial 로 변수의 갯수를 늘려서 예측도를 높혀보자

## 데이터 전처리

다항회귀는, 데이터를 돌리기 전, x1 x2 -> x1^2 , x1x2, x2^2 처럼 데이터를 바꾸어야 한다.를 해주어야한다.


```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures()  
# degree : 몇 차수까진 늘릴지 결정
#        : (default)=2
# interaction_only : only 자기 자신의 거듭제곱만 넣을지 말지. (ex x1 x2 -> x1^2 , ^2)
#                  : (default)=False (interaction 항 넣기) (EX x1 x2 -> x1^2 , x1x2, x2^2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.fit_transform(X_test)
```

## 모델 세우기


```python
model = LinearRegression()
model.fit(X_train_poly, y_train)
```




    LinearRegression()



## 모델 평가


```python
from sklearn import metrics
y_pred = model.predict(X_test_poly)
print ("MSE :", metrics.mean_squared_error(y_test,y_pred))
print("R squared :", metrics.r2_score(y_test,y_pred))
```

    MSE : 10.588639961853904
    R squared : 0.01768600502455997
    

# Ridge regression

$w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda \sum_{j=1}^M w_j^2 \right) $

## 모델 세우기


```python
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import Ridge

para_range = np.logspace(-3, 3, num=50) # 10^-3 ~ 10^3 
model = RidgeCV(alphas = para_range , cv=5)
# cross validaaion 을 5 fold cross validation 으로 지정 , 
# para_range 의 범위만큼 ,cv 를 해서 최적의 alpha 를 구하겠다는 의미
# RidgeCV = cv 와 ridge 를 동시에 합친 model 이다.
model.fit(X_train, y_train) ;
# X_train, Y_train 으로 ridgecv 를 fitting 해야 비로소 의미가 생긴다. 위는 setting 작업임.
#ridgecv.fit 은 alpha 를 위에서 구한 최적의 값을 써서 ridge regression 을 하겠다는 의미이다.
```

## 모델 평가


```python
from sklearn import metrics

predicted = model.predict(X_test) 
# 우리가 fitting 한 coefficient 로 X_test 를 이용해 Y_test 를 predict
#ridgecv.alpha_ # Estimated regularization parametor (최적값)
#ridgecv.coef_ # fitting 한 coff 값들

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',model.score(X_test, y_test)) # C-V 로 찾은 최적의 ridge 로 계산한 R^2
```

    MSE : 8.606003653527091
    R_squared : 0.20161627365510082
    

# ElasticNet Regression

**[모델 원리]**
- $w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda_1 \sum_{j=1}^M | w_j | + \lambda_2 \sum_{j=1}^M w_j^2 \right)$ 로 w 를 정하는 regression

**[Hyperparameter]** 
- sklearn 에서는 $0.5 \times \text{RSS}/N + \text{alpha} \times \big( 0.5 \times (1-\text{L1_wt})\sum w_i^2 + \text{L1_wt} \sum |w_i| \big)$ 을 최소화 한다.(사실상 위랑 똑같다.)
- l1_ratio : float, default=0.5
     - For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2



```python
from sklearn.linear_model import ElasticNetCV
from sklearn.linear_model import ElasticNet
```

## 모델 세우기


```python
ElasticCV = ElasticNetCV(l1_ratio =[.1, .5, .7, .9, .95, .99,1], cv=5, random_state=0)
# l1 Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in  [.1, .5, .7, .9, .95, .99, 1] 

ElasticCV.fit(X_train, y_train)
print(ElasticCV.alpha_) # alpha 값
print(ElasticCV.l1_ratio_) #l1 ratio 1 이면 사실상 lasso 이다... 1 이 나오면 그냥 lasso 로 하라는 의미
```

    0.0009717603376703314
    1.0
    

## 모델 평가


```python
from sklearn import metrics

predicted = ElasticCV.predict(X_test) 
# 우리가 fitting 한 coefficient 로 X_test 를 이용해 Y_test 를 predict
#ridgecv.alpha_ # Estimated regularization parametor (최적값)
#ridgecv.coef_ # fitting 한 coff 값들

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',model.score(X_test, y_test)) # C-V 로 찾은 최적의 ridge 로 계산한 R^2
```

    MSE : 8.607510464307282
    R_squared : 0.20161627365510082
    

## 변수 중요도


```python
coef = pd.Series(ElasticCV.coef_, index = X_train.columns).sort_values()
imp_coef = pd.concat([coef.head(5), coef.tail(5)])
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Model")
```




    Text(0.5, 1.0, 'Coefficients in the Model')




    
<img src="/assets/images/1.Regression/output_33_1.png">
    


# lasso regression

$w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda \sum_{j=1}^M | w_j | \right)$

## 모델 세우기


```python
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
alphas = np.logspace(-3, 3, num=50) # 10^-3 ~ 10^3 
lassocv = LassoCV(alphas = alphas, cv=5)
# 위 ridge 때와 동일
lassocv.fit(X_train, y_train) ;
```

## 모델 평가


```python
from sklearn import metrics
predicted = lassocv.predict(X_test) 
# 우리가 fitting 한 coefficient 로 X_test 를 이용해 Y_test 를 predict
#lassocv.alpha_ # Estimated regularization parametor (최적값)
#lassocv.coef_ # fitting 한 coff 값들

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',lassocv.score(X_test, y_test)) # C-V 로 찾은 최적의 ridge 로 계산한 R^2
```

    MSE : 8.607540368924692
    R_squared : 0.20147371171638206
    

## 변수 중요도

상위 6개의 관려있는 계수들을 보여준다.


```python
coef = pd.Series(lassocv.coef_, index = X_train.columns).sort_values()
imp_coef = pd.concat([coef.head(5), coef.tail(5)])
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Model")
```




    Text(0.5, 1.0, 'Coefficients in the Model')




    
<img src="/assets/images/1.Regression/output_42_1.png">
    


# Kernel Regression

독립변수 벡터  x 를 입력으로 가지는 여러개의 비선형 함수  $\phi_j(x)$ 들을 생각해 내어 원래의 입력 변수  x 대신 $\phi_j(x)$ 들을 입력변수로 사용한 다음과 같은 모형을 쓰면 더 좋은 예측 성능을 가질 수도 있다.

$ y_i = \sum_{j=1}^{M} w_j \phi_j(x)  = w^T \phi(x) $



# K-Neighbors Regression

<b> 모델 원리 </b>

- KNN은 새로운 데이터가 주어졌을 때 기존 데이터 가운데 가장 가까운 k개 이웃의 정보로 새로운 데이터를 예측하는 방법론입니다
- Classification 인 경우, test point x 에 대해 가장 가까운 train set 의 k 개 이웃중 제일 많은 class로 예측을 하게 되고
- Regression 의 경우 test point x 에 대해 가장 가까운 train set 의 k 개의 이웃의 값들의 평균으로 예측을 하게 됩니다.

<b> 모델 개요 </b>

- 데이터 포인트 사이의 거리를 재는 방법과 이웃의 수 2개가 hyper parameter 이다.
- 거리를 재는 방법은 기본적으로 여러 환경에서 잘 동작하는 유클리디안 거리 방식을 사용하므로 여기서는 다루지 않겠다.

<b> 주의점 </b>
- k-NN 알고리즘을 사용할 땐 '거리' 가 기준이 되기때문에 데이터 값들을 모두 표준화 하여 전처리해주어야 합니다.

<b> 장점 </b>
- 이해하기 매우 쉬운 모델 
- parameter을 많이 조정하지 않아도 자주 좋은 성능을 발휘합니다.
- 더 복잡한 알고리즘을 적용해보기 전에 시도해볼 수 있는 좋은 시작점입니다.

<b> 단점 </b>
- 보통 최근접 이웃 모델은 매우 빠르게 만들 수 있지만, 훈련 세트가 매우 크면 (특성의 수나 샘플의 수가 클 경우) 예측이 느려집니다. 
- 그리고 (수백 개 이상의) 많은 특성을 가진 데이터셋에는 잘 동작하지 않으며, 특성 값 대부분이 0인 데이터셋과는 특히 잘 작동하지 않습니다.
- k-최근접 이웃 알고리즘이 이해하긴 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해 현업에서는 잘 쓰지 않습니다.

<b> Hyperparameter </b>


n_neighbors
- 이웃의 수


## 모델 세우기


```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
model = KNeighborsRegressor() 
param_grid ={'n_neighbors' : np.arange(1,10)}
model = GridSearchCV(model,param_grid,cv=5)
model.fit(X_train,y_train)
```




    GridSearchCV(cv=5, estimator=KNeighborsRegressor(),
                 param_grid={'n_neighbors': array([1, 2, 3, 4, 5, 6, 7, 8, 9])})




```python
print(model.best_params_) # 우리가 추정한 best parameter
print(model.best_score_) # 우리가 지정한 scoring 에 기반한 제일 좋은 score
```

    {'n_neighbors': 9}
    0.07674181821732715
    

## 모델 평가


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print("R squared :", metrics.r2_score(y_test, y_pred))
```

    MSE : 10.361920123323124
    R squared : 0.03871893003945159
    

#  DecisionTree Regression

<b> [모델 원리] </b>

의사결정나무의 학습 과정은 입력 변수 영역을 두 개로 구분하는 재귀적 분기(recursive partitioning)와 너무 자세하게 구분된 영역을 통합하는 가지치기(pruning) 두 가지 과정으로 나뉩니다. 

재귀적 분기
- 우선 데이터를 한 변수 기준(ex주택 크기)으로 정렬합니다. 이후 가능한 모든 분기점에 대해 엔트로피/지니계수를 구해 분기 전과 비교해 정보획득을 조사합니다. 
- 그리고 다른 변수 기준 으로 정렬한 후 위같은 과정을 반복합니다.
- 모든 경우의 수 가운데 정보획득이 가장 큰 변수와 그 지점을 택해 첫번째 분기를 하게 됩니다. 
- 이후 또 같은 작업을 반복해 두번째, 세번째… 이렇게 분기를 full tree 가 될때까지 계속 해 나갑니다.

가지치기
- 의사결정나무 모델 학습의 또다른 축은 가지치기(pruning)입니다. 모든 terminal node의 순도가 100%인 상태를 Full tree라고 하는데요. 이렇게 Full tree를 생성한 뒤 적절한 수준에서 terminal node를 결합해주어야 합니다. 왜냐하면 분기가 너무 많아서 학습데이터에 과적합(overfitting)할 염려가 생기기 때문입니다.
- 가지치기는 데이터를 버리는 개념이 아니고 분기를 합치는(merge) 개념으로 이해해야 합니다.
- 가지치기의 경우 비용함수(costfunction = Err(T)+α×L(T) (Err(T) : 오분류율 , L(T) : terminal node(leaf)의 수(구조의 복잡도), α : 가중치 상수 보통 0.1~0.01 )  가 최소가 되도록 가지를 치게 됩니다.

<b> [장점] </b>
- 만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉽습니다(비교적 작은 트리일 때). 
- 그리고 데이터의 스케일에 구애받지 않습니다. 데이터의 변수마다 트리에서는 개별적으로 처리되어 데이터를 분할하는 데 데이터 스케일의 영향을 받지 않으므로 결정 트리에서는 특성의 정규화나 표준화 같은 전처리 과정이 필요 없습니다. 특히 특성의 스케일이 서로 다르거나 이진 특성(범주형)과 연속적(연속형)인 특성이 혼합되어 있을 때도 잘 작동합니다.
- 계산복잡성 대비 높은 예측 성능을 내는 것으로 정평이 나 있습니다. 

<b> [단점] </b>
- 결정경계(decision boundary)가 데이터 축에 수직이어서 특정 데이터에만 잘 작동할 가능성이 높습니다.
- data 의 변화에 매우 민감합니다. 데이터가 약간만 변화해도 tree 가 완전히 변화하기 떄문입니다.
- 두 변수가 multicollinearity 관계에 있으면 decision tree will greedily choose the best one. 즉 한 변수는 아예 쳐내버립니다. 두 변수를 모두 이용하지 않는다는것입니다.
- sample 범위 바깥의 값을 예측할 때에는 오히려 선형회귀보다 훨씬 안좋은 fitting 이 나옵니다.
- 결정 트리의 주요 단점은 사전 가지치기를 사용함에도 불구하고 과대적합되는 경향이 있어 일반화 성능이 좋지 않다는 것입니다. 그래서 다음에 설명할 앙상블 방법을 단일 결정 트리의 대안으로 흔히 사용합니다.


<b> [Hyperparameter] </b>

criterion 
- 불순성의 기준을 뭐로 할지 'gini' / 'entropy' 가능
- (defalut)= 'gini'

max_depth 
- 트리의 최대 깊이. 
- 이를 이용해 사전 가지치기를 하고/ overfitting 을 해결할 수 있다.
- (defalut) : full tree 가 될때까지 확장.

min_samples_split 
- 노드에서 가지 분리할 떄 필요한 최소 sample 갯수에 제한을 준다.
- (default) = 2 

min_samples_leaf
- leaf 에서 가져야 할 최소 sample 
- (default) = 1

max_features
- Decision tree 를 만들때 사용할 수 있는 변수의 갯수 제한
- (default) = 총 변수 갯수 사용

<b> [Hyperparameter tuning] </b>
-  max_depth 하나만 지정해도 과대적합을 막는 데 충분합니다.

<b>[feature importance]</b>

계산 원리

- 노드 중요도 부터 정의해야합니다. 노드 중요도란 Information Gain을 결국 말합니다. 의사결정나무에서는 그 때 Information Gain을 최대화하는 feature를 기준으로 노드를 째게 됩니다. 어떤 노드의 노드 중요도 값이 크다는 것은 그 노드에서 특히 불순도가 크게 감소한다는 것을 의미합니다.
- i번째 feature의 중요도 : 전체 노드의 중요도를 합한 것 대비 i번째 feature에 의해 째진 노드들의 중요도를 합한 것 입니다.

의미

- 이 값은 0~1 사이의 숫자로, 0은 전혀 사용되지 않았다는 뜻이고 1은 완벽하게 타깃 클래스를 예측했다는 뜻입니다. 특성 중요도의 전체 합은 1입니다.
- Scikit-learn에서는 지니 중요도(Gini Importance)를 이용해서 각 feature의 중요도를 측정합니다
- 그러나 어떤 특성의 feature_importance_ 값이 낮다고 해서 이 특성이 유용하지 않다는 뜻은 아닙니다. 단지 트리가 그 특성을 선택하지 않았을 뿐이며 다른 특성이 동일한 정보를 지니고 있어서일 수 있습니다.
- 선형 모델의 계수와는 달리, 특성 중요도는 항상 양수이며 특성이 어떤 클래스를 지지하는지는 알 수 없습니다. 즉 이 변수의 영향력이 긍정적인지 부정적인지 알 수는 없습니다.
- 하지만 feature Importance는 다소 biased하다. 특히, 랜덤 포레스트는 연속형 변수 또는 카테고리 개수가 매우 많은 변수, 즉 ‘high cardinality’ 변수들의 중요도를 더욱 부풀릴 가능성이 높다고 한다. 왜 이런 결과가 나오는지는 정확히 알 수 없으나, cardinality가 큰 변수일 수록, 노드를 쨀 게 훨씬 더 많아서 노드 중요도 값이 높게 나오는 게 아닐까 싶다.

## 모델 세우기


```python
# hyper parameter 추정하기
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

model = DecisionTreeRegressor(min_samples_split=50) 
param_grid ={'max_depth' : np.arange(1,10)}
grid_search = GridSearchCV(model,param_grid,cv=5)
# scoring 은 default 이므로 model 의 자체 scoring 으로 들어간다. 
grid_search.fit(X_train,y_train)
print(grid_search.best_params_) # 우리가 추정한 best parameter
print(grid_search.best_score_) # 우리가 지정한 scoring 에 기반한 제일 좋은 score
```

    {'max_depth': 3}
    0.16422632977859336
    


```python
# 위에서 구한 최상의 parameter 로 추정하기
model = DecisionTreeRegressor(max_depth=3,min_samples_split=50)
model.fit(X_train, y_train)
```




    DecisionTreeRegressor(max_depth=3, min_samples_split=50)



## 모델 평가


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print("R squared :", metrics.r2_score(y_test, y_pred))
```

    MSE : 10.167059541425555
    R squared : 0.05679625416763212
    

## 변수 중요도


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_69_1.png">
    


# RandomForest Regressoion

그래디언트 부스팅 회귀 트리는 여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또 다른 앙상블 방법입니다. 이름이 회귀지만 이 모델은 회귀와 분류 모두에 사용할 수 있습니다. 5 랜덤 포레스트와는 달리 그래디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다. 기본적으로 그래디언트 부스팅 회귀 트리에는 무작위성이 없습니다. 대신 강력한 사전 가지치기가 사용됩니다. 그래디언트 부스팅 트리는 보통 하나에서 다섯 정도의 깊지 않은 트리를 사용하므로 메모리를 적게 사용하고 예측도 빠릅니다. 그래디언트 부스팅의 근본 아이디어는 이런 얕은 트리 같은 간단한 모델(약한 학습기weak learner라고도 합니다)을 많이 연결하는 것입니다. 각각의 트리는 데이터의 일부에 대해서만 예측을 잘 수행할 수 있어서 트리가 많이 추가될수록 성능이 좋아집니다. 6

그래디언트 부스팅 트리는 머신러닝 경연 대회에서 우승을 많이 차지하였고 업계에서도 널리 사용합니다. 랜덤 포레스트보다는 매개변수 설정에 조금 더 민감하지만 잘 조정하면 더 높은 정확도를 제공해줍니다.

앙상블 방식에 있는 사전 가지치기나 트리 개수 외에도 그래디언트 부스팅에서 중요한 매개변수는 이전 트리의 오차를 얼마나 강하게 보정할 것인지를 제어하는 learning_rate입니다. 학습률이 크면 트리는 보정을 강하게 하기 때문에 복잡한 모델을 만듭니다. n_estimators 값을 키우면 앙상블에 트리가 더 많이 추가되어 모델의 복잡도가 커지고 훈련 세트에서의 실수를 바로잡을 기회가 더 많아집니다.


랜덤 포레스트는 텍스트 데이터 같이 매우 차원이 높고 희소한 데이터에는 잘 작동하지 않습니다. 이런 데이터에는 선형 모델이 더 적합합니다. 랜덤 포레스트는 매우 큰 데이터셋에도 잘 작동하며 훈련은 여러 CPU 코어로 간단하게 병렬화할 수 있습니다. 하지만 랜덤 포레스트는 선형 모델보다 많은 메모리를 사용하며 훈련과 예측이 느립니다. 속도와 메모리 사용에 제약이 있는 애플리케이션이라면 선형 모델이 적합할 수 있습니다.




랜덤 포레스트에서 변수 중요도는 어떻게 될까요? 랜덤 포레스트는 의사결정나무들을 병렬적으로 합한 것이므로, 랜덤 포레스트에서 변수중요도는 결국 각 트리에서의 변수중요도를 모두 평균 낸 것에 해당합니다!

## 모델 세우기


```python
from sklearn.ensemble import RandomForestRegressor
model =  RandomForestRegressor(n_estimators=100,min_samples_split=50,oob_score = True)
# scoring 은 default 이므로 model 의 자체 scoring 으로 들어간다. 
# n_estimator = 500 클수록 좋으나 내 컴퓨터가 버티질 못할듯.
model.fit(X_train,y_train)
```




    RandomForestRegressor(min_samples_split=50, oob_score=True)



## 모델 평가


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.1050983784859
    R_squared : 0.15531497954068496
    

## 변수 중요도


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_80_1.png">
    


# Gradient Boosting resgressoion

<b> 모델 원리 </b>

- 부스팅은 약한 분류기를 세트로 묶어서 정확도를 예측하는 기법이다. 모래 자갈, 먼지가 섞여 있는 물질에 여러 타입의 체를 가지고 조합해서 그것을 분류하는 과정과 유사하다.

- 예를 들어, 어떤 학습기 M에 대하여 Y를 예측하는 모델 Y = M(x) + error(1) 가 있다고 하자.
- error(1) 에 대해서 설명하는 모델 G(x) 를 fitting 한다. error(1) = G(x) + error(2)
- error(2) 에 대해서 설명하는 모델 H(x) 를 fitting 한다. error(2) = G(x) + error(3)
- 즉 이전 모델에서 fitting 되고 남은 에러를 계속 fitting 해서 설명하는 형식으로 진행이 된다.
- 위 모델들을 처음 model 에 적용하게 되면
- Y = M(x) + G(x) + ............
- 그리고 각각 분류기의 성능이 다른데, 같은비중(지금은1) 을 두면 모델의 성능이 좋지 않을것이다. 그러므로 각 모델의 성능에 비례하여 이제 각 모델 앞에 비중(weights)을 두면
- 최종 모델 : Y = m * M(x) + g * G(x) + h * H(x) +................

<b> 모델 개요 </b>
- 그래디언트 부스팅의 근본 아이디어는 이런 얕은 트리 같은 간단한 모델(약한 학습기weak learner라고도 합니다)을 많이 연결하는 것입니다. <br>
- 이름이 회귀지만 이 모델은 회귀와 분류 모두에 사용할 수 있습니다. <br>
- 랜덤 포레스트와는 달리 그래디언트 부스팅은 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듭니다.<br>
- 각각의 트리는 데이터의 일부에 대해서만 예측을 잘 수행할 수 있어서 트리가 많이 추가될수록 성능이 좋아집니다. <br>
- 그래디언트 부스팅 트리는 머신러닝 경연 대회에서 우승을 많이 차지하였고 업계에서도 널리 사용합니다. 랜덤 포레스트보다는 매개변수 설정에 조금 더 민감하지만 잘 조정하면 더 높은 정확도를 제공해줍니다.<br>

<b> 장점 </b>
- 그래디언트 부스팅 트리는 보통 하나에서 다섯 정도의 깊지 않은 트리를 사용하므로 메모리를 적게 사용하고 예측도 빠릅니다. <br>
- 다른 트리 기반 모델처럼 특성의 스케일을 조정하지 않아도 되고 이진 특성이나 연속적인 특성에서도 잘 동작합니다.
 

<b> 단점 </b>
- 단점은 매개변수를 잘 조정해야 한다는 것과(매개변수 설정에 random forest 보다 민감하기때문) 훈련 시간이 길다는 것입니다.
- 트리 기반 모델의 특성상 희소한 고차원 데이터에는 잘 작동하지 않습니다.<br>

<b> Hyperparameter </b>
 - n_estimators : 트리의 개수를 지정 <br>
 - learning_rate : 이전 트리의 오차를 보정하는 정도를 조절 <br>
 - max_depth : 각 모델 트리의 최대 깊이 <br>

<b> Hyperparameter 조절 </b>
- 일반적으로 기본값을 쓰는 것이 좋은 방법입니다 <br>
- n_estimators는 클수록 좋다. 더 많은 트리를 평균하면 과대적합을 줄여 더 안정적인 모델을 만든다. 하지만 트리가 많으면 시간이 오래걸린다 <br>
- 일반적인 관례는 가용한 시간과 메모리 한도에서 n_estimators를 맞추고 나서 적절한 learning_rate를 찾는 것.<br>
- 각 트리의 복잡도를 낮추는 max_depth입니다. 통상 그래디언트 부스팅 모델에서는 max_depth를 매우 작게 설정하며 트리의 깊이가 5보다 깊어지지 않게 합니다<br>

**early stoppiny**
- 이떄에 fitting 할때에 early stopping 을 써서, 더욱더 좋은 모델을 만들 수 있습니다.
- early stopping 이란, 점점 더 tree 를 증가시키면서 (residual 을 학습하는것이니까요!) 학습하는 과정인데, 어느순간 과적합의 순간이 와서 val loss 가 오히려 증가할 수 있습니다.
- 그런경우를 막기 위해 tree 를 마치 epoch 처럼 생각해 n_eastimator 를 증가시키면서 val loss 의 추이를 보고, 어느정도의 n_eastimator 를 지정해줄지 알게 된답니다!
- fit 할떄 쓰임! bossting 모델들은 모두 가지고 있다고 생각하시면 됩니다.

## 모델 세우기


```python
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)
```




    GradientBoostingRegressor(random_state=42)



## 모델 평가


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.239837743558098
    R_squared : 0.1428151340023206
    

## 변수 중요도


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_94_1.png">
    


# AdaBoosting Regression

<b> 모델 원리 </b>
- 처음에는 weight 1/N을 가지는(즉 뽑힐 확률이 똑같은) data로 classifier(h1)를 학습 시키고, testing을 수행한다

- h1의 분류 결과로, 잘못 분류된 data가 있을 것이고, 이 data에 높은 weight(뽑힐 확률이 크게한다)를 준다

- h2를 train 할 때는 h1가 틀린 데이터가 많이 포함된 데이터들로 학습 될 것이다

- 그럼 h2는 h1가 틀린 데이터를 잘 분류하도록 학습 될 것이다

- 즉 먼저 학습된 분류기가 잘못 분류한 것에 대한 정보를 다음 분류기가 학습할 때 사용하고, 단점을 보완하는 것이다

- 이런식으로 계속 분류기가 학습된다 h1 -> h2 -> ...->hN

- 각 모델이 data 를 얼마나 잘 분류했는지에 대해 a1,a2 ... 의 weight 를 준다.(잘 분류하면 큰 weight)

- 예측 성능이 조금 낮은 약한 분류기들을 조합하여 최종적으로 하나의 강한 분류기를 생성한다

- 최종 분류기 : f(x)= a1* h1(x)+a2* h2(x)+...+aN* hN(x)

## 모델 세우기


```python
from sklearn.ensemble import AdaBoostRegressor
model =AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)
```




    AdaBoostRegressor(learning_rate=0.1, n_estimators=100, random_state=42)



## 모델 평가


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.204469959820555
    R_squared : 0.14609622289209434
    

## 변수 중요도


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_102_1.png">
    


 # xgboost Regression

## 모델 세우기

xgboost란?

1. 훌륭한 그라디언트 부스팅 라이브러리. 병렬 처리를 사용하기에 학습과 분류가 빠르다

2. 유연성이 좋다. 평가 함수를 포함하여 다양한 커스텀 최적화 옵션을 제공한다

3. 욕심쟁이(Greedy-algorithm)를 사용한 자동 가지치기가 가능하다. 따라서 과적합(Overfitting)이 잘 일어나지 않는다

4. 다른 알고리즘과 연계 활용성이 좋다. xgboost 분류기 결론부 아래에 다른 알고리즘을 붙여서 앙상블 학습이 가능하다 ResNet 마지막 바로 이전 단을 Feature layer로 응용하는 것과 비슷하다.

xgboost는 욕심쟁이(Greedy algorithm)를 사용하여 분류기 M, G, H를 발견하고, 분산처리를 사용하여 빠른(Extreme) 속도로 적합한 비중 파라메터를 찾는 알고리즘이다. 분류기는 Regression Score를 사용하여 정확도 스코어(accuracy score)를 측정하고, 각 순서에 따라 강한 분류기부터 약한 분류기까지 랜덤 하게 생성된다. 이렇게 만들어진 분류기를 트리(tree)라고 하며, 분류기를 조합한 최종 알고리즘을 포레스트(forest)라고 한다. 여기까지가 기본적인 boosting algorithm 원리다.

xgboost는 트리를 만들 때 CART(Classification And Regression Trees)라 불리는 앙상블 모델을 사용한다. 이후 트리 부스팅을 사용하여, 각 분류 기간 비중(weights)을 최적화한다. CART 모델은 일반적인 의사결정 트리(Decision Tree)와 조금 다르다. 리프 노드 하나에 대해서만 디시전 벨류를 갖는는 의사결정 트리와 달리, CART 방식은 모든 리프들이 모델의 최종 스코어에 연관되어 있다. 따라서 의사결정 트리가 분류를 제대로 했는지에 대해서만 초점을 맞추는 반면, CART는 같은 분류 결과를 갖는 모델끼리도 모델의 우위를 비교할 수 있다(스코어를 비교하면 된다). 즉, 모든 트레이닝 세트 X에 대하여 포레스트에 넣고, 결괏값으로 나오는 점수의 절대값을 더한다. 많은 데이터를 +와 -의 방향으로 보낼수록, 좋은 분류 모델이라 할 수 있다.

2-1. 일반 파라메터


booster: 어떤 부스터 구조를 쓸지 결정한다. 이것은 gbtree, gblinear, dart가 있다.

nthread: 몇 개의 쓰레드를 동시에 처리하도록 할지 결정한다. 디폴트는 “가능한 한 많이”.

num_feature: feature 차원의 숫자를 정해야 하는 경우 옵션을 세팅한다. 디폴트는 “가능한 한 많이.”



2-2. 부스팅 파라메터


eta: learning rate다. 트리에 가지가 많을수록 과적합(overfitting) 하기 쉽다. 매 부스팅 스탭마다 weight를 주어 부스팅 과정에 과적합이 일어나지 않도록 한다

gamma: 정보 획득(Information Gain)에서 -r로 표현한 바 있다. 이것이 커지면, 트리 깊이가 줄어들어 보수적인 모델이 된다. 디폴트 값은 0이다

max_depth: 한 트리의 maximum depth. 숫자를 키울수록 모델의 복잡도가 커진다. 과적합 하기 쉽다. 디폴트는 6. 

lambda(L2 reg-form): L2 Regularization Form에 달리는 weights이다. 숫자가 클수록 보수적인 모델이 된다

alpha(L1 reg-form): L1 Regularization Form weights다. 숫자가 클수록 보수적인 모델이 된다 



2-3. 학습 과정 파라메터

objective: 목적 함수다. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) 등 다양하다

eval_metric: 모델의 평가 함수를 조정하는 함수다. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) 등, 해당 데이터의 특성에 맞게 평가 함수를 조정한다



2-4. 커맨드 라인 파라메터
num_rounds: boosting 라운드를 결정한다. 랜덤 하게 생성되는 모델이니만큼 이 수가 적당히 큰 게 좋다. epoch 옵션과 동일하다

1. booster (부스터 모양)
2. eval_metric (평가 함수) / objective (목적 함수)
3. eta (러닝 레이트)
4. L1 form (L1 레귤러라이제이션 폼이 L2보다 아웃라이어에 민감하다)
5. L2 form

alpha [default=0] <br>
-L1 regularization term on weight (analogous to Lasso regression)<br>
-Can be used in case of very high dimensionality so that the algorithm runs faster when implemented


```python
pip install xgboost
```

    Collecting xgboost
      Using cached xgboost-1.1.1-py3-none-win_amd64.whl (54.4 MB)
    Requirement already satisfied: numpy in c:\users\goran\anaconda3\envs\tensor\lib\site-packages (from xgboost) (1.18.5)
    Requirement already satisfied: scipy in c:\users\goran\anaconda3\envs\tensor\lib\site-packages (from xgboost) (1.4.1)
    Installing collected packages: xgboost
    Successfully installed xgboost-1.1.1
    Note: you may need to restart the kernel to use updated packages.
    


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
model = XGBRegressor(n_estimators=100, learning_rate=0.08,max_depth=3,reg_alpha=1)
model.fit(X_train, y_train)
```




    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                 colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
                 importance_type='gain', interaction_constraints='',
                 learning_rate=0.08, max_delta_step=0, max_depth=3,
                 min_child_weight=1, missing=nan, monotone_constraints='()',
                 n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
                 reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=1,
                 tree_method='exact', validate_parameters=1, verbosity=None)



## 모델 평가


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.123864862015253
    R_squared : 0.15357400246773023
    

## 변수 중요도


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_115_1.png">
    


# SVR regression

Pros:<br>
It works really well with a clear margin of separation<br>
It is effective in high dimensional spaces.<br>
It is effective in cases where the number of dimensions is greater than the number of samples.<br>
It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.<br>
Cons:<br>
It doesn’t perform well when we have large data set because the required training time is higher<br>
It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping<br>
SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is included in the related SVC method of Python scikit-learn library.

parameters :
- kernel : 커널에는 Polynomial 커널, Sigmoid 커널, 가우시안 RBF 커널 등 종류가 많은데, 그 중 가장 성능이 좋아 자주 사용되는 것이 가우시안 RBF 커널이다. 커널 기법은 주어진 데이터를 고차원 특징 공간으로 사상해주는 것이다
- gamma : gamma는 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정,결정 경계의 곡률을 조정한다고 말할 수도 있다 ,너무 낮으면 과소적합될 가능성이 크고, 너무 높으면 과대적합의 위험이 있다 <br>

- C : cost(C)이다. C는 얼마나 많은 데이터 샘플이 다른 클래스에 놓이는 것을 허용하는지를 결정한다. 작을 수록 많이 허용하고, 클 수록 적게 허용한다. 다른 말로, C값을 낮게 설정하면 이상치들이 있을 가능성을 크게 잡아 일반적인 결정 경계를 찾아내고, 높게 설정하면 반대로 이상치의 존재 가능성을 작게 봐서 좀 더 세심하게 결정 경계를 찾아낸다. "난 데이터 샘플하나도 잘못 분류할 수 없어!"라면 C를 높여야한다. 반대로 "몇 개는 놓쳐도 괜찮아, 이상치들이 꽤 있을 수도 있으니까"라면 C를 낮춰야한다. 높으면 overfitting 의 문제가 있다

## 모델 세우기


```python
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV 
SVR = SVR()
# gridsearch 로 최적의 parameter 를 고르자.
param_grid = {'C': [0.1, 1, 10, 100, 1000],  
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','linear','poly']}  
model = GridSearchCV(SVR, param_grid) 

# fitting the model for grid search 
model.fit(X_train, y_train) 
```




    GridSearchCV(estimator=SVR(),
                 param_grid={'C': [0.1, 1, 10, 100, 1000],
                             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
                             'kernel': ['rbf', 'linear', 'poly']})




```python
# 모델은 어떤 경우에 최고 좋았는가?
model.best_params_
```




    {'C': 100, 'gamma': 1, 'kernel': 'rbf'}



## 모델 평가


```python
from sklearn import metrics
y_pred = svr_rbf.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 8.79871044038635
    R_squared : 0.2010870121689774
    

# Light gbm

1. learning_rate

일반적으로 0.01 ~ 0.1 정도로 맞추고 다른 파라미터를 튜닝한다. 나중에 성능을 더 높일 때 learning rate를 더 줄인다.

2. num_iterations

기본값이 100인데 1000정도는 해주는게 좋다. 너무 크게하면 과적합이 발생할 수 있다.

같은 뜻으로 사용되는 옵션: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators

3. max_depth

-1로 설정하면 제한없이 분기한다. feature가 많다면 크게 설정한다. 파라미터 설정 시 우선적으로 설정한다.

4. boosting 

부스팅 방법: 기본값은 gbdt이며 정확도가 중요할때는 딥러닝의 드랍아웃과 같은 dart를 사용한다. 샘플링을 이용하는 goss도 있다.

- boosting 🔗︎, default = gbdt, options: gbdt, rf, dart, goss
- gbdt : traditional Gradient Boosting Decision Tree, aliases: gbrt
- rf : Random Forest, aliases: random_forest
- dart : Dropouts meet Multiple Additive Regression Trees
- goss : Gradient-based One-Side Sampling

5. bagging_fraction

배깅을 하기위해서 데이터를 랜덤 샘플링하여 학습에 사용한다. 비율은 0 < fraction <= 1 이며 0이 되지 않게 해야한다.

6. feature_fraction

feature_fraction이 1보다 작다면 LGBM은 매 iteration(tree)마다 다른 feature를 랜덤하게 추출하여 학습하게된다. 만약, 0.8로 값을 설정하면 매 tree를 구성할 때, feature의 80%만 랜덤하게 선택한다. 과적합을 방지하기 위해 사용할 수 있으며 학습 속도가 향상된다.

7. scale_pos_weight

클래스 불균형의 데이터 셋에서 weight를 주는 방식으로 positive를 증가시킨다. 기본값은 1이며 불균형의 정도에 따라 조절한다.

8. early_stopping_round

Validation 셋에서 평가지표가 더 이상 향상되지 않으면 학습을 정지한다. 평가지표의 향상이 n round 이상 지속되면 학습을 정지한다.

9. lambda_l1, lambda_l2

정규화를 통해 과적합을 방지할 수 있지만, 정확도를 저하시킬수도 있기 때문에 일반적으로 default 값인 0으로 둔다.

더 빠른 속도
- bagging_fraction
- max_bin은 작게
- save_binary를 쓰면 데이터 로딩속도가 빨라짐
- parallel learning 사용

더 높은 정확도
- max_bin을 크게
- num_iterations 는 크게하고 learning_rate는 작게
- num_leaves를 크게(과적합의 원인이 될 수 있음)
- boosting 알고리즘 'dart' 사용

과적합을 줄이기
- max_bin을 작게
- num_leaves를 작게
- min_data_in_leaf와 min_sum_hessian_in_leaf 사용하기


**PARAMETERS 파라미터 소개**

max_depth : Tree의 최대 깊이를 말합니다. 이 파라미터는 모델 과적합을 다룰 때 사용됩니다. 만약 여러분의 모델이 과적합된 것 같다고 느끼신다면 제 조언은 max_depth 값을 줄이라는 것입니다.

min_data_in_leaf : Leaf가 가지고 있는 최소한의 레코드 수입니다. 디폴트값은 20으로 최적 값입니다. 과적합을 해결할 때 사용되는 파라미터입니다.

feature_fraction : Boosting (나중에 다뤄질 것입니다) 이 랜덤 포레스트일 경우 사용합니다. 0.8 feature_fraction의 의미는 Light GBM이 Tree를 만들 때 매번 각각의 iteration 반복에서 파라미터 중에서 80%를 랜덤하게 선택하는 것을 의미합니다.

bagging_fraction : 매번 iteration을 돌 때 사용되는 데이터의 일부를 선택하는데 트레이닝 속도를 높이고 과적합을 방지할 때 주로 사용됩니다.

early_stopping_round : 이 파라미터는 분석 속도를 높이는데 도움이 됩니다. 모델은 만약 어떤 validation 데이터 중 하나의 지표가 지난 early_stopping_round 라운드에서 향상되지 않았다면 학습을 중단합니다. 이는 지나친 iteration을 줄이는데 도움이 됩니다.

lambda : lambda 값은 regularization 정규화를 합니다. 일반적인 값의 범위는 0 에서 1 사이입니다.

min_gain_to_split : 이 파라미터는 분기하기 위해 필요한 최소한의 gain을 의미합니다. Tree에서 유용한 분기의 수를 컨트롤하는데 사용됩니다.

max_cat_group : 카테고리 수가 클 때, 과적합을 방지하는 분기 포인트를 찾습니다. 그래서 Light GBM 알고리즘이 카테고리 그룹을 max_cat_group 그룹으로 합치고 그룹 경계선에서 분기 포인트를 찾습니다. 디폴트 값은 64 입니다.

**Core Parameters 핵심 파라미터 소개**

Task : 데이터에 대해서 수행하고자 하는 임무를 구체화합니다. train 트레이닝일 수도 있고 predict 예측일 수도 있습니다.

application : 가장 중요한 파라미터로, 모델의 어플리케이션을 정하는데 이것이 regression 회귀분석 문제인지 또는 classification 분류 문제인지를 정합니다. Light GBM에서 디폴트는 regression 회귀분석 모델입니다.

regression: 회귀분석
binary: 이진 분류
multiclass: 다중 분류
boosting : 실행하고자 하는 알고리즘 타입을 정의합니다. 디폴트값은 gdbt 입니다.

- gdbt : Traditional Gradient Boosting Decision Tree
- rf : Random Forest
- dart : Dropouts meet Multiple Additive Regression Trees
- goss : Gradient-based One-Side Sampling

num_boost_round : boosting iteration 수로 일반적으로 100 이상입니다.

learning_rate : 최종 결과에 대한 각각의 Tree에 영향을 미치는 변수입니다. GBM은 초기의 추정값에서 시작하여 각각의Tree 결과를 사용하여 추정값을 업데이트 합니다. 학습 파라미터는 이러한 추정에서 발생하는 변화의 크기를 컨트롤합니다. 일반적인 값은 0.1, 0.001, 0.003 등등이 있습니다.

num_leaves : 전체 Tree의 leave 수 이고, 디폴트값은 31입니다.

device : 디폴트 값은 cpu 인데 gpu로 변경할 수도 있습니다.

**Metric parameter 지표 파라미터**

metric : 모델을 구현할 때 손실을 정하기 때문에 중요한 변수 중에 하나입니다. regression과 classification 을 위한 일반적인 손실 값이 아래에 나와있습니다.

- mae : mean absolute error
- mse : mean squared error
- binary_logloss : loss for binary classification
- multi_logloss : loss for multi classification

**IO parameter IO 파라미터**

max_bin : feature 값의 최대 bin 수를 의미합니다.

categorical_feature : 범주형 feature의 인덱스를 의미합니다. 만약 categorical_features 가 0, 1, 2 이면 column 0, column 1, column 2 가 범주형 변수들입니다.

ignore_column : categorical_features와 동일한 것인데 범주형 feature로써 특정 칼럼을 고려하지 않는 것입니다. 그 변수들을 무시하는 것입니다.

save_binary : 데이터 파일의 메모리 사이즈를 처리해야 한다면 이 파라미터 값을 True로 설정하십시오. 이 값을 True로 세팅하면 데이터 세트를 바이너리 파일로 저장할 것이고, 이 바이너리 파일은 다음에 데이터를 읽어올 때 그 속도를 줄여줄 것입니다.

## 모델 세우기


```python
# dataset train/test set 으로 나누기
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```


```python
# 먼저 train 셋을 다시 validation 과 train 셋으로 나눕니다
# 나누는 이유는 lgb 는 DNN 처럼 early stopping 이 있어, 어디까지 tree 를 학습시킬지 결정해야 하기 떄문
# 그래서 validation 의 score 로 중단할지 말지를 결정하기 때문에 필요하다.
X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=0)
```


```python
import lightgbm as lgb
train_ds = lgb.Dataset(X_tra, label = y_tra) # lgb 쓰려면 lgb dataset 형태로 만들어야한다.
val_ds = lgb.Dataset(X_val, label = y_val)
params = {'learning_rate': 0.01,
          'num_iterations' : 500, # 몇번 반복할지
          'max_depth': 16, 
          'boosting': 'gbdt', # 부스팅을 진
          'objective': 'regression', # regression 문제일떄는 이렇게 지정해주어야한다.
          'metric': 'mse', # metric
          'num_leaves': 144,
          'feature_fraction': 0.9,
          'bagging_fraction': 0.7,
          'bagging_freq': 5,
          'seed':2020}
# 그리고 파라미터는 이런 dic 형태로 정의한 다음 train 시에 해주는게 좋다.
# 그렇지 않으면 너무 귀찮다.
```


```python
model = lgb.train(params,
                  train_set = train_ds, 
                  valid_sets = val_ds,
                  verbose_eval=10, # 10 번마다 결과 보여주기 
                  early_stopping_rounds=20) # 20번의 iteration 동안 계속 안좋아지면 거기서 멈춤
 
y_pred=model.predict(X_test)
```

    Training until validation scores don't improve for 20 rounds
    [10]	valid_0's l2: 10.7871
    [20]	valid_0's l2: 10.4263
    [30]	valid_0's l2: 10.1375
    [40]	valid_0's l2: 9.95388
    [50]	valid_0's l2: 9.7608
    [60]	valid_0's l2: 9.60271
    [70]	valid_0's l2: 9.4487
    [80]	valid_0's l2: 9.35395
    [90]	valid_0's l2: 9.29904
    [100]	valid_0's l2: 9.22571
    [110]	valid_0's l2: 9.16716
    [120]	valid_0's l2: 9.10326
    [130]	valid_0's l2: 9.0634
    [140]	valid_0's l2: 8.99244
    [150]	valid_0's l2: 8.9855
    [160]	valid_0's l2: 8.94343
    [170]	valid_0's l2: 8.9201
    [180]	valid_0's l2: 8.92365
    [190]	valid_0's l2: 8.90043
    [200]	valid_0's l2: 8.9158
    [210]	valid_0's l2: 8.94221
    Early stopping, best iteration is:
    [191]	valid_0's l2: 8.89466
    

## 모델 평가


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
```

    MSE : 9.002660123598709
    

## 변수 중요도


```python
ax = lgb.plot_importance(model, max_num_features=80, figsize=(15,15))
plt.show()
```


    
<img src="/assets/images/1.Regression/output_135_0.png">
    


# Catboost

## 모델 세우기


```python
from catboost import CatBoostRegressor
# Initialize data
from sklearn.model_selection import train_test_split
X_train_2, X_eval, y_train_2, y_eval = train_test_split(X_train, y_train, test_size=0.20, random_state=0)
```


```python
CatBoostRegressor()
```




    <catboost.core.CatBoostRegressor at 0x1a7b8a3be88>




```python
# https://catboost.ai/docs/concepts/python-reference_parameters-list.html (parameter 설명)
model = CatBoostRegressor(iterations=100, # 몇개의 trees 로 훈련
                          learning_rate=0.1, # 학습 속도
                          max_depth=10, # 트리의 최대 깊이
                          loss_function = 'RMSE', # https://catboost.ai/docs/references/eval-metric__supported-metrics.html
                        )
```


```python
# Fit model
model.fit(X_train_2,y_train_2, # 각각 적용할 X,y set 을 넣어준다.
          eval_set=(X_eval,y_eval), # 훈련을 지켜보면서, 판단할 evaluation set
          verbose_eval = 10, # 몇번마다 한번씩 결과를 보여줄지
          )
```

    0:	learn: 3.2855664	test: 3.3164971	best: 3.3164971 (0)	total: 226ms	remaining: 22.4s
    10:	learn: 2.6975929	test: 3.1766838	best: 3.1742496 (9)	total: 658ms	remaining: 5.32s
    20:	learn: 2.3046747	test: 3.1270483	best: 3.1270483 (20)	total: 1.11s	remaining: 4.18s
    30:	learn: 2.0082468	test: 3.1024159	best: 3.1024159 (30)	total: 1.57s	remaining: 3.49s
    40:	learn: 1.7946485	test: 3.0957864	best: 3.0957864 (40)	total: 2.01s	remaining: 2.89s
    50:	learn: 1.6009077	test: 3.0856002	best: 3.0856002 (50)	total: 2.45s	remaining: 2.35s
    60:	learn: 1.4313088	test: 3.0860660	best: 3.0828759 (58)	total: 2.9s	remaining: 1.86s
    70:	learn: 1.2964716	test: 3.0753949	best: 3.0749393 (67)	total: 3.35s	remaining: 1.37s
    80:	learn: 1.1765645	test: 3.0761430	best: 3.0746362 (72)	total: 3.77s	remaining: 884ms
    90:	learn: 1.0875473	test: 3.0766189	best: 3.0728343 (83)	total: 4.19s	remaining: 414ms
    99:	learn: 1.0049267	test: 3.0737350	best: 3.0728343 (83)	total: 4.59s	remaining: 0us
    
    bestTest = 3.072834339
    bestIteration = 83
    
    Shrink model to first 84 iterations.
    




    <catboost.core.CatBoostRegressor at 0x1a7b8d136c8>



## 모델 평가


```python
from sklearn import metrics
preds = model.predict(X_eval)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.002660123598709
    R_squared : 0.1500162325753287
    


```python

```
