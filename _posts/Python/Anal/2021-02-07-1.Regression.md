---
title:  "1.Regression"
excerpt: "íšŒê·€ ë°©ë²•ë¡ ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤."
categories:
  - Py_Analysis
tags:
  - ê¸°ì´ˆ
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"

use_math: true
---

```python
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.datasets import make_regression
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn import metrics # ëª¨ë¸í‰ê°€ì‹œ ì´ìš©
```


```python
# dataset í˜•ì„±í•˜ê¸°
X, y = make_regression(n_samples=2000, # sample ì˜ ìˆ˜ëŠ” 2000ê°œ
                       n_features=20, # x ë³€ìˆ˜ëŠ” 20ê°œ
                       n_informative=4, # ê·¸ì¤‘ y ë³€ìˆ˜ì™€ ê´€ë ¨ë˜ëŠ” ë³€ìˆ˜ëŠ” 4ê°œ  
                       noise = 3       # nosie 
                      ,effective_rank=15 #  x ë³€ìˆ˜ì¤‘ ì„œë¡œ ë…ë¦½ì¸ ìˆ˜ (5ê°œëŠ” ì„œë¡œ ê´€ë ¨)
                      ,tail_strength=0.3 # ê´€ë ¨ëœ ë³€ìˆ˜ì˜ ê´€ë ¨ì„±
                      ,random_state=0) # random state ê³ ì •
X = pd.DataFrame(X)
X[[1,4,6,10,13]]=X[[1,4,6,10,13]]*-1
```


```python
# dataset train/test set ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```


```python
y
```




    array([ 4.7989583 ,  2.60992309,  3.15534149, ..., -0.88908075,
           -5.57811736,  3.9804761 ])



# Multi Linear Regression

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```




    LinearRegression()



## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test,y_pred))
print("R squared :", metrics.r2_score(y_test,y_pred))
```

    MSE : 8.611762902052247
    R squared : 0.20108198497901186
    

# Polynomial regression

ë³€ìˆ˜ê°€ ì ë‹¤ë©´, ë°ì´í„° ë¶„ì„ì‹œ ë¶€ì •í™• í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ polynomial ë¡œ ë³€ìˆ˜ì˜ ê°¯ìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ ì˜ˆì¸¡ë„ë¥¼ ë†’í˜€ë³´ì

## ë°ì´í„° ì „ì²˜ë¦¬

ë‹¤í•­íšŒê·€ëŠ”, ë°ì´í„°ë¥¼ ëŒë¦¬ê¸° ì „, x1 x2 -> x1^2 , x1x2, x2^2 ì²˜ëŸ¼ ë°ì´í„°ë¥¼ ë°”ê¾¸ì–´ì•¼ í•œë‹¤.ë¥¼ í•´ì£¼ì–´ì•¼í•œë‹¤.


```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures()  
# degree : ëª‡ ì°¨ìˆ˜ê¹Œì§„ ëŠ˜ë¦´ì§€ ê²°ì •
#        : (default)=2
# interaction_only : only ìê¸° ìì‹ ì˜ ê±°ë“­ì œê³±ë§Œ ë„£ì„ì§€ ë§ì§€. (ex x1 x2 -> x1^2 , ^2)
#                  : (default)=False (interaction í•­ ë„£ê¸°) (EX x1 x2 -> x1^2 , x1x2, x2^2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.fit_transform(X_test)
```

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
model = LinearRegression()
model.fit(X_train_poly, y_train)
```




    LinearRegression()



## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
y_pred = model.predict(X_test_poly)
print ("MSE :", metrics.mean_squared_error(y_test,y_pred))
print("R squared :", metrics.r2_score(y_test,y_pred))
```

    MSE : 10.588639961853904
    R squared : 0.01768600502455997
    

# Ridge regression

$w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda \sum_{j=1}^M w_j^2 \right) $

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.linear_model import RidgeCV
from sklearn.linear_model import Ridge

para_range = np.logspace(-3, 3, num=50) # 10^-3 ~ 10^3 
model = RidgeCV(alphas = para_range , cv=5)
# cross validaaion ì„ 5 fold cross validation ìœ¼ë¡œ ì§€ì • , 
# para_range ì˜ ë²”ìœ„ë§Œí¼ ,cv ë¥¼ í•´ì„œ ìµœì ì˜ alpha ë¥¼ êµ¬í•˜ê² ë‹¤ëŠ” ì˜ë¯¸
# RidgeCV = cv ì™€ ridge ë¥¼ ë™ì‹œì— í•©ì¹œ model ì´ë‹¤.
model.fit(X_train, y_train) ;
# X_train, Y_train ìœ¼ë¡œ ridgecv ë¥¼ fitting í•´ì•¼ ë¹„ë¡œì†Œ ì˜ë¯¸ê°€ ìƒê¸´ë‹¤. ìœ„ëŠ” setting ì‘ì—…ì„.
#ridgecv.fit ì€ alpha ë¥¼ ìœ„ì—ì„œ êµ¬í•œ ìµœì ì˜ ê°’ì„ ì¨ì„œ ridge regression ì„ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.
```

## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics

predicted = model.predict(X_test) 
# ìš°ë¦¬ê°€ fitting í•œ coefficient ë¡œ X_test ë¥¼ ì´ìš©í•´ Y_test ë¥¼ predict
#ridgecv.alpha_ # Estimated regularization parametor (ìµœì ê°’)
#ridgecv.coef_ # fitting í•œ coff ê°’ë“¤

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',model.score(X_test, y_test)) # C-V ë¡œ ì°¾ì€ ìµœì ì˜ ridge ë¡œ ê³„ì‚°í•œ R^2
```

    MSE : 8.606003653527091
    R_squared : 0.20161627365510082
    

# ElasticNet Regression

**[ëª¨ë¸ ì›ë¦¬]**
- $w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda_1 \sum_{j=1}^M | w_j | + \lambda_2 \sum_{j=1}^M w_j^2 \right)$ ë¡œ w ë¥¼ ì •í•˜ëŠ” regression

**[Hyperparameter]** 
- sklearn ì—ì„œëŠ” $0.5 \times \text{RSS}/N + \text{alpha} \times \big( 0.5 \times (1-\text{L1_wt})\sum w_i^2 + \text{L1_wt} \sum |w_i| \big)$ ì„ ìµœì†Œí™” í•œë‹¤.(ì‚¬ì‹¤ìƒ ìœ„ë‘ ë˜‘ê°™ë‹¤.)
- l1_ratio : float, default=0.5
     - For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2



```python
from sklearn.linear_model import ElasticNetCV
from sklearn.linear_model import ElasticNet
```

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
ElasticCV = ElasticNetCV(l1_ratio =[.1, .5, .7, .9, .95, .99,1], cv=5, random_state=0)
# l1 Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in  [.1, .5, .7, .9, .95, .99, 1] 

ElasticCV.fit(X_train, y_train)
print(ElasticCV.alpha_) # alpha ê°’
print(ElasticCV.l1_ratio_) #l1 ratio 1 ì´ë©´ ì‚¬ì‹¤ìƒ lasso ì´ë‹¤... 1 ì´ ë‚˜ì˜¤ë©´ ê·¸ëƒ¥ lasso ë¡œ í•˜ë¼ëŠ” ì˜ë¯¸
```

    0.0009717603376703314
    1.0
    

## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics

predicted = ElasticCV.predict(X_test) 
# ìš°ë¦¬ê°€ fitting í•œ coefficient ë¡œ X_test ë¥¼ ì´ìš©í•´ Y_test ë¥¼ predict
#ridgecv.alpha_ # Estimated regularization parametor (ìµœì ê°’)
#ridgecv.coef_ # fitting í•œ coff ê°’ë“¤

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',model.score(X_test, y_test)) # C-V ë¡œ ì°¾ì€ ìµœì ì˜ ridge ë¡œ ê³„ì‚°í•œ R^2
```

    MSE : 8.607510464307282
    R_squared : 0.20161627365510082
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
coef = pd.Series(ElasticCV.coef_, index = X_train.columns).sort_values()
imp_coef = pd.concat([coef.head(5), coef.tail(5)])
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Model")
```




    Text(0.5, 1.0, 'Coefficients in the Model')




    
<img src="/assets/images/1.Regression/output_33_1.png">
    


# lasso regression

$w = \text{arg}\min_w \left( \sum_{i=1}^N e_i^2 + \lambda \sum_{j=1}^M | w_j | \right)$

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
alphas = np.logspace(-3, 3, num=50) # 10^-3 ~ 10^3 
lassocv = LassoCV(alphas = alphas, cv=5)
# ìœ„ ridge ë•Œì™€ ë™ì¼
lassocv.fit(X_train, y_train) ;
```

## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
predicted = lassocv.predict(X_test) 
# ìš°ë¦¬ê°€ fitting í•œ coefficient ë¡œ X_test ë¥¼ ì´ìš©í•´ Y_test ë¥¼ predict
#lassocv.alpha_ # Estimated regularization parametor (ìµœì ê°’)
#lassocv.coef_ # fitting í•œ coff ê°’ë“¤

print ("MSE :", metrics.mean_squared_error(y_test, predicted))
print('R_squared :',lassocv.score(X_test, y_test)) # C-V ë¡œ ì°¾ì€ ìµœì ì˜ ridge ë¡œ ê³„ì‚°í•œ R^2
```

    MSE : 8.607540368924692
    R_squared : 0.20147371171638206
    

## ë³€ìˆ˜ ì¤‘ìš”ë„

ìƒìœ„ 6ê°œì˜ ê´€ë ¤ìˆëŠ” ê³„ìˆ˜ë“¤ì„ ë³´ì—¬ì¤€ë‹¤.


```python
coef = pd.Series(lassocv.coef_, index = X_train.columns).sort_values()
imp_coef = pd.concat([coef.head(5), coef.tail(5)])
imp_coef.plot(kind = "barh")
plt.title("Coefficients in the Model")
```




    Text(0.5, 1.0, 'Coefficients in the Model')




    
<img src="/assets/images/1.Regression/output_42_1.png">
    


# Kernel Regression

ë…ë¦½ë³€ìˆ˜ ë²¡í„°  x ë¥¼ ì…ë ¥ìœ¼ë¡œ ê°€ì§€ëŠ” ì—¬ëŸ¬ê°œì˜ ë¹„ì„ í˜• í•¨ìˆ˜  $\phi_j(x)$ ë“¤ì„ ìƒê°í•´ ë‚´ì–´ ì›ë˜ì˜ ì…ë ¥ ë³€ìˆ˜  x ëŒ€ì‹  $\phi_j(x)$ ë“¤ì„ ì…ë ¥ë³€ìˆ˜ë¡œ ì‚¬ìš©í•œ ë‹¤ìŒê³¼ ê°™ì€ ëª¨í˜•ì„ ì“°ë©´ ë” ì¢‹ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤.

$ y_i = \sum_{j=1}^{M} w_j \phi_j(x)  = w^T \phi(x) $



# K-Neighbors Regression

<b> ëª¨ë¸ ì›ë¦¬ </b>

- KNNì€ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê¸°ì¡´ ë°ì´í„° ê°€ìš´ë° ê°€ì¥ ê°€ê¹Œìš´ kê°œ ì´ì›ƒì˜ ì •ë³´ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤
- Classification ì¸ ê²½ìš°, test point x ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ train set ì˜ k ê°œ ì´ì›ƒì¤‘ ì œì¼ ë§ì€ classë¡œ ì˜ˆì¸¡ì„ í•˜ê²Œ ë˜ê³ 
- Regression ì˜ ê²½ìš° test point x ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ train set ì˜ k ê°œì˜ ì´ì›ƒì˜ ê°’ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.

<b> ëª¨ë¸ ê°œìš” </b>

- ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ë°©ë²•ê³¼ ì´ì›ƒì˜ ìˆ˜ 2ê°œê°€ hyper parameter ì´ë‹¤.
- ê±°ë¦¬ë¥¼ ì¬ëŠ” ë°©ë²•ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ì˜ ë™ì‘í•˜ëŠ” ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ë°©ì‹ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë‹¤ë£¨ì§€ ì•Šê² ë‹¤.

<b> ì£¼ì˜ì  </b>
- k-NN ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ë• 'ê±°ë¦¬' ê°€ ê¸°ì¤€ì´ ë˜ê¸°ë•Œë¬¸ì— ë°ì´í„° ê°’ë“¤ì„ ëª¨ë‘ í‘œì¤€í™” í•˜ì—¬ ì „ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

<b> ì¥ì  </b>
- ì´í•´í•˜ê¸° ë§¤ìš° ì‰¬ìš´ ëª¨ë¸ 
- parameterì„ ë§ì´ ì¡°ì •í•˜ì§€ ì•Šì•„ë„ ìì£¼ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.
- ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ë³´ê¸° ì „ì— ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì‹œì‘ì ì…ë‹ˆë‹¤.

<b> ë‹¨ì  </b>
- ë³´í†µ ìµœê·¼ì ‘ ì´ì›ƒ ëª¨ë¸ì€ ë§¤ìš° ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ, í›ˆë ¨ ì„¸íŠ¸ê°€ ë§¤ìš° í¬ë©´ (íŠ¹ì„±ì˜ ìˆ˜ë‚˜ ìƒ˜í”Œì˜ ìˆ˜ê°€ í´ ê²½ìš°) ì˜ˆì¸¡ì´ ëŠë ¤ì§‘ë‹ˆë‹¤. 
- ê·¸ë¦¬ê³  (ìˆ˜ë°± ê°œ ì´ìƒì˜) ë§ì€ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ì…‹ì—ëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë©°, íŠ¹ì„± ê°’ ëŒ€ë¶€ë¶„ì´ 0ì¸ ë°ì´í„°ì…‹ê³¼ëŠ” íŠ¹íˆ ì˜ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- k-ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì´ ì´í•´í•˜ê¸´ ì‰½ì§€ë§Œ, ì˜ˆì¸¡ì´ ëŠë¦¬ê³  ë§ì€ íŠ¹ì„±ì„ ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•´ í˜„ì—…ì—ì„œëŠ” ì˜ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.

<b> Hyperparameter </b>


n_neighbors
- ì´ì›ƒì˜ ìˆ˜


## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
model = KNeighborsRegressor() 
param_grid ={'n_neighbors' : np.arange(1,10)}
model = GridSearchCV(model,param_grid,cv=5)
model.fit(X_train,y_train)
```




    GridSearchCV(cv=5, estimator=KNeighborsRegressor(),
                 param_grid={'n_neighbors': array([1, 2, 3, 4, 5, 6, 7, 8, 9])})




```python
print(model.best_params_) # ìš°ë¦¬ê°€ ì¶”ì •í•œ best parameter
print(model.best_score_) # ìš°ë¦¬ê°€ ì§€ì •í•œ scoring ì— ê¸°ë°˜í•œ ì œì¼ ì¢‹ì€ score
```

    {'n_neighbors': 9}
    0.07674181821732715
    

## ëª¨ë¸ í‰ê°€


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print("R squared :", metrics.r2_score(y_test, y_pred))
```

    MSE : 10.361920123323124
    R squared : 0.03871893003945159
    

#  DecisionTree Regression

<b> [ëª¨ë¸ ì›ë¦¬] </b>

ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ í•™ìŠµ ê³¼ì •ì€ ì…ë ¥ ë³€ìˆ˜ ì˜ì—­ì„ ë‘ ê°œë¡œ êµ¬ë¶„í•˜ëŠ” ì¬ê·€ì  ë¶„ê¸°(recursive partitioning)ì™€ ë„ˆë¬´ ìì„¸í•˜ê²Œ êµ¬ë¶„ëœ ì˜ì—­ì„ í†µí•©í•˜ëŠ” ê°€ì§€ì¹˜ê¸°(pruning) ë‘ ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. 

ì¬ê·€ì  ë¶„ê¸°
- ìš°ì„  ë°ì´í„°ë¥¼ í•œ ë³€ìˆ˜ ê¸°ì¤€(exì£¼íƒ í¬ê¸°)ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤. ì´í›„ ê°€ëŠ¥í•œ ëª¨ë“  ë¶„ê¸°ì ì— ëŒ€í•´ ì—”íŠ¸ë¡œí”¼/ì§€ë‹ˆê³„ìˆ˜ë¥¼ êµ¬í•´ ë¶„ê¸° ì „ê³¼ ë¹„êµí•´ ì •ë³´íšë“ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. 
- ê·¸ë¦¬ê³  ë‹¤ë¥¸ ë³€ìˆ˜ ê¸°ì¤€ ìœ¼ë¡œ ì •ë ¬í•œ í›„ ìœ„ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
- ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ ê°€ìš´ë° ì •ë³´íšë“ì´ ê°€ì¥ í° ë³€ìˆ˜ì™€ ê·¸ ì§€ì ì„ íƒí•´ ì²«ë²ˆì§¸ ë¶„ê¸°ë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤. 
- ì´í›„ ë˜ ê°™ì€ ì‘ì—…ì„ ë°˜ë³µí•´ ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸â€¦ ì´ë ‡ê²Œ ë¶„ê¸°ë¥¼ full tree ê°€ ë ë•Œê¹Œì§€ ê³„ì† í•´ ë‚˜ê°‘ë‹ˆë‹¤.

ê°€ì§€ì¹˜ê¸°
- ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ í•™ìŠµì˜ ë˜ë‹¤ë¥¸ ì¶•ì€ ê°€ì§€ì¹˜ê¸°(pruning)ì…ë‹ˆë‹¤. ëª¨ë“  terminal nodeì˜ ìˆœë„ê°€ 100%ì¸ ìƒíƒœë¥¼ Full treeë¼ê³  í•˜ëŠ”ë°ìš”. ì´ë ‡ê²Œ Full treeë¥¼ ìƒì„±í•œ ë’¤ ì ì ˆí•œ ìˆ˜ì¤€ì—ì„œ terminal nodeë¥¼ ê²°í•©í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ë¶„ê¸°ê°€ ë„ˆë¬´ ë§ì•„ì„œ í•™ìŠµë°ì´í„°ì— ê³¼ì í•©(overfitting)í•  ì—¼ë ¤ê°€ ìƒê¸°ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
- ê°€ì§€ì¹˜ê¸°ëŠ” ë°ì´í„°ë¥¼ ë²„ë¦¬ëŠ” ê°œë…ì´ ì•„ë‹ˆê³  ë¶„ê¸°ë¥¼ í•©ì¹˜ëŠ”(merge) ê°œë…ìœ¼ë¡œ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.
- ê°€ì§€ì¹˜ê¸°ì˜ ê²½ìš° ë¹„ìš©í•¨ìˆ˜(costfunction = Err(T)+Î±Ã—L(T) (Err(T) : ì˜¤ë¶„ë¥˜ìœ¨ , L(T) : terminal node(leaf)ì˜ ìˆ˜(êµ¬ì¡°ì˜ ë³µì¡ë„), Î± : ê°€ì¤‘ì¹˜ ìƒìˆ˜ ë³´í†µ 0.1~0.01 )  ê°€ ìµœì†Œê°€ ë˜ë„ë¡ ê°€ì§€ë¥¼ ì¹˜ê²Œ ë©ë‹ˆë‹¤.

<b> [ì¥ì ] </b>
- ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ ì‰½ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆì–´ì„œ ë¹„ì „ë¬¸ê°€ë„ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤(ë¹„êµì  ì‘ì€ íŠ¸ë¦¬ì¼ ë•Œ). 
- ê·¸ë¦¬ê³  ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì— êµ¬ì• ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ë³€ìˆ˜ë§ˆë‹¤ íŠ¸ë¦¬ì—ì„œëŠ” ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ ë°ì´í„°ë¥¼ ë¶„í• í•˜ëŠ” ë° ë°ì´í„° ìŠ¤ì¼€ì¼ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ ê²°ì • íŠ¸ë¦¬ì—ì„œëŠ” íŠ¹ì„±ì˜ ì •ê·œí™”ë‚˜ í‘œì¤€í™” ê°™ì€ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤. íŠ¹íˆ íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ì´ ì„œë¡œ ë‹¤ë¥´ê±°ë‚˜ ì´ì§„ íŠ¹ì„±(ë²”ì£¼í˜•)ê³¼ ì—°ì†ì (ì—°ì†í˜•)ì¸ íŠ¹ì„±ì´ í˜¼í•©ë˜ì–´ ìˆì„ ë•Œë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤.
- ê³„ì‚°ë³µì¡ì„± ëŒ€ë¹„ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒìœ¼ë¡œ ì •í‰ì´ ë‚˜ ìˆìŠµë‹ˆë‹¤. 

<b> [ë‹¨ì ] </b>
- ê²°ì •ê²½ê³„(decision boundary)ê°€ ë°ì´í„° ì¶•ì— ìˆ˜ì§ì´ì–´ì„œ íŠ¹ì • ë°ì´í„°ì—ë§Œ ì˜ ì‘ë™í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
- data ì˜ ë³€í™”ì— ë§¤ìš° ë¯¼ê°í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì•½ê°„ë§Œ ë³€í™”í•´ë„ tree ê°€ ì™„ì „íˆ ë³€í™”í•˜ê¸° ë–„ë¬¸ì…ë‹ˆë‹¤.
- ë‘ ë³€ìˆ˜ê°€ multicollinearity ê´€ê³„ì— ìˆìœ¼ë©´ decision tree will greedily choose the best one. ì¦‰ í•œ ë³€ìˆ˜ëŠ” ì•„ì˜ˆ ì³ë‚´ë²„ë¦½ë‹ˆë‹¤. ë‘ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì´ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ”ê²ƒì…ë‹ˆë‹¤.
- sample ë²”ìœ„ ë°”ê¹¥ì˜ ê°’ì„ ì˜ˆì¸¡í•  ë•Œì—ëŠ” ì˜¤íˆë ¤ ì„ í˜•íšŒê·€ë³´ë‹¤ í›¨ì”¬ ì•ˆì¢‹ì€ fitting ì´ ë‚˜ì˜µë‹ˆë‹¤.
- ê²°ì • íŠ¸ë¦¬ì˜ ì£¼ìš” ë‹¨ì ì€ ì‚¬ì „ ê°€ì§€ì¹˜ê¸°ë¥¼ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ê³¼ëŒ€ì í•©ë˜ëŠ” ê²½í–¥ì´ ìˆì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‹¤ìŒì— ì„¤ëª…í•  ì•™ìƒë¸” ë°©ë²•ì„ ë‹¨ì¼ ê²°ì • íŠ¸ë¦¬ì˜ ëŒ€ì•ˆìœ¼ë¡œ í”íˆ ì‚¬ìš©í•©ë‹ˆë‹¤.


<b> [Hyperparameter] </b>

criterion 
- ë¶ˆìˆœì„±ì˜ ê¸°ì¤€ì„ ë­ë¡œ í• ì§€ 'gini' / 'entropy' ê°€ëŠ¥
- (defalut)= 'gini'

max_depth 
- íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´. 
- ì´ë¥¼ ì´ìš©í•´ ì‚¬ì „ ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ê³ / overfitting ì„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.
- (defalut) : full tree ê°€ ë ë•Œê¹Œì§€ í™•ì¥.

min_samples_split 
- ë…¸ë“œì—ì„œ ê°€ì§€ ë¶„ë¦¬í•  ë–„ í•„ìš”í•œ ìµœì†Œ sample ê°¯ìˆ˜ì— ì œí•œì„ ì¤€ë‹¤.
- (default) = 2 

min_samples_leaf
- leaf ì—ì„œ ê°€ì ¸ì•¼ í•  ìµœì†Œ sample 
- (default) = 1

max_features
- Decision tree ë¥¼ ë§Œë“¤ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ì˜ ê°¯ìˆ˜ ì œí•œ
- (default) = ì´ ë³€ìˆ˜ ê°¯ìˆ˜ ì‚¬ìš©

<b> [Hyperparameter tuning] </b>
-  max_depth í•˜ë‚˜ë§Œ ì§€ì •í•´ë„ ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ë° ì¶©ë¶„í•©ë‹ˆë‹¤.

<b>[feature importance]</b>

ê³„ì‚° ì›ë¦¬

- ë…¸ë“œ ì¤‘ìš”ë„ ë¶€í„° ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤. ë…¸ë“œ ì¤‘ìš”ë„ë€ Information Gainì„ ê²°êµ­ ë§í•©ë‹ˆë‹¤. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì—ì„œëŠ” ê·¸ ë•Œ Information Gainì„ ìµœëŒ€í™”í•˜ëŠ” featureë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œë¥¼ ì§¸ê²Œ ë©ë‹ˆë‹¤. ì–´ë–¤ ë…¸ë“œì˜ ë…¸ë“œ ì¤‘ìš”ë„ ê°’ì´ í¬ë‹¤ëŠ” ê²ƒì€ ê·¸ ë…¸ë“œì—ì„œ íŠ¹íˆ ë¶ˆìˆœë„ê°€ í¬ê²Œ ê°ì†Œí•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- ië²ˆì§¸ featureì˜ ì¤‘ìš”ë„ : ì „ì²´ ë…¸ë“œì˜ ì¤‘ìš”ë„ë¥¼ í•©í•œ ê²ƒ ëŒ€ë¹„ ië²ˆì§¸ featureì— ì˜í•´ ì§¸ì§„ ë…¸ë“œë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í•©í•œ ê²ƒ ì…ë‹ˆë‹¤.

ì˜ë¯¸

- ì´ ê°’ì€ 0~1 ì‚¬ì´ì˜ ìˆ«ìë¡œ, 0ì€ ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ëœ»ì´ê³  1ì€ ì™„ë²½í•˜ê²Œ íƒ€ê¹ƒ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í–ˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. íŠ¹ì„± ì¤‘ìš”ë„ì˜ ì „ì²´ í•©ì€ 1ì…ë‹ˆë‹¤.
- Scikit-learnì—ì„œëŠ” ì§€ë‹ˆ ì¤‘ìš”ë„(Gini Importance)ë¥¼ ì´ìš©í•´ì„œ ê° featureì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤
- ê·¸ëŸ¬ë‚˜ ì–´ë–¤ íŠ¹ì„±ì˜ feature_importance_ ê°’ì´ ë‚®ë‹¤ê³  í•´ì„œ ì´ íŠ¹ì„±ì´ ìœ ìš©í•˜ì§€ ì•Šë‹¤ëŠ” ëœ»ì€ ì•„ë‹™ë‹ˆë‹¤. ë‹¨ì§€ íŠ¸ë¦¬ê°€ ê·¸ íŠ¹ì„±ì„ ì„ íƒí•˜ì§€ ì•Šì•˜ì„ ë¿ì´ë©° ë‹¤ë¥¸ íŠ¹ì„±ì´ ë™ì¼í•œ ì •ë³´ë¥¼ ì§€ë‹ˆê³  ìˆì–´ì„œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„ í˜• ëª¨ë¸ì˜ ê³„ìˆ˜ì™€ëŠ” ë‹¬ë¦¬, íŠ¹ì„± ì¤‘ìš”ë„ëŠ” í•­ìƒ ì–‘ìˆ˜ì´ë©° íŠ¹ì„±ì´ ì–´ë–¤ í´ë˜ìŠ¤ë¥¼ ì§€ì§€í•˜ëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰ ì´ ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì´ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ ì•Œ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.
- í•˜ì§€ë§Œ feature ImportanceëŠ” ë‹¤ì†Œ biasedí•˜ë‹¤. íŠ¹íˆ, ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì—°ì†í˜• ë³€ìˆ˜ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ê°€ ë§¤ìš° ë§ì€ ë³€ìˆ˜, ì¦‰ â€˜high cardinalityâ€™ ë³€ìˆ˜ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ë”ìš± ë¶€í’€ë¦´ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  í•œë‹¤. ì™œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ”ì§€ëŠ” ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìœ¼ë‚˜, cardinalityê°€ í° ë³€ìˆ˜ì¼ ìˆ˜ë¡, ë…¸ë“œë¥¼ ì¨€ ê²Œ í›¨ì”¬ ë” ë§ì•„ì„œ ë…¸ë“œ ì¤‘ìš”ë„ ê°’ì´ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ê²Œ ì•„ë‹ê¹Œ ì‹¶ë‹¤.

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
# hyper parameter ì¶”ì •í•˜ê¸°
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

model = DecisionTreeRegressor(min_samples_split=50) 
param_grid ={'max_depth' : np.arange(1,10)}
grid_search = GridSearchCV(model,param_grid,cv=5)
# scoring ì€ default ì´ë¯€ë¡œ model ì˜ ìì²´ scoring ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. 
grid_search.fit(X_train,y_train)
print(grid_search.best_params_) # ìš°ë¦¬ê°€ ì¶”ì •í•œ best parameter
print(grid_search.best_score_) # ìš°ë¦¬ê°€ ì§€ì •í•œ scoring ì— ê¸°ë°˜í•œ ì œì¼ ì¢‹ì€ score
```

    {'max_depth': 3}
    0.16422632977859336
    


```python
# ìœ„ì—ì„œ êµ¬í•œ ìµœìƒì˜ parameter ë¡œ ì¶”ì •í•˜ê¸°
model = DecisionTreeRegressor(max_depth=3,min_samples_split=50)
model.fit(X_train, y_train)
```




    DecisionTreeRegressor(max_depth=3, min_samples_split=50)



## ëª¨ë¸ í‰ê°€


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print("R squared :", metrics.r2_score(y_test, y_pred))
```

    MSE : 10.167059541425555
    R squared : 0.05679625416763212
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_69_1.png">
    


# RandomForest Regressoion

ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íšŒê·€ íŠ¸ë¦¬ëŠ” ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ë¬¶ì–´ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë˜ ë‹¤ë¥¸ ì•™ìƒë¸” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¦„ì´ íšŒê·€ì§€ë§Œ ì´ ëª¨ë¸ì€ íšŒê·€ì™€ ë¶„ë¥˜ ëª¨ë‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 5 ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ëŠ” ë‹¬ë¦¬ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€ ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íšŒê·€ íŠ¸ë¦¬ì—ëŠ” ë¬´ì‘ìœ„ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ê°•ë ¥í•œ ì‚¬ì „ ê°€ì§€ì¹˜ê¸°ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íŠ¸ë¦¬ëŠ” ë³´í†µ í•˜ë‚˜ì—ì„œ ë‹¤ì„¯ ì •ë„ì˜ ê¹Šì§€ ì•Šì€ íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡ë„ ë¹ ë¦…ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ê·¼ë³¸ ì•„ì´ë””ì–´ëŠ” ì´ëŸ° ì–•ì€ íŠ¸ë¦¬ ê°™ì€ ê°„ë‹¨í•œ ëª¨ë¸(ì•½í•œ í•™ìŠµê¸°weak learnerë¼ê³ ë„ í•©ë‹ˆë‹¤)ì„ ë§ì´ ì—°ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°ê°ì˜ íŠ¸ë¦¬ëŠ” ë°ì´í„°ì˜ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ì„ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ì„œ íŠ¸ë¦¬ê°€ ë§ì´ ì¶”ê°€ë ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. 6

ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íŠ¸ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê²½ì—° ëŒ€íšŒì—ì„œ ìš°ìŠ¹ì„ ë§ì´ ì°¨ì§€í•˜ì˜€ê³  ì—…ê³„ì—ì„œë„ ë„ë¦¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ëŠ” ë§¤ê°œë³€ìˆ˜ ì„¤ì •ì— ì¡°ê¸ˆ ë” ë¯¼ê°í•˜ì§€ë§Œ ì˜ ì¡°ì •í•˜ë©´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•´ì¤ë‹ˆë‹¤.

ì•™ìƒë¸” ë°©ì‹ì— ìˆëŠ” ì‚¬ì „ ê°€ì§€ì¹˜ê¸°ë‚˜ íŠ¸ë¦¬ ê°œìˆ˜ ì™¸ì—ë„ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì—ì„œ ì¤‘ìš”í•œ ë§¤ê°œë³€ìˆ˜ëŠ” ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë³´ì •í•  ê²ƒì¸ì§€ë¥¼ ì œì–´í•˜ëŠ” learning_rateì…ë‹ˆë‹¤. í•™ìŠµë¥ ì´ í¬ë©´ íŠ¸ë¦¬ëŠ” ë³´ì •ì„ ê°•í•˜ê²Œ í•˜ê¸° ë•Œë¬¸ì— ë³µì¡í•œ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. n_estimators ê°’ì„ í‚¤ìš°ë©´ ì•™ìƒë¸”ì— íŠ¸ë¦¬ê°€ ë” ë§ì´ ì¶”ê°€ë˜ì–´ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì»¤ì§€ê³  í›ˆë ¨ ì„¸íŠ¸ì—ì„œì˜ ì‹¤ìˆ˜ë¥¼ ë°”ë¡œì¡ì„ ê¸°íšŒê°€ ë” ë§ì•„ì§‘ë‹ˆë‹¤.


ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„° ê°™ì´ ë§¤ìš° ì°¨ì›ì´ ë†’ê³  í¬ì†Œí•œ ë°ì´í„°ì—ëŠ” ì˜ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ° ë°ì´í„°ì—ëŠ” ì„ í˜• ëª¨ë¸ì´ ë” ì í•©í•©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë§¤ìš° í° ë°ì´í„°ì…‹ì—ë„ ì˜ ì‘ë™í•˜ë©° í›ˆë ¨ì€ ì—¬ëŸ¬ CPU ì½”ì–´ë¡œ ê°„ë‹¨í•˜ê²Œ ë³‘ë ¬í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì„ í˜• ëª¨ë¸ë³´ë‹¤ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©° í›ˆë ¨ê³¼ ì˜ˆì¸¡ì´ ëŠë¦½ë‹ˆë‹¤. ì†ë„ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ì— ì œì•½ì´ ìˆëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë¼ë©´ ì„ í˜• ëª¨ë¸ì´ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.




ëœë¤ í¬ë ˆìŠ¤íŠ¸ì—ì„œ ë³€ìˆ˜ ì¤‘ìš”ë„ëŠ” ì–´ë–»ê²Œ ë ê¹Œìš”? ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë“¤ì„ ë³‘ë ¬ì ìœ¼ë¡œ í•©í•œ ê²ƒì´ë¯€ë¡œ, ëœë¤ í¬ë ˆìŠ¤íŠ¸ì—ì„œ ë³€ìˆ˜ì¤‘ìš”ë„ëŠ” ê²°êµ­ ê° íŠ¸ë¦¬ì—ì„œì˜ ë³€ìˆ˜ì¤‘ìš”ë„ë¥¼ ëª¨ë‘ í‰ê·  ë‚¸ ê²ƒì— í•´ë‹¹í•©ë‹ˆë‹¤!

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.ensemble import RandomForestRegressor
model =  RandomForestRegressor(n_estimators=100,min_samples_split=50,oob_score = True)
# scoring ì€ default ì´ë¯€ë¡œ model ì˜ ìì²´ scoring ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤. 
# n_estimator = 500 í´ìˆ˜ë¡ ì¢‹ìœ¼ë‚˜ ë‚´ ì»´í“¨í„°ê°€ ë²„í‹°ì§ˆ ëª»í• ë“¯.
model.fit(X_train,y_train)
```




    RandomForestRegressor(min_samples_split=50, oob_score=True)



## ëª¨ë¸ í‰ê°€


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.1050983784859
    R_squared : 0.15531497954068496
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_80_1.png">
    


# Gradient Boosting resgressoion

<b> ëª¨ë¸ ì›ë¦¬ </b>

- ë¶€ìŠ¤íŒ…ì€ ì•½í•œ ë¶„ë¥˜ê¸°ë¥¼ ì„¸íŠ¸ë¡œ ë¬¶ì–´ì„œ ì •í™•ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ëª¨ë˜ ìê°ˆ, ë¨¼ì§€ê°€ ì„ì—¬ ìˆëŠ” ë¬¼ì§ˆì— ì—¬ëŸ¬ íƒ€ì…ì˜ ì²´ë¥¼ ê°€ì§€ê³  ì¡°í•©í•´ì„œ ê·¸ê²ƒì„ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ê³¼ ìœ ì‚¬í•˜ë‹¤.

- ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ í•™ìŠµê¸° Mì— ëŒ€í•˜ì—¬ Yë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ Y = M(x) + error(1) ê°€ ìˆë‹¤ê³  í•˜ì.
- error(1) ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ëŠ” ëª¨ë¸ G(x) ë¥¼ fitting í•œë‹¤. error(1) = G(x) + error(2)
- error(2) ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ëŠ” ëª¨ë¸ H(x) ë¥¼ fitting í•œë‹¤. error(2) = G(x) + error(3)
- ì¦‰ ì´ì „ ëª¨ë¸ì—ì„œ fitting ë˜ê³  ë‚¨ì€ ì—ëŸ¬ë¥¼ ê³„ì† fitting í•´ì„œ ì„¤ëª…í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì§„í–‰ì´ ëœë‹¤.
- ìœ„ ëª¨ë¸ë“¤ì„ ì²˜ìŒ model ì— ì ìš©í•˜ê²Œ ë˜ë©´
- Y = M(x) + G(x) + ............
- ê·¸ë¦¬ê³  ê°ê° ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì´ ë‹¤ë¥¸ë°, ê°™ì€ë¹„ì¤‘(ì§€ê¸ˆì€1) ì„ ë‘ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¹„ë¡€í•˜ì—¬ ì´ì œ ê° ëª¨ë¸ ì•ì— ë¹„ì¤‘(weights)ì„ ë‘ë©´
- ìµœì¢… ëª¨ë¸ : Y = m * M(x) + g * G(x) + h * H(x) +................

<b> ëª¨ë¸ ê°œìš” </b>
- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ê·¼ë³¸ ì•„ì´ë””ì–´ëŠ” ì´ëŸ° ì–•ì€ íŠ¸ë¦¬ ê°™ì€ ê°„ë‹¨í•œ ëª¨ë¸(ì•½í•œ í•™ìŠµê¸°weak learnerë¼ê³ ë„ í•©ë‹ˆë‹¤)ì„ ë§ì´ ì—°ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br>
- ì´ë¦„ì´ íšŒê·€ì§€ë§Œ ì´ ëª¨ë¸ì€ íšŒê·€ì™€ ë¶„ë¥˜ ëª¨ë‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>
- ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ëŠ” ë‹¬ë¦¬ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€ ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.<br>
- ê°ê°ì˜ íŠ¸ë¦¬ëŠ” ë°ì´í„°ì˜ ì¼ë¶€ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ì„ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ì„œ íŠ¸ë¦¬ê°€ ë§ì´ ì¶”ê°€ë ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. <br>
- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íŠ¸ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê²½ì—° ëŒ€íšŒì—ì„œ ìš°ìŠ¹ì„ ë§ì´ ì°¨ì§€í•˜ì˜€ê³  ì—…ê³„ì—ì„œë„ ë„ë¦¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ëŠ” ë§¤ê°œë³€ìˆ˜ ì„¤ì •ì— ì¡°ê¸ˆ ë” ë¯¼ê°í•˜ì§€ë§Œ ì˜ ì¡°ì •í•˜ë©´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•´ì¤ë‹ˆë‹¤.<br>

<b> ì¥ì  </b>
- ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… íŠ¸ë¦¬ëŠ” ë³´í†µ í•˜ë‚˜ì—ì„œ ë‹¤ì„¯ ì •ë„ì˜ ê¹Šì§€ ì•Šì€ íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡ë„ ë¹ ë¦…ë‹ˆë‹¤. <br>
- ë‹¤ë¥¸ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì²˜ëŸ¼ íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•˜ì§€ ì•Šì•„ë„ ë˜ê³  ì´ì§„ íŠ¹ì„±ì´ë‚˜ ì—°ì†ì ì¸ íŠ¹ì„±ì—ì„œë„ ì˜ ë™ì‘í•©ë‹ˆë‹¤.
 

<b> ë‹¨ì  </b>
- ë‹¨ì ì€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì˜ ì¡°ì •í•´ì•¼ í•œë‹¤ëŠ” ê²ƒê³¼(ë§¤ê°œë³€ìˆ˜ ì„¤ì •ì— random forest ë³´ë‹¤ ë¯¼ê°í•˜ê¸°ë•Œë¬¸) í›ˆë ¨ ì‹œê°„ì´ ê¸¸ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.
- íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì˜ íŠ¹ì„±ìƒ í¬ì†Œí•œ ê³ ì°¨ì› ë°ì´í„°ì—ëŠ” ì˜ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.<br>

<b> Hyperparameter </b>
 - n_estimators : íŠ¸ë¦¬ì˜ ê°œìˆ˜ë¥¼ ì§€ì • <br>
 - learning_rate : ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë³´ì •í•˜ëŠ” ì •ë„ë¥¼ ì¡°ì ˆ <br>
 - max_depth : ê° ëª¨ë¸ íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ <br>

<b> Hyperparameter ì¡°ì ˆ </b>
- ì¼ë°˜ì ìœ¼ë¡œ ê¸°ë³¸ê°’ì„ ì“°ëŠ” ê²ƒì´ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤ <br>
- n_estimatorsëŠ” í´ìˆ˜ë¡ ì¢‹ë‹¤. ë” ë§ì€ íŠ¸ë¦¬ë¥¼ í‰ê· í•˜ë©´ ê³¼ëŒ€ì í•©ì„ ì¤„ì—¬ ë” ì•ˆì •ì ì¸ ëª¨ë¸ì„ ë§Œë“ ë‹¤. í•˜ì§€ë§Œ íŠ¸ë¦¬ê°€ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦°ë‹¤ <br>
- ì¼ë°˜ì ì¸ ê´€ë¡€ëŠ” ê°€ìš©í•œ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ í•œë„ì—ì„œ n_estimatorsë¥¼ ë§ì¶”ê³  ë‚˜ì„œ ì ì ˆí•œ learning_rateë¥¼ ì°¾ëŠ” ê²ƒ.<br>
- ê° íŠ¸ë¦¬ì˜ ë³µì¡ë„ë¥¼ ë‚®ì¶”ëŠ” max_depthì…ë‹ˆë‹¤. í†µìƒ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ì—ì„œëŠ” max_depthë¥¼ ë§¤ìš° ì‘ê²Œ ì„¤ì •í•˜ë©° íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ 5ë³´ë‹¤ ê¹Šì–´ì§€ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤<br>

**early stoppiny**
- ì´ë–„ì— fitting í• ë•Œì— early stopping ì„ ì¨ì„œ, ë”ìš±ë” ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- early stopping ì´ë€, ì ì  ë” tree ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ (residual ì„ í•™ìŠµí•˜ëŠ”ê²ƒì´ë‹ˆê¹Œìš”!) í•™ìŠµí•˜ëŠ” ê³¼ì •ì¸ë°, ì–´ëŠìˆœê°„ ê³¼ì í•©ì˜ ìˆœê°„ì´ ì™€ì„œ val loss ê°€ ì˜¤íˆë ¤ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ°ê²½ìš°ë¥¼ ë§‰ê¸° ìœ„í•´ tree ë¥¼ ë§ˆì¹˜ epoch ì²˜ëŸ¼ ìƒê°í•´ n_eastimator ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ val loss ì˜ ì¶”ì´ë¥¼ ë³´ê³ , ì–´ëŠì •ë„ì˜ n_eastimator ë¥¼ ì§€ì •í•´ì¤„ì§€ ì•Œê²Œ ëœë‹µë‹ˆë‹¤!
- fit í• ë–„ ì“°ì„! bossting ëª¨ë¸ë“¤ì€ ëª¨ë‘ ê°€ì§€ê³  ìˆë‹¤ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)
```




    GradientBoostingRegressor(random_state=42)



## ëª¨ë¸ í‰ê°€


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.239837743558098
    R_squared : 0.1428151340023206
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_94_1.png">
    


# AdaBoosting Regression

<b> ëª¨ë¸ ì›ë¦¬ </b>
- ì²˜ìŒì—ëŠ” weight 1/Nì„ ê°€ì§€ëŠ”(ì¦‰ ë½‘í í™•ë¥ ì´ ë˜‘ê°™ì€) dataë¡œ classifier(h1)ë¥¼ í•™ìŠµ ì‹œí‚¤ê³ , testingì„ ìˆ˜í–‰í•œë‹¤

- h1ì˜ ë¶„ë¥˜ ê²°ê³¼ë¡œ, ì˜ëª» ë¶„ë¥˜ëœ dataê°€ ìˆì„ ê²ƒì´ê³ , ì´ dataì— ë†’ì€ weight(ë½‘í í™•ë¥ ì´ í¬ê²Œí•œë‹¤)ë¥¼ ì¤€ë‹¤

- h2ë¥¼ train í•  ë•ŒëŠ” h1ê°€ í‹€ë¦° ë°ì´í„°ê°€ ë§ì´ í¬í•¨ëœ ë°ì´í„°ë“¤ë¡œ í•™ìŠµ ë  ê²ƒì´ë‹¤

- ê·¸ëŸ¼ h2ëŠ” h1ê°€ í‹€ë¦° ë°ì´í„°ë¥¼ ì˜ ë¶„ë¥˜í•˜ë„ë¡ í•™ìŠµ ë  ê²ƒì´ë‹¤

- ì¦‰ ë¨¼ì € í•™ìŠµëœ ë¶„ë¥˜ê¸°ê°€ ì˜ëª» ë¶„ë¥˜í•œ ê²ƒì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹¤ìŒ ë¶„ë¥˜ê¸°ê°€ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ê³ , ë‹¨ì ì„ ë³´ì™„í•˜ëŠ” ê²ƒì´ë‹¤

- ì´ëŸ°ì‹ìœ¼ë¡œ ê³„ì† ë¶„ë¥˜ê¸°ê°€ í•™ìŠµëœë‹¤ h1 -> h2 -> ...->hN

- ê° ëª¨ë¸ì´ data ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¥˜í–ˆëŠ”ì§€ì— ëŒ€í•´ a1,a2 ... ì˜ weight ë¥¼ ì¤€ë‹¤.(ì˜ ë¶„ë¥˜í•˜ë©´ í° weight)

- ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë‚®ì€ ì•½í•œ ë¶„ë¥˜ê¸°ë“¤ì„ ì¡°í•©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°•í•œ ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•œë‹¤

- ìµœì¢… ë¶„ë¥˜ê¸° : f(x)= a1* h1(x)+a2* h2(x)+...+aN* hN(x)

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.ensemble import AdaBoostRegressor
model =AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model.fit(X_train, y_train)
```




    AdaBoostRegressor(learning_rate=0.1, n_estimators=100, random_state=42)



## ëª¨ë¸ í‰ê°€


```python
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.204469959820555
    R_squared : 0.14609622289209434
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_102_1.png">
    


 # xgboost Regression

## ëª¨ë¸ ì„¸ìš°ê¸°

xgboostë€?

1. í›Œë¥­í•œ ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¼ì´ë¸ŒëŸ¬ë¦¬. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸°ì— í•™ìŠµê³¼ ë¶„ë¥˜ê°€ ë¹ ë¥´ë‹¤

2. ìœ ì—°ì„±ì´ ì¢‹ë‹¤. í‰ê°€ í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ì»¤ìŠ¤í…€ ìµœì í™” ì˜µì…˜ì„ ì œê³µí•œë‹¤

3. ìš•ì‹¬ìŸì´(Greedy-algorithm)ë¥¼ ì‚¬ìš©í•œ ìë™ ê°€ì§€ì¹˜ê¸°ê°€ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ê³¼ì í•©(Overfitting)ì´ ì˜ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤

4. ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ì—°ê³„ í™œìš©ì„±ì´ ì¢‹ë‹¤. xgboost ë¶„ë¥˜ê¸° ê²°ë¡ ë¶€ ì•„ë˜ì— ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ë¶™ì—¬ì„œ ì•™ìƒë¸” í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ ResNet ë§ˆì§€ë§‰ ë°”ë¡œ ì´ì „ ë‹¨ì„ Feature layerë¡œ ì‘ìš©í•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤.

xgboostëŠ” ìš•ì‹¬ìŸì´(Greedy algorithm)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ê¸° M, G, Hë¥¼ ë°œê²¬í•˜ê³ , ë¶„ì‚°ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸(Extreme) ì†ë„ë¡œ ì í•©í•œ ë¹„ì¤‘ íŒŒë¼ë©”í„°ë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë¶„ë¥˜ê¸°ëŠ” Regression Scoreë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ ìŠ¤ì½”ì–´(accuracy score)ë¥¼ ì¸¡ì •í•˜ê³ , ê° ìˆœì„œì— ë”°ë¼ ê°•í•œ ë¶„ë¥˜ê¸°ë¶€í„° ì•½í•œ ë¶„ë¥˜ê¸°ê¹Œì§€ ëœë¤ í•˜ê²Œ ìƒì„±ëœë‹¤. ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ ë¶„ë¥˜ê¸°ë¥¼ íŠ¸ë¦¬(tree)ë¼ê³  í•˜ë©°, ë¶„ë¥˜ê¸°ë¥¼ ì¡°í•©í•œ ìµœì¢… ì•Œê³ ë¦¬ì¦˜ì„ í¬ë ˆìŠ¤íŠ¸(forest)ë¼ê³  í•œë‹¤. ì—¬ê¸°ê¹Œì§€ê°€ ê¸°ë³¸ì ì¸ boosting algorithm ì›ë¦¬ë‹¤.

xgboostëŠ” íŠ¸ë¦¬ë¥¼ ë§Œë“¤ ë•Œ CART(Classification And Regression Trees)ë¼ ë¶ˆë¦¬ëŠ” ì•™ìƒë¸” ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ì´í›„ íŠ¸ë¦¬ ë¶€ìŠ¤íŒ…ì„ ì‚¬ìš©í•˜ì—¬, ê° ë¶„ë¥˜ ê¸°ê°„ ë¹„ì¤‘(weights)ì„ ìµœì í™”í•œë‹¤. CART ëª¨ë¸ì€ ì¼ë°˜ì ì¸ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬(Decision Tree)ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. ë¦¬í”„ ë…¸ë“œ í•˜ë‚˜ì— ëŒ€í•´ì„œë§Œ ë””ì‹œì „ ë²¨ë¥˜ë¥¼ ê°–ëŠ”ëŠ” ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ì™€ ë‹¬ë¦¬, CART ë°©ì‹ì€ ëª¨ë“  ë¦¬í”„ë“¤ì´ ëª¨ë¸ì˜ ìµœì¢… ìŠ¤ì½”ì–´ì— ì—°ê´€ë˜ì–´ ìˆë‹¤. ë”°ë¼ì„œ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ê°€ ë¶„ë¥˜ë¥¼ ì œëŒ€ë¡œ í–ˆëŠ”ì§€ì— ëŒ€í•´ì„œë§Œ ì´ˆì ì„ ë§ì¶”ëŠ” ë°˜ë©´, CARTëŠ” ê°™ì€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ê°–ëŠ” ëª¨ë¸ë¼ë¦¬ë„ ëª¨ë¸ì˜ ìš°ìœ„ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤(ìŠ¤ì½”ì–´ë¥¼ ë¹„êµí•˜ë©´ ëœë‹¤). ì¦‰, ëª¨ë“  íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ Xì— ëŒ€í•˜ì—¬ í¬ë ˆìŠ¤íŠ¸ì— ë„£ê³ , ê²°ê´ê°’ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì ìˆ˜ì˜ ì ˆëŒ€ê°’ì„ ë”í•œë‹¤. ë§ì€ ë°ì´í„°ë¥¼ +ì™€ -ì˜ ë°©í–¥ìœ¼ë¡œ ë³´ë‚¼ìˆ˜ë¡, ì¢‹ì€ ë¶„ë¥˜ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆë‹¤.

2-1. ì¼ë°˜ íŒŒë¼ë©”í„°


booster: ì–´ë–¤ ë¶€ìŠ¤í„° êµ¬ì¡°ë¥¼ ì“¸ì§€ ê²°ì •í•œë‹¤. ì´ê²ƒì€ gbtree, gblinear, dartê°€ ìˆë‹¤.

nthread: ëª‡ ê°œì˜ ì“°ë ˆë“œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ë„ë¡ í• ì§€ ê²°ì •í•œë‹¤. ë””í´íŠ¸ëŠ” â€œê°€ëŠ¥í•œ í•œ ë§ì´â€.

num_feature: feature ì°¨ì›ì˜ ìˆ«ìë¥¼ ì •í•´ì•¼ í•˜ëŠ” ê²½ìš° ì˜µì…˜ì„ ì„¸íŒ…í•œë‹¤. ë””í´íŠ¸ëŠ” â€œê°€ëŠ¥í•œ í•œ ë§ì´.â€



2-2. ë¶€ìŠ¤íŒ… íŒŒë¼ë©”í„°


eta: learning rateë‹¤. íŠ¸ë¦¬ì— ê°€ì§€ê°€ ë§ì„ìˆ˜ë¡ ê³¼ì í•©(overfitting) í•˜ê¸° ì‰½ë‹¤. ë§¤ ë¶€ìŠ¤íŒ… ìŠ¤íƒ­ë§ˆë‹¤ weightë¥¼ ì£¼ì–´ ë¶€ìŠ¤íŒ… ê³¼ì •ì— ê³¼ì í•©ì´ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡ í•œë‹¤

gamma: ì •ë³´ íšë“(Information Gain)ì—ì„œ -rë¡œ í‘œí˜„í•œ ë°” ìˆë‹¤. ì´ê²ƒì´ ì»¤ì§€ë©´, íŠ¸ë¦¬ ê¹Šì´ê°€ ì¤„ì–´ë“¤ì–´ ë³´ìˆ˜ì ì¸ ëª¨ë¸ì´ ëœë‹¤. ë””í´íŠ¸ ê°’ì€ 0ì´ë‹¤

max_depth: í•œ íŠ¸ë¦¬ì˜ maximum depth. ìˆ«ìë¥¼ í‚¤ìš¸ìˆ˜ë¡ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì»¤ì§„ë‹¤. ê³¼ì í•© í•˜ê¸° ì‰½ë‹¤. ë””í´íŠ¸ëŠ” 6. 

lambda(L2 reg-form): L2 Regularization Formì— ë‹¬ë¦¬ëŠ” weightsì´ë‹¤. ìˆ«ìê°€ í´ìˆ˜ë¡ ë³´ìˆ˜ì ì¸ ëª¨ë¸ì´ ëœë‹¤

alpha(L1 reg-form): L1 Regularization Form weightsë‹¤. ìˆ«ìê°€ í´ìˆ˜ë¡ ë³´ìˆ˜ì ì¸ ëª¨ë¸ì´ ëœë‹¤ 



2-3. í•™ìŠµ ê³¼ì • íŒŒë¼ë©”í„°

objective: ëª©ì  í•¨ìˆ˜ë‹¤. reg:linear(linear-regression), binary:logistic(binary-logistic classification), count:poisson(count data poison regression) ë“± ë‹¤ì–‘í•˜ë‹¤

eval_metric: ëª¨ë¸ì˜ í‰ê°€ í•¨ìˆ˜ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ë‹¤. rmse(root mean square error), logloss(log-likelihood), map(mean average precision) ë“±, í•´ë‹¹ ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ê²Œ í‰ê°€ í•¨ìˆ˜ë¥¼ ì¡°ì •í•œë‹¤



2-4. ì»¤ë§¨ë“œ ë¼ì¸ íŒŒë¼ë©”í„°
num_rounds: boosting ë¼ìš´ë“œë¥¼ ê²°ì •í•œë‹¤. ëœë¤ í•˜ê²Œ ìƒì„±ë˜ëŠ” ëª¨ë¸ì´ë‹ˆë§Œí¼ ì´ ìˆ˜ê°€ ì ë‹¹íˆ í° ê²Œ ì¢‹ë‹¤. epoch ì˜µì…˜ê³¼ ë™ì¼í•˜ë‹¤

1. booster (ë¶€ìŠ¤í„° ëª¨ì–‘)
2. eval_metric (í‰ê°€ í•¨ìˆ˜) / objective (ëª©ì  í•¨ìˆ˜)
3. eta (ëŸ¬ë‹ ë ˆì´íŠ¸)
4. L1 form (L1 ë ˆê·¤ëŸ¬ë¼ì´ì œì´ì…˜ í¼ì´ L2ë³´ë‹¤ ì•„ì›ƒë¼ì´ì–´ì— ë¯¼ê°í•˜ë‹¤)
5. L2 form

alpha [default=0] <br>
-L1 regularization term on weight (analogous to Lasso regression)<br>
-Can be used in case of very high dimensionality so that the algorithm runs faster when implemented


```python
pip install xgboost
```

    Collecting xgboost
      Using cached xgboost-1.1.1-py3-none-win_amd64.whl (54.4 MB)
    Requirement already satisfied: numpy in c:\users\goran\anaconda3\envs\tensor\lib\site-packages (from xgboost) (1.18.5)
    Requirement already satisfied: scipy in c:\users\goran\anaconda3\envs\tensor\lib\site-packages (from xgboost) (1.4.1)
    Installing collected packages: xgboost
    Successfully installed xgboost-1.1.1
    Note: you may need to restart the kernel to use updated packages.
    


```python
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
model = XGBRegressor(n_estimators=100, learning_rate=0.08,max_depth=3,reg_alpha=1)
model.fit(X_train, y_train)
```




    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                 colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,
                 importance_type='gain', interaction_constraints='',
                 learning_rate=0.08, max_delta_step=0, max_depth=3,
                 min_child_weight=1, missing=nan, monotone_constraints='()',
                 n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,
                 reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=1,
                 tree_method='exact', validate_parameters=1, verbosity=None)



## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.123864862015253
    R_squared : 0.15357400246773023
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
```




    <AxesSubplot:>




    
<img src="/assets/images/1.Regression/output_115_1.png">
    


# SVR regression

Pros:<br>
It works really well with a clear margin of separation<br>
It is effective in high dimensional spaces.<br>
It is effective in cases where the number of dimensions is greater than the number of samples.<br>
It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.<br>
Cons:<br>
It doesnâ€™t perform well when we have large data set because the required training time is higher<br>
It also doesnâ€™t perform very well, when the data set has more noise i.e. target classes are overlapping<br>
SVM doesnâ€™t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is included in the related SVC method of Python scikit-learn library.

parameters :
- kernel : ì»¤ë„ì—ëŠ” Polynomial ì»¤ë„, Sigmoid ì»¤ë„, ê°€ìš°ì‹œì•ˆ RBF ì»¤ë„ ë“± ì¢…ë¥˜ê°€ ë§ì€ë°, ê·¸ ì¤‘ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•„ ìì£¼ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ê°€ìš°ì‹œì•ˆ RBF ì»¤ë„ì´ë‹¤. ì»¤ë„ ê¸°ë²•ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê³ ì°¨ì› íŠ¹ì§• ê³µê°„ìœ¼ë¡œ ì‚¬ìƒí•´ì£¼ëŠ” ê²ƒì´ë‹¤
- gamma : gammaëŠ” í•˜ë‚˜ì˜ ë°ì´í„° ìƒ˜í”Œì´ ì˜í–¥ë ¥ì„ í–‰ì‚¬í•˜ëŠ” ê±°ë¦¬ë¥¼ ê²°ì •,ê²°ì • ê²½ê³„ì˜ ê³¡ë¥ ì„ ì¡°ì •í•œë‹¤ê³  ë§í•  ìˆ˜ë„ ìˆë‹¤ ,ë„ˆë¬´ ë‚®ìœ¼ë©´ ê³¼ì†Œì í•©ë  ê°€ëŠ¥ì„±ì´ í¬ê³ , ë„ˆë¬´ ë†’ìœ¼ë©´ ê³¼ëŒ€ì í•©ì˜ ìœ„í—˜ì´ ìˆë‹¤ <br>

- C : cost(C)ì´ë‹¤. CëŠ” ì–¼ë§ˆë‚˜ ë§ì€ ë°ì´í„° ìƒ˜í”Œì´ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ë†“ì´ëŠ” ê²ƒì„ í—ˆìš©í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•œë‹¤. ì‘ì„ ìˆ˜ë¡ ë§ì´ í—ˆìš©í•˜ê³ , í´ ìˆ˜ë¡ ì ê²Œ í—ˆìš©í•œë‹¤. ë‹¤ë¥¸ ë§ë¡œ, Cê°’ì„ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ì´ìƒì¹˜ë“¤ì´ ìˆì„ ê°€ëŠ¥ì„±ì„ í¬ê²Œ ì¡ì•„ ì¼ë°˜ì ì¸ ê²°ì • ê²½ê³„ë¥¼ ì°¾ì•„ë‚´ê³ , ë†’ê²Œ ì„¤ì •í•˜ë©´ ë°˜ëŒ€ë¡œ ì´ìƒì¹˜ì˜ ì¡´ì¬ ê°€ëŠ¥ì„±ì„ ì‘ê²Œ ë´ì„œ ì¢€ ë” ì„¸ì‹¬í•˜ê²Œ ê²°ì • ê²½ê³„ë¥¼ ì°¾ì•„ë‚¸ë‹¤. "ë‚œ ë°ì´í„° ìƒ˜í”Œí•˜ë‚˜ë„ ì˜ëª» ë¶„ë¥˜í•  ìˆ˜ ì—†ì–´!"ë¼ë©´ Cë¥¼ ë†’ì—¬ì•¼í•œë‹¤. ë°˜ëŒ€ë¡œ "ëª‡ ê°œëŠ” ë†“ì³ë„ ê´œì°®ì•„, ì´ìƒì¹˜ë“¤ì´ ê½¤ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆê¹Œ"ë¼ë©´ Cë¥¼ ë‚®ì¶°ì•¼í•œë‹¤. ë†’ìœ¼ë©´ overfitting ì˜ ë¬¸ì œê°€ ìˆë‹¤

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV 
SVR = SVR()
# gridsearch ë¡œ ìµœì ì˜ parameter ë¥¼ ê³ ë¥´ì.
param_grid = {'C': [0.1, 1, 10, 100, 1000],  
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf','linear','poly']}  
model = GridSearchCV(SVR, param_grid) 

# fitting the model for grid search 
model.fit(X_train, y_train) 
```




    GridSearchCV(estimator=SVR(),
                 param_grid={'C': [0.1, 1, 10, 100, 1000],
                             'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
                             'kernel': ['rbf', 'linear', 'poly']})




```python
# ëª¨ë¸ì€ ì–´ë–¤ ê²½ìš°ì— ìµœê³  ì¢‹ì•˜ëŠ”ê°€?
model.best_params_
```




    {'C': 100, 'gamma': 1, 'kernel': 'rbf'}



## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
y_pred = svr_rbf.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 8.79871044038635
    R_squared : 0.2010870121689774
    

# Light gbm

1. learning_rate

ì¼ë°˜ì ìœ¼ë¡œ 0.01 ~ 0.1 ì •ë„ë¡œ ë§ì¶”ê³  ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•œë‹¤. ë‚˜ì¤‘ì— ì„±ëŠ¥ì„ ë” ë†’ì¼ ë•Œ learning rateë¥¼ ë” ì¤„ì¸ë‹¤.

2. num_iterations

ê¸°ë³¸ê°’ì´ 100ì¸ë° 1000ì •ë„ëŠ” í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤. ë„ˆë¬´ í¬ê²Œí•˜ë©´ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

ê°™ì€ ëœ»ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì˜µì…˜: num_iteration, n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators

3. max_depth

-1ë¡œ ì„¤ì •í•˜ë©´ ì œí•œì—†ì´ ë¶„ê¸°í•œë‹¤. featureê°€ ë§ë‹¤ë©´ í¬ê²Œ ì„¤ì •í•œë‹¤. íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œ ìš°ì„ ì ìœ¼ë¡œ ì„¤ì •í•œë‹¤.

4. boosting 

ë¶€ìŠ¤íŒ… ë°©ë²•: ê¸°ë³¸ê°’ì€ gbdtì´ë©° ì •í™•ë„ê°€ ì¤‘ìš”í• ë•ŒëŠ” ë”¥ëŸ¬ë‹ì˜ ë“œëì•„ì›ƒê³¼ ê°™ì€ dartë¥¼ ì‚¬ìš©í•œë‹¤. ìƒ˜í”Œë§ì„ ì´ìš©í•˜ëŠ” gossë„ ìˆë‹¤.

- boosting ğŸ”—ï¸, default = gbdt, options: gbdt, rf, dart, goss
- gbdt : traditional Gradient Boosting Decision Tree, aliases: gbrt
- rf : Random Forest, aliases: random_forest
- dart : Dropouts meet Multiple Additive Regression Trees
- goss : Gradient-based One-Side Sampling

5. bagging_fraction

ë°°ê¹…ì„ í•˜ê¸°ìœ„í•´ì„œ ë°ì´í„°ë¥¼ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©í•œë‹¤. ë¹„ìœ¨ì€ 0 < fraction <= 1 ì´ë©° 0ì´ ë˜ì§€ ì•Šê²Œ í•´ì•¼í•œë‹¤.

6. feature_fraction

feature_fractionì´ 1ë³´ë‹¤ ì‘ë‹¤ë©´ LGBMì€ ë§¤ iteration(tree)ë§ˆë‹¤ ë‹¤ë¥¸ featureë¥¼ ëœë¤í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµí•˜ê²Œëœë‹¤. ë§Œì•½, 0.8ë¡œ ê°’ì„ ì„¤ì •í•˜ë©´ ë§¤ treeë¥¼ êµ¬ì„±í•  ë•Œ, featureì˜ 80%ë§Œ ëœë¤í•˜ê²Œ ì„ íƒí•œë‹¤. ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° í•™ìŠµ ì†ë„ê°€ í–¥ìƒëœë‹¤.

7. scale_pos_weight

í´ë˜ìŠ¤ ë¶ˆê· í˜•ì˜ ë°ì´í„° ì…‹ì—ì„œ weightë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ positiveë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤. ê¸°ë³¸ê°’ì€ 1ì´ë©° ë¶ˆê· í˜•ì˜ ì •ë„ì— ë”°ë¼ ì¡°ì ˆí•œë‹¤.

8. early_stopping_round

Validation ì…‹ì—ì„œ í‰ê°€ì§€í‘œê°€ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì •ì§€í•œë‹¤. í‰ê°€ì§€í‘œì˜ í–¥ìƒì´ n round ì´ìƒ ì§€ì†ë˜ë©´ í•™ìŠµì„ ì •ì§€í•œë‹¤.

9. lambda_l1, lambda_l2

ì •ê·œí™”ë¥¼ í†µí•´ ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ë¥¼ ì €í•˜ì‹œí‚¬ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ default ê°’ì¸ 0ìœ¼ë¡œ ë‘”ë‹¤.

ë” ë¹ ë¥¸ ì†ë„
- bagging_fraction
- max_binì€ ì‘ê²Œ
- save_binaryë¥¼ ì“°ë©´ ë°ì´í„° ë¡œë”©ì†ë„ê°€ ë¹¨ë¼ì§
- parallel learning ì‚¬ìš©

ë” ë†’ì€ ì •í™•ë„
- max_binì„ í¬ê²Œ
- num_iterations ëŠ” í¬ê²Œí•˜ê³  learning_rateëŠ” ì‘ê²Œ
- num_leavesë¥¼ í¬ê²Œ(ê³¼ì í•©ì˜ ì›ì¸ì´ ë  ìˆ˜ ìˆìŒ)
- boosting ì•Œê³ ë¦¬ì¦˜ 'dart' ì‚¬ìš©

ê³¼ì í•©ì„ ì¤„ì´ê¸°
- max_binì„ ì‘ê²Œ
- num_leavesë¥¼ ì‘ê²Œ
- min_data_in_leafì™€ min_sum_hessian_in_leaf ì‚¬ìš©í•˜ê¸°


**PARAMETERS íŒŒë¼ë¯¸í„° ì†Œê°œ**

max_depth : Treeì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ë§í•©ë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ ê³¼ì í•©ì„ ë‹¤ë£° ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë§Œì•½ ì—¬ëŸ¬ë¶„ì˜ ëª¨ë¸ì´ ê³¼ì í•©ëœ ê²ƒ ê°™ë‹¤ê³  ëŠë¼ì‹ ë‹¤ë©´ ì œ ì¡°ì–¸ì€ max_depth ê°’ì„ ì¤„ì´ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

min_data_in_leaf : Leafê°€ ê°€ì§€ê³  ìˆëŠ” ìµœì†Œí•œì˜ ë ˆì½”ë“œ ìˆ˜ì…ë‹ˆë‹¤. ë””í´íŠ¸ê°’ì€ 20ìœ¼ë¡œ ìµœì  ê°’ì…ë‹ˆë‹¤. ê³¼ì í•©ì„ í•´ê²°í•  ë•Œ ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

feature_fraction : Boosting (ë‚˜ì¤‘ì— ë‹¤ë¤„ì§ˆ ê²ƒì…ë‹ˆë‹¤) ì´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì¼ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤. 0.8 feature_fractionì˜ ì˜ë¯¸ëŠ” Light GBMì´ Treeë¥¼ ë§Œë“¤ ë•Œ ë§¤ë²ˆ ê°ê°ì˜ iteration ë°˜ë³µì—ì„œ íŒŒë¼ë¯¸í„° ì¤‘ì—ì„œ 80%ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

bagging_fraction : ë§¤ë²ˆ iterationì„ ëŒ ë•Œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì„ íƒí•˜ëŠ”ë° íŠ¸ë ˆì´ë‹ ì†ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•  ë•Œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

early_stopping_round : ì´ íŒŒë¼ë¯¸í„°ëŠ” ë¶„ì„ ì†ë„ë¥¼ ë†’ì´ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ëª¨ë¸ì€ ë§Œì•½ ì–´ë–¤ validation ë°ì´í„° ì¤‘ í•˜ë‚˜ì˜ ì§€í‘œê°€ ì§€ë‚œ early_stopping_round ë¼ìš´ë“œì—ì„œ í–¥ìƒë˜ì§€ ì•Šì•˜ë‹¤ë©´ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì´ëŠ” ì§€ë‚˜ì¹œ iterationì„ ì¤„ì´ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

lambda : lambda ê°’ì€ regularization ì •ê·œí™”ë¥¼ í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê°’ì˜ ë²”ìœ„ëŠ” 0 ì—ì„œ 1 ì‚¬ì´ì…ë‹ˆë‹¤.

min_gain_to_split : ì´ íŒŒë¼ë¯¸í„°ëŠ” ë¶„ê¸°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ gainì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Treeì—ì„œ ìœ ìš©í•œ ë¶„ê¸°ì˜ ìˆ˜ë¥¼ ì»¨íŠ¸ë¡¤í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

max_cat_group : ì¹´í…Œê³ ë¦¬ ìˆ˜ê°€ í´ ë•Œ, ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë¶„ê¸° í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ Light GBM ì•Œê³ ë¦¬ì¦˜ì´ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì„ max_cat_group ê·¸ë£¹ìœ¼ë¡œ í•©ì¹˜ê³  ê·¸ë£¹ ê²½ê³„ì„ ì—ì„œ ë¶„ê¸° í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ë””í´íŠ¸ ê°’ì€ 64 ì…ë‹ˆë‹¤.

**Core Parameters í•µì‹¬ íŒŒë¼ë¯¸í„° ì†Œê°œ**

Task : ë°ì´í„°ì— ëŒ€í•´ì„œ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ì„ë¬´ë¥¼ êµ¬ì²´í™”í•©ë‹ˆë‹¤. train íŠ¸ë ˆì´ë‹ì¼ ìˆ˜ë„ ìˆê³  predict ì˜ˆì¸¡ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

application : ê°€ì¥ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë¡œ, ëª¨ë¸ì˜ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì •í•˜ëŠ”ë° ì´ê²ƒì´ regression íšŒê·€ë¶„ì„ ë¬¸ì œì¸ì§€ ë˜ëŠ” classification ë¶„ë¥˜ ë¬¸ì œì¸ì§€ë¥¼ ì •í•©ë‹ˆë‹¤. Light GBMì—ì„œ ë””í´íŠ¸ëŠ” regression íšŒê·€ë¶„ì„ ëª¨ë¸ì…ë‹ˆë‹¤.

regression: íšŒê·€ë¶„ì„
binary: ì´ì§„ ë¶„ë¥˜
multiclass: ë‹¤ì¤‘ ë¶„ë¥˜
boosting : ì‹¤í–‰í•˜ê³ ì í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ íƒ€ì…ì„ ì •ì˜í•©ë‹ˆë‹¤. ë””í´íŠ¸ê°’ì€ gdbt ì…ë‹ˆë‹¤.

- gdbt : Traditional Gradient Boosting Decision Tree
- rf : Random Forest
- dart : Dropouts meet Multiple Additive Regression Trees
- goss : Gradient-based One-Side Sampling

num_boost_round : boosting iteration ìˆ˜ë¡œ ì¼ë°˜ì ìœ¼ë¡œ 100 ì´ìƒì…ë‹ˆë‹¤.

learning_rate : ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ê°ê°ì˜ Treeì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. GBMì€ ì´ˆê¸°ì˜ ì¶”ì •ê°’ì—ì„œ ì‹œì‘í•˜ì—¬ ê°ê°ì˜Tree ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì •ê°’ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤. í•™ìŠµ íŒŒë¼ë¯¸í„°ëŠ” ì´ëŸ¬í•œ ì¶”ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë³€í™”ì˜ í¬ê¸°ë¥¼ ì»¨íŠ¸ë¡¤í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê°’ì€ 0.1, 0.001, 0.003 ë“±ë“±ì´ ìˆìŠµë‹ˆë‹¤.

num_leaves : ì „ì²´ Treeì˜ leave ìˆ˜ ì´ê³ , ë””í´íŠ¸ê°’ì€ 31ì…ë‹ˆë‹¤.

device : ë””í´íŠ¸ ê°’ì€ cpu ì¸ë° gpuë¡œ ë³€ê²½í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

**Metric parameter ì§€í‘œ íŒŒë¼ë¯¸í„°**

metric : ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì†ì‹¤ì„ ì •í•˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•œ ë³€ìˆ˜ ì¤‘ì— í•˜ë‚˜ì…ë‹ˆë‹¤. regressionê³¼ classification ì„ ìœ„í•œ ì¼ë°˜ì ì¸ ì†ì‹¤ ê°’ì´ ì•„ë˜ì— ë‚˜ì™€ìˆìŠµë‹ˆë‹¤.

- mae : mean absolute error
- mse : mean squared error
- binary_logloss : loss for binary classification
- multi_logloss : loss for multi classification

**IO parameter IO íŒŒë¼ë¯¸í„°**

max_bin : feature ê°’ì˜ ìµœëŒ€ bin ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

categorical_feature : ë²”ì£¼í˜• featureì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ categorical_features ê°€ 0, 1, 2 ì´ë©´ column 0, column 1, column 2 ê°€ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì…ë‹ˆë‹¤.

ignore_column : categorical_featuresì™€ ë™ì¼í•œ ê²ƒì¸ë° ë²”ì£¼í˜• featureë¡œì¨ íŠ¹ì • ì¹¼ëŸ¼ì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ ë³€ìˆ˜ë“¤ì„ ë¬´ì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

save_binary : ë°ì´í„° íŒŒì¼ì˜ ë©”ëª¨ë¦¬ ì‚¬ì´ì¦ˆë¥¼ ì²˜ë¦¬í•´ì•¼ í•œë‹¤ë©´ ì´ íŒŒë¼ë¯¸í„° ê°’ì„ Trueë¡œ ì„¤ì •í•˜ì‹­ì‹œì˜¤. ì´ ê°’ì„ Trueë¡œ ì„¸íŒ…í•˜ë©´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ì €ì¥í•  ê²ƒì´ê³ , ì´ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì€ ë‹¤ìŒì— ë°ì´í„°ë¥¼ ì½ì–´ì˜¬ ë•Œ ê·¸ ì†ë„ë¥¼ ì¤„ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤.

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
# dataset train/test set ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)
```


```python
# ë¨¼ì € train ì…‹ì„ ë‹¤ì‹œ validation ê³¼ train ì…‹ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
# ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” lgb ëŠ” DNN ì²˜ëŸ¼ early stopping ì´ ìˆì–´, ì–´ë””ê¹Œì§€ tree ë¥¼ í•™ìŠµì‹œí‚¬ì§€ ê²°ì •í•´ì•¼ í•˜ê¸° ë–„ë¬¸
# ê·¸ë˜ì„œ validation ì˜ score ë¡œ ì¤‘ë‹¨í• ì§€ ë§ì§€ë¥¼ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— í•„ìš”í•˜ë‹¤.
X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=0)
```


```python
import lightgbm as lgb
train_ds = lgb.Dataset(X_tra, label = y_tra) # lgb ì“°ë ¤ë©´ lgb dataset í˜•íƒœë¡œ ë§Œë“¤ì–´ì•¼í•œë‹¤.
val_ds = lgb.Dataset(X_val, label = y_val)
params = {'learning_rate': 0.01,
          'num_iterations' : 500, # ëª‡ë²ˆ ë°˜ë³µí• ì§€
          'max_depth': 16, 
          'boosting': 'gbdt', # ë¶€ìŠ¤íŒ…ì„ ì§„
          'objective': 'regression', # regression ë¬¸ì œì¼ë–„ëŠ” ì´ë ‡ê²Œ ì§€ì •í•´ì£¼ì–´ì•¼í•œë‹¤.
          'metric': 'mse', # metric
          'num_leaves': 144,
          'feature_fraction': 0.9,
          'bagging_fraction': 0.7,
          'bagging_freq': 5,
          'seed':2020}
# ê·¸ë¦¬ê³  íŒŒë¼ë¯¸í„°ëŠ” ì´ëŸ° dic í˜•íƒœë¡œ ì •ì˜í•œ ë‹¤ìŒ train ì‹œì— í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤.
# ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë„ˆë¬´ ê·€ì°®ë‹¤.
```


```python
model = lgb.train(params,
                  train_set = train_ds, 
                  valid_sets = val_ds,
                  verbose_eval=10, # 10 ë²ˆë§ˆë‹¤ ê²°ê³¼ ë³´ì—¬ì£¼ê¸° 
                  early_stopping_rounds=20) # 20ë²ˆì˜ iteration ë™ì•ˆ ê³„ì† ì•ˆì¢‹ì•„ì§€ë©´ ê±°ê¸°ì„œ ë©ˆì¶¤
 
y_pred=model.predict(X_test)
```

    Training until validation scores don't improve for 20 rounds
    [10]	valid_0's l2: 10.7871
    [20]	valid_0's l2: 10.4263
    [30]	valid_0's l2: 10.1375
    [40]	valid_0's l2: 9.95388
    [50]	valid_0's l2: 9.7608
    [60]	valid_0's l2: 9.60271
    [70]	valid_0's l2: 9.4487
    [80]	valid_0's l2: 9.35395
    [90]	valid_0's l2: 9.29904
    [100]	valid_0's l2: 9.22571
    [110]	valid_0's l2: 9.16716
    [120]	valid_0's l2: 9.10326
    [130]	valid_0's l2: 9.0634
    [140]	valid_0's l2: 8.99244
    [150]	valid_0's l2: 8.9855
    [160]	valid_0's l2: 8.94343
    [170]	valid_0's l2: 8.9201
    [180]	valid_0's l2: 8.92365
    [190]	valid_0's l2: 8.90043
    [200]	valid_0's l2: 8.9158
    [210]	valid_0's l2: 8.94221
    Early stopping, best iteration is:
    [191]	valid_0's l2: 8.89466
    

## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
y_pred = model.predict(X_test)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
```

    MSE : 9.002660123598709
    

## ë³€ìˆ˜ ì¤‘ìš”ë„


```python
ax = lgb.plot_importance(model, max_num_features=80, figsize=(15,15))
plt.show()
```


    
<img src="/assets/images/1.Regression/output_135_0.png">
    


# Catboost

## ëª¨ë¸ ì„¸ìš°ê¸°


```python
from catboost import CatBoostRegressor
# Initialize data
from sklearn.model_selection import train_test_split
X_train_2, X_eval, y_train_2, y_eval = train_test_split(X_train, y_train, test_size=0.20, random_state=0)
```


```python
CatBoostRegressor()
```




    <catboost.core.CatBoostRegressor at 0x1a7b8a3be88>




```python
# https://catboost.ai/docs/concepts/python-reference_parameters-list.html (parameter ì„¤ëª…)
model = CatBoostRegressor(iterations=100, # ëª‡ê°œì˜ trees ë¡œ í›ˆë ¨
                          learning_rate=0.1, # í•™ìŠµ ì†ë„
                          max_depth=10, # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
                          loss_function = 'RMSE', # https://catboost.ai/docs/references/eval-metric__supported-metrics.html
                        )
```


```python
# Fit model
model.fit(X_train_2,y_train_2, # ê°ê° ì ìš©í•  X,y set ì„ ë„£ì–´ì¤€ë‹¤.
          eval_set=(X_eval,y_eval), # í›ˆë ¨ì„ ì§€ì¼œë³´ë©´ì„œ, íŒë‹¨í•  evaluation set
          verbose_eval = 10, # ëª‡ë²ˆë§ˆë‹¤ í•œë²ˆì”© ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ì§€
          )
```

    0:	learn: 3.2855664	test: 3.3164971	best: 3.3164971 (0)	total: 226ms	remaining: 22.4s
    10:	learn: 2.6975929	test: 3.1766838	best: 3.1742496 (9)	total: 658ms	remaining: 5.32s
    20:	learn: 2.3046747	test: 3.1270483	best: 3.1270483 (20)	total: 1.11s	remaining: 4.18s
    30:	learn: 2.0082468	test: 3.1024159	best: 3.1024159 (30)	total: 1.57s	remaining: 3.49s
    40:	learn: 1.7946485	test: 3.0957864	best: 3.0957864 (40)	total: 2.01s	remaining: 2.89s
    50:	learn: 1.6009077	test: 3.0856002	best: 3.0856002 (50)	total: 2.45s	remaining: 2.35s
    60:	learn: 1.4313088	test: 3.0860660	best: 3.0828759 (58)	total: 2.9s	remaining: 1.86s
    70:	learn: 1.2964716	test: 3.0753949	best: 3.0749393 (67)	total: 3.35s	remaining: 1.37s
    80:	learn: 1.1765645	test: 3.0761430	best: 3.0746362 (72)	total: 3.77s	remaining: 884ms
    90:	learn: 1.0875473	test: 3.0766189	best: 3.0728343 (83)	total: 4.19s	remaining: 414ms
    99:	learn: 1.0049267	test: 3.0737350	best: 3.0728343 (83)	total: 4.59s	remaining: 0us
    
    bestTest = 3.072834339
    bestIteration = 83
    
    Shrink model to first 84 iterations.
    




    <catboost.core.CatBoostRegressor at 0x1a7b8d136c8>



## ëª¨ë¸ í‰ê°€


```python
from sklearn import metrics
preds = model.predict(X_eval)
print ("MSE :", metrics.mean_squared_error(y_test, y_pred))
print('R_squared :',model.score(X_test, y_test)) 
```

    MSE : 9.002660123598709
    R_squared : 0.1500162325753287
    


```python

```
