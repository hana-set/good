---
title:  "1.Time Series(DNN)"
excerpt: "뉴럴넷을 이용한 시계열 분석에 대해 알아봅시다."
categories:
  - Py_Analysis
tags:
  - 기초
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true
---

```python
import tensorflow as tf

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

```


```python
zip_path = tf.keras.utils.get_file(
    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',
    fname='jena_climate_2009_2016.csv.zip',
    extract=True)
csv_path, _ = os.path.splitext(zip_path)

```


```python
df = pd.read_csv(csv_path)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date Time</th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>01.01.2009 00:10:00</td>
      <td>996.52</td>
      <td>-8.02</td>
      <td>265.40</td>
      <td>-8.90</td>
      <td>93.3</td>
      <td>3.33</td>
      <td>3.11</td>
      <td>0.22</td>
      <td>1.94</td>
      <td>3.12</td>
      <td>1307.75</td>
      <td>1.03</td>
      <td>1.75</td>
      <td>152.3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>01.01.2009 00:20:00</td>
      <td>996.57</td>
      <td>-8.41</td>
      <td>265.01</td>
      <td>-9.28</td>
      <td>93.4</td>
      <td>3.23</td>
      <td>3.02</td>
      <td>0.21</td>
      <td>1.89</td>
      <td>3.03</td>
      <td>1309.80</td>
      <td>0.72</td>
      <td>1.50</td>
      <td>136.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>01.01.2009 00:30:00</td>
      <td>996.53</td>
      <td>-8.51</td>
      <td>264.91</td>
      <td>-9.31</td>
      <td>93.9</td>
      <td>3.21</td>
      <td>3.01</td>
      <td>0.20</td>
      <td>1.88</td>
      <td>3.02</td>
      <td>1310.24</td>
      <td>0.19</td>
      <td>0.63</td>
      <td>171.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01.01.2009 00:40:00</td>
      <td>996.51</td>
      <td>-8.31</td>
      <td>265.12</td>
      <td>-9.07</td>
      <td>94.2</td>
      <td>3.26</td>
      <td>3.07</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.08</td>
      <td>1309.19</td>
      <td>0.34</td>
      <td>0.50</td>
      <td>198.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>01.01.2009 00:50:00</td>
      <td>996.51</td>
      <td>-8.27</td>
      <td>265.15</td>
      <td>-9.04</td>
      <td>94.1</td>
      <td>3.27</td>
      <td>3.08</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.09</td>
      <td>1309.00</td>
      <td>0.32</td>
      <td>0.63</td>
      <td>214.3</td>
    </tr>
  </tbody>
</table>
</div>




```python
# history_size : 과거 information window의 크기 입니다 (몇 개의 과거 데이터를 학습할것인지)
# target_size : 예측해야하는 레이블 입니다. 얼마나 멀리있는 예측을 배워야 하는가이다.
# start_index : 어떤 index 부터 시작할지.
# end_index : 어떤 index 까지 사용할지
def univariate_data(dataset, start_index, end_index, history_size, target_size):
    data = []
    labels = []

    start_index = start_index + history_size
    
    if end_index is None: # end index 를 지정을 안해주면
        end_index = len(dataset) - target_size # dataset 의 크기 - target size . 즉 최대한도로 모두 window 로 만들겠다는 의미입니다.

    for i in range(start_index, end_index): # start_index 에서 끝 인덱스까지
        indices = range(i-history_size, i) 
        # Reshape data from (history_size,) to (history_size, 1)
        data.append(np.reshape(dataset[indices], (history_size, 1))) # reshape 을 해주어야한다. 맨 마지막은 batch size 가 될것!
        labels.append(dataset[i+target_size]) # labels 는 y target 이 되는 값으로서, target size 만큼 떨어져있어야 하므로
        
    return np.array(data), np.array(labels)
```

- 데이터의 처음 300,000개 행은 train dataset이고 나머지는 validation dataset입니다
- 2100일 분량의 train data에 해당합니다



```python
TRAIN_SPLIT = 300000
# 재현성을 보장하기 위해 시드 설정.
tf.random.set_seed(13)
```

# 1 부 : Univariate 시계열 예측


단일 특성(온도)만 사용하여 모델을 학습하고 향후 해당 값을 예측하는 데 사용합니다

데이터 세트에서 온도만 추출합니다



```python
uni_data = df['T (degC)']
uni_data.index = df['Date Time']
uni_data.head()
```




    Date Time
    01.01.2009 00:10:00   -8.02
    01.01.2009 00:20:00   -8.41
    01.01.2009 00:30:00   -8.51
    01.01.2009 00:40:00   -8.31
    01.01.2009 00:50:00   -8.27
    Name: T (degC), dtype: float64




```python
uni_data.plot(subplots=True) ;
```

    C:\Users\goran\Anaconda3\envs\tensor\lib\site-packages\pandas\plotting\_matplotlib\core.py:1235: UserWarning: FixedFormatter should only be used together with FixedLocator
      ax.set_xticklabels(xticklabels)
    


    
<img src="/assets/images/1.Time Series DNN/output_9_1.png">
    



```python
uni_data = uni_data.values
```

- 신경망을 훈련하기 전 기능을 확장하는 것이 중요하다
-  표준화는 평균을 빼고 각 피처의 표준 편차로 나눔으로써 스케일링을 수행하는 일반적인 방법이다
- tf.keras.utils.normalize값은 [0,1]범위로 재조정 하는 방법을 사용할 수도 있다
- Note: 평균 및 표준 편차는 훈련 데이터만을 사용하여 계산해야합니다


```python
uni_train_mean = uni_data[:TRAIN_SPLIT].mean()
uni_train_std = uni_data[:TRAIN_SPLIT].std()
# 데이터를 표준화합시다.
uni_data = (uni_data-uni_train_mean)/uni_train_std

```


```python
univariate_past_history = 20
univariate_future_target = 0

x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,
                                           univariate_past_history,
                                           univariate_future_target)
x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,
                                       univariate_past_history,
                                       univariate_future_target)
```


```python
print ('Single window of past history')
print (x_train_uni[0])
print ('\n Target temperature to predict')
print (y_train_uni[0])
```

    Single window of past history
    [[-1.99766294]
     [-2.04281897]
     [-2.05439744]
     [-2.0312405 ]
     [-2.02660912]
     [-2.00113649]
     [-1.95134907]
     [-1.95134907]
     [-1.98492663]
     [-2.04513467]
     [-2.08334362]
     [-2.09723778]
     [-2.09376424]
     [-2.09144854]
     [-2.07176515]
     [-2.07176515]
     [-2.07639653]
     [-2.08913285]
     [-2.09260639]
     [-2.10418486]]
    
     Target temperature to predict
    -2.1041848598100876
    


```python
def create_time_steps(length):
    return list(range(-length, 0))

```


```python
def show_plot(plot_data, delta, title):
  labels = ['History', 'True Future', 'Model Prediction']
  marker = ['.-', 'rx', 'go']
  time_steps = create_time_steps(plot_data[0].shape[0])
  if delta:
    future = delta
  else:
    future = 0

  plt.title(title)
  for i, x in enumerate(plot_data):
    if i:
      plt.plot(future, plot_data[i], marker[i], markersize=10,
               label=labels[i])
    else:
      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
  plt.legend()
  plt.xlim([time_steps[0], (future+5)*2])
  plt.xlabel('Time-Step')
  return plt

```


```python
show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')

```




    <module 'matplotlib.pyplot' from 'C:\\Users\\goran\\Anaconda3\\envs\\tensor\\lib\\site-packages\\matplotlib\\pyplot.py'>




    
<img src="/assets/images/1.Time Series DNN/output_17_1.png">
    



```python
def baseline(history):
  return np.mean(history)

```


```python
show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0,
           'Baseline Prediction Example')

```




    <module 'matplotlib.pyplot' from 'C:\\Users\\goran\\Anaconda3\\envs\\tensor\\lib\\site-packages\\matplotlib\\pyplot.py'>




    
<img src="/assets/images/1.Time Series DNN/output_19_1.png">
    



```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset = dataset.repeat(2)
list(dataset.as_numpy_iterator())
```




    [1, 2, 3, 1, 2, 3]




```python
BATCH_SIZE = 256
BUFFER_SIZE = 10000

#tf.data.Dataset의 from_tensor_slices()는 주어진 텐서들 ((x_train_uni, y_train_uni))을 첫번째 차원(여기서는 sample 이겟죠) 따라 슬라이스합니다.
#모든 입력 텐서는 첫번째 차원과 같은 크기를 가져야합니다.
train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))
# tensorflow 에서는 data 를 tf.data.Dataset 형태로 받으면 많은것을 할 수 있다. ( 변형이라던지/...)
# from_tensor_slices 클래스 메서드를 사용하면 리스트, 넘파이, 텐서플로 자료형에서 데이터셋을 만들 수 있다!


train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
# cache()는 데이터셋을 캐시, 즉 메모리 또는 파일에 보관합니다. 따라서 두번째 이터레이션부터는 캐시된 데이터를 사용합니다.
# 해당 데이터셋이 반복되는 상황에서 캐싱한다는 뜻입니다. 캐싱이라는 것은, 데이터를 RAM과 같이 빨리 사용할수 있는 구간에 저장해두고, 다음에 사용할 때 빠르게 사용하기 위한 방식입니다.
# 이미지 데이터를 로딩한 이후, 한번 사용했다고 메모리에서 지우고 다시 불러오기를 반복한다면 성능저하가 심하죠.

# shuffle()는 데이터셋을 임의로 섞어줍니다. BUFFER_SIZE개로 이루어진 버퍼로부터 임의로 샘플을 뽑고, 뽑은 샘플은 다른 샘플로 대체합니다. 
# 완벽한 셔플을 위해서 전체 데이터셋의 크기에 비해 크거나 같은 버퍼 크기가 요구됩니다.

# batch()는 데이터셋의 항목들을 하나의 배치로 묶어줍니다.
# repeat라는 함수는 데이터셋을 읽다가 마지막에 도달했을 경우, 다시 처음부터 조회하는 함수입니다.
```


```python
val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni)) # 주어진 tesnsor 를 data 로  

val_univariate = val_univariate.batch(BATCH_SIZE).repeat() 
# 이떄에는 Batchsize 만큼 묶어줍니다.
# validation 이라서 학습을 하지 않기떄문에, 순서는 굳이 섞을필요가 없습니다.
```


```python
simple_lstm_model = tf.keras.models.Sequential([
      # x_train_uni.shape = (299980, 20, 1)
    # input shape 는 sample 을 제외한 형태로 들어가야해요!
    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[1:]),
    tf.keras.layers.Dense(1)
])

simple_lstm_model.compile(optimizer='adam', loss='mae')

```


```python
EVALUATION_INTERVAL = 200 # 데이터셋이 커서 200개만 하기로했어용
EPOCHS = 10

simple_lstm_model.fit(train_univariate, epochs=EPOCHS,
                      steps_per_epoch=EVALUATION_INTERVAL, 
                      validation_data=val_univariate, validation_steps=50)
```

    Train for 200 steps, validate for 50 steps
    Epoch 1/10
    200/200 [==============================] - 3s 17ms/step - loss: 0.4075 - val_loss: 0.1351
    Epoch 2/10
    200/200 [==============================] - 2s 8ms/step - loss: 0.1118 - val_loss: 0.0359
    Epoch 3/10
    200/200 [==============================] - 2s 8ms/step - loss: 0.0489 - val_loss: 0.0290
    Epoch 4/10
    200/200 [==============================] - 2s 8ms/step - loss: 0.0443 - val_loss: 0.0258
    Epoch 5/10
    200/200 [==============================] - 2s 8ms/step - loss: 0.0299 - val_loss: 0.0235
    Epoch 6/10
    200/200 [==============================] - 2s 8ms/step - loss: 0.0317 - val_loss: 0.0224
    Epoch 7/10
    200/200 [==============================] - 2s 10ms/step - loss: 0.0286 - val_loss: 0.0206
    Epoch 8/10
    200/200 [==============================] - 2s 11ms/step - loss: 0.0263 - val_loss: 0.0197
    Epoch 9/10
    200/200 [==============================] - 2s 11ms/step - loss: 0.0253 - val_loss: 0.0182
    Epoch 10/10
    200/200 [==============================] - 2s 10ms/step - loss: 0.0227 - val_loss: 0.0174
    




    <tensorflow.python.keras.callbacks.History at 0x20100335248>




```python
for x, y in val_univariate.take(3):
  plot = show_plot([x[0].numpy(), y[0].numpy(),
                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')
  plot.show()
```


    
<img src="/assets/images/1.Time Series DNN/output_25_0.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_25_1.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_25_2.png">
    


# 2부 Multivariate 시계열 예측

- 원본 데이터 세트에는 14개의 특성이 있습니다. 간단하게하기 위해 14개 중 3개만 고려합니다.
- 사용되는 특성은 대기 온도, 대기압, 공기 밀도입니다
- 더 많은 특성을 사용하려면 해당 특성 이름을 목록에 추가하세요.



```python
features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']
# 3개만 고려하겠다.
features = df[features_considered]
features.index = df['Date Time']
features.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>rho (g/m**3)</th>
    </tr>
    <tr>
      <th>Date Time</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>01.01.2009 00:10:00</th>
      <td>996.52</td>
      <td>-8.02</td>
      <td>1307.75</td>
    </tr>
    <tr>
      <th>01.01.2009 00:20:00</th>
      <td>996.57</td>
      <td>-8.41</td>
      <td>1309.80</td>
    </tr>
    <tr>
      <th>01.01.2009 00:30:00</th>
      <td>996.53</td>
      <td>-8.51</td>
      <td>1310.24</td>
    </tr>
    <tr>
      <th>01.01.2009 00:40:00</th>
      <td>996.51</td>
      <td>-8.31</td>
      <td>1309.19</td>
    </tr>
    <tr>
      <th>01.01.2009 00:50:00</th>
      <td>996.51</td>
      <td>-8.27</td>
      <td>1309.00</td>
    </tr>
  </tbody>
</table>
</div>




```python
features.plot(subplots=True)
plt.show()
```

    C:\Users\goran\Anaconda3\envs\tensor\lib\site-packages\pandas\plotting\_matplotlib\core.py:1235: UserWarning: FixedFormatter should only be used together with FixedLocator
      ax.set_xticklabels(xticklabels)
    


    
<img src="/assets/images/1.Time Series DNN/output_29_1.png">
    



```python
# 표준화를 해 주어야한다.
dataset = features.values
data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)
data_std = dataset[:TRAIN_SPLIT].std(axis=0)
dataset = (dataset-data_mean)/data_std
print(dataset)
```

    [[ 0.95547359 -1.99766294  2.2350791 ]
     [ 0.96154485 -2.04281897  2.28524007]
     [ 0.95668784 -2.05439744  2.29600633]
     ...
     [ 1.35617678 -1.43494935  1.76136375]
     [ 1.35496252 -1.55883897  1.88786728]
     [ 1.35617678 -1.62715193  1.95686921]]
    

## Single step 예측


```python
def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):
    data = []
    labels = []

    start_index = start_index + history_size
    if end_index is None:
        end_index = len(dataset) - target_size

    for i in range(start_index, end_index):
        indices = range(i - history_size, i, step) # step 이 추가되었다. 즉 몇번씩 건너뛰면서 예측해야되는 경우가 해단된다.
        data.append(dataset[indices])

        if single_step:
            labels.append(target[i + target_size]) # labels 는 y target 이 되는값.
        else:
            labels.append(target[i:i + target_size]) # i + target_size 만큼 labelss 를 append 하겠다는 의미
    return np.array(data), np.array(labels)
# 이 경우는 single step 이 들어간 경우이다.
# t
```

- 네트워크에 지난 5일 동안의 데이터, 즉 매 시간마다 샘플링되는 720개의 관측치가 표시됩니다
- 60분 내에 급격한 변화가 예상되지 않으므로 샘플링은 1시간마다 수행됩니다
- 120개의 관측치는 지난 5일의 이력을 나타냅니다
- Single step model의 경우 데이터 포인트의 레이블은 12 시간 뒤의 온도입니다. (12시간 뒤의 온도 예측)
- 이를위한 레이블을 만들기 위해 72(12*6)관찰 후 온도가 사용됩니다



```python
past_history = 720 # 720개의 관측치를 토대로 예측할거야!
future_target = 72 # 예측하고싶은건 12시간 뒤야! (그러면 72 뒤의 데이터를 예측해야함.)
STEP = 6 
# 하나의 data 간격이 10분이다. 60분 동안은 큰 변화가 없어보이므로, 샘플링은 6개 간격으로 하자.!
# 이렇게 하면, STEP 을 1로 두고 하면, 같은 시간대비 데이터가 길어지고, SAMPLE 수가 작아져서 효율적인 학습이 안된다.

x_train_single, y_train_single = multivariate_data(dataset, # Whole train data set 
                                                   dataset[:, 1], # tearget 이 되는 데이터셋
                                                   0, # 언제부터 시작? (거의다 0이죠뭐.. 0번쨰부터 해야되니까)
                                                   TRAIN_SPLIT, # 끝나는 지점인데 30000개까지가 train set 이라고 했으니까 30000!
                                                   past_history,# 몇개를 토대로 예측을 혀형성할지
                                                   future_target, # 몇스텝 뒤의 예측값을 형성하고픈지
                                                   STEP, # step 을 건너뛰면서 예측데이터를 형성할건지.
                                                   single_step=True # 이는 single step, 즉 y value 를 하나만 예측하겠다는 소리이다.
                                                  )
x_val_single, y_val_single = multivariate_data(dataset, 
                                               dataset[:, 1],
                                               TRAIN_SPLIT, # 이 경우는 val 데이터이기떄문에 시작점이 위에서 end 지점이다.
                                               None, # NOne 이면 끝까지 다 쓰겟다는거!
                                               past_history, 
                                               future_target,
                                               STEP,
                                               single_step=True)

```


```python
print ('Single window of past history : {}'.format(x_train_single[0].shape))
```

    Single window of past history : (120, 3)
    


```python
# tf.dataset 형태로 바꾸어주어야 합니다.
train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))
train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))
val_data_single = val_data_single.batch(BATCH_SIZE).repeat()

```


```python

single_step_model = tf.keras.models.Sequential() # sequential 을 쌓는다.
single_step_model.add(tf.keras.layers.LSTM(32, # 여기서의 output 은 hidden note 에서 출력되는 값의 dim 을 말한다. 
                                           input_shape=x_train_single.shape[-2:]))
# output 이 계속 이어지면서 나중에는 출력시에도 이용된다. 
single_step_model.add(tf.keras.layers.Dense(1))
# 위 32개의 hidden node 의 출력을 받아 NN 층에서 합쳐진다.
single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')

```


```python
single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,
                                            steps_per_epoch=EVALUATION_INTERVAL,
                                            validation_data=val_data_single,
                                            validation_steps=50) # VALIDATION 의 경우에도 너무 오래걸릴거같아서 STEP 제한했다...
# BATCH 는 이미 위에서 모두 정해서 여기에서는 BATCH 가 없는 모습이다.

```

    Epoch 1/10
    200/200 [==============================] - 17s 83ms/step - loss: 0.3360 - val_loss: 0.2671
    Epoch 2/10
    200/200 [==============================] - 20s 102ms/step - loss: 0.2653 - val_loss: 0.2420
    Epoch 3/10
    200/200 [==============================] - 25s 123ms/step - loss: 0.2596 - val_loss: 0.2378
    Epoch 4/10
    200/200 [==============================] - 30s 149ms/step - loss: 0.2605 - val_loss: 0.2393
    Epoch 5/10
    200/200 [==============================] - 35s 176ms/step - loss: 0.2280 - val_loss: 0.2425
    Epoch 6/10
    200/200 [==============================] - 39s 193ms/step - loss: 0.2404 - val_loss: 0.2526
    Epoch 7/10
    200/200 [==============================] - 39s 197ms/step - loss: 0.2411 - val_loss: 0.2492
    Epoch 8/10
    200/200 [==============================] - 40s 199ms/step - loss: 0.2426 - val_loss: 0.2428
    Epoch 9/10
    200/200 [==============================] - 39s 193ms/step - loss: 0.2462 - val_loss: 0.2424
    Epoch 10/10
    200/200 [==============================] - 36s 179ms/step - loss: 0.2424 - val_loss: 0.2342
    


```python
def plot_train_history(history, title):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(len(loss))

  plt.figure()

  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title(title)
  plt.legend()

  plt.show()

```


```python
plot_train_history(single_step_history,
                   'Single Step Training and validation loss')

```


    
<img src="/assets/images/1.Time Series DNN/output_40_0.png">
    



```python
for x, y in val_data_single.take(3):
  plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),
                    single_step_model.predict(x)[0]], 12,
                   'Single Step Prediction')
  plot.show()

```


    
<img src="/assets/images/1.Time Series DNN/output_41_0.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_41_1.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_41_2.png">
    


## Multi step model


모델은 과거 히스토리가 주어지면 미래의 값 범위를 예측하는 법을 배워야합니다

하나의 미래 포인트만 예측하는 Single step model과 달리 Multi step model은 미래의 시퀀스를 예측합니다

Multi step model의 훈련 데이터는 다시 한 시간마다 샘플링 된 지난 5일 동안의 기록으로 구성됩니다

12시간 동안의 온도를 예측하는 방법을 학습해야합니다

10분마다 관측이 수행되므로 결과는 72(1x6x12)개의 예측입니다

데이터 세트를 적절히 다시 준비해야합니다.




```python
future_target = 72
x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,
                                                 TRAIN_SPLIT, past_history,
                                                 future_target, STEP)
x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1],
                                             TRAIN_SPLIT, None, past_history,
                                             future_target, STEP)

```


```python
print ('Single window of past history : {}'.format(x_train_multi[0].shape))
print ('\n Target temperature to predict : {}'.format(y_train_multi[0].shape))
```

    Single window of past history : (120, 3)
    
     Target temperature to predict : (72,)
    


```python
train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))
train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))
val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()

```


```python
def multi_step_plot(history, true_future, prediction):
  plt.figure(figsize=(12, 6))
  num_in = create_time_steps(len(history))
  num_out = len(true_future)

  plt.plot(num_in, np.array(history[:, 1]), label='History')
  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',
           label='True Future')
  if prediction.any():
    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',
             label='Predicted Future')
  plt.legend(loc='upper left')
  plt.show()

```


```python
for x, y in train_data_multi.take(1):
  multi_step_plot(x[0], y[0], np.array([0]))

```


    
<img src="/assets/images/1.Time Series DNN/output_48_0.png">
    


이전 작업보다 조금 더 복잡하기 때문에 모델을 두 개의 LSTM 계층으로 구성합니다
마지막으로 72개의 예측이 이루어지므로, Dense layer는 72개의 예측을 출력합니다



```python
multi_step_model = tf.keras.models.Sequential()
multi_step_model.add(tf.keras.layers.LSTM(32, # 제일 아래층
                                          return_sequences=True, 
                                          input_shape=x_train_multi.shape[-2:]))
# LSTM 에서는 return sequence 를 true로 해 주어야 서로 쌓일 수 있다.
# 왜냐하면 LSTM레이어의 Params를 계산하는 방법이 일반 Dense레이어와 다르다고 하였는데, 이전의 기록들이 다음 레이어로 들어가기 때문에 LSTM레이어끼리 엮기 위해서는 return_sequence를 사용하여야 합니다
multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))
# 이떄에는 return_sequences=True 가 없습니다. 왜냐하면 어짜피 우리가 dense 에서 쓸것은 hidden 의 결과이기떄문에, 이것만 16dim 으로 받게되는것이다.
# 16개의 OUTPUT 이 뒤에서 DENSE 로 합쳐준다.
multi_step_model.add(tf.keras.layers.Dense(72)) 
# 우리는 72개의 값을 출력하므로, 마지막 DENSE 층에서 출력은 72 이다.

multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')
```


```python
for x, y in val_data_multi.take(1):
  print (multi_step_model.predict(x).shape)

```

    (256, 72)
    


```python
multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,
                                          steps_per_epoch=EVALUATION_INTERVAL,
                                          validation_data=val_data_multi,
                                          validation_steps=50)
```

    Epoch 1/10
    200/200 [==============================] - 65s 325ms/step - loss: 0.5146 - val_loss: 0.2902
    Epoch 2/10
    200/200 [==============================] - 69s 343ms/step - loss: 0.3281 - val_loss: 0.2618
    Epoch 3/10
    200/200 [==============================] - 79s 394ms/step - loss: 0.2898 - val_loss: 0.2375
    Epoch 4/10
    200/200 [==============================] - 86s 430ms/step - loss: 0.2348 - val_loss: 0.2003
    Epoch 5/10
    200/200 [==============================] - 98s 492ms/step - loss: 0.1986 - val_loss: 0.2051
    Epoch 6/10
    200/200 [==============================] - 112s 560ms/step - loss: 0.2085 - val_loss: 0.2141
    Epoch 7/10
    200/200 [==============================] - 105s 526ms/step - loss: 0.2004 - val_loss: 0.2071
    Epoch 8/10
    200/200 [==============================] - 107s 536ms/step - loss: 0.1962 - val_loss: 0.1921
    Epoch 9/10
    200/200 [==============================] - 113s 564ms/step - loss: 0.2006 - val_loss: 0.1930
    Epoch 10/10
    200/200 [==============================] - 113s 563ms/step - loss: 0.1904 - val_loss: 0.1876
    


```python
plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')
# 훈련은 어느정도 잘 된거같네요
```


    
<img src="/assets/images/1.Time Series DNN/output_53_0.png">
    



```python
# 모델 예측도.. 뭐 나름? 한거같아요
for x, y in val_data_multi.take(3):
  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])
```


    
<img src="/assets/images/1.Time Series DNN/output_54_0.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_54_1.png">
    



    
<img src="/assets/images/1.Time Series DNN/output_54_2.png">
    


# Ref

- 한국 Tensorflow 공식 사이트 example 에서 데이터 및 분석 틀 발제


```python

```
