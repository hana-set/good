---
title:  "1.Time Series(Stat)"
excerpt: "전통적 모델을 이용한 시계열 분석에 대해 알아봅시다."
categories:
  - Py_Analysis
tags:
  - 2
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

```python
# https://www.statsmodels.org/stable/examples/notebooks/generated/tsa_arma_0.html 이 페이지를 적극 참조하도록 하자.
```


```python
import seaborn as sns
import matplotlib.pyplot as plt
import warnings 
warnings.filterwarnings('ignore')
from IPython.display import Image
```


```python
import pandas as pd
import numpy as np
```

- 다음의 데이터를 이용해서 시계열 예측을 진행하자.
- 이 데이터는 Tensorflow 홈페이지에서 제공하는 데이터셋


```python
df = pd.read_csv('./Data/timedata.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date Time</th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>01.01.2009 00:10:00</td>
      <td>996.52</td>
      <td>-8.02</td>
      <td>265.40</td>
      <td>-8.90</td>
      <td>93.3</td>
      <td>3.33</td>
      <td>3.11</td>
      <td>0.22</td>
      <td>1.94</td>
      <td>3.12</td>
      <td>1307.75</td>
      <td>1.03</td>
      <td>1.75</td>
      <td>152.3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>01.01.2009 00:20:00</td>
      <td>996.57</td>
      <td>-8.41</td>
      <td>265.01</td>
      <td>-9.28</td>
      <td>93.4</td>
      <td>3.23</td>
      <td>3.02</td>
      <td>0.21</td>
      <td>1.89</td>
      <td>3.03</td>
      <td>1309.80</td>
      <td>0.72</td>
      <td>1.50</td>
      <td>136.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>01.01.2009 00:30:00</td>
      <td>996.53</td>
      <td>-8.51</td>
      <td>264.91</td>
      <td>-9.31</td>
      <td>93.9</td>
      <td>3.21</td>
      <td>3.01</td>
      <td>0.20</td>
      <td>1.88</td>
      <td>3.02</td>
      <td>1310.24</td>
      <td>0.19</td>
      <td>0.63</td>
      <td>171.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01.01.2009 00:40:00</td>
      <td>996.51</td>
      <td>-8.31</td>
      <td>265.12</td>
      <td>-9.07</td>
      <td>94.2</td>
      <td>3.26</td>
      <td>3.07</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.08</td>
      <td>1309.19</td>
      <td>0.34</td>
      <td>0.50</td>
      <td>198.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>01.01.2009 00:50:00</td>
      <td>996.51</td>
      <td>-8.27</td>
      <td>265.15</td>
      <td>-9.04</td>
      <td>94.1</td>
      <td>3.27</td>
      <td>3.08</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.09</td>
      <td>1309.00</td>
      <td>0.32</td>
      <td>0.63</td>
      <td>214.3</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 데이터 형식을 맞추어 주어야 한다.
date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')
```


```python
date_time
```




    0        2009-01-01 00:10:00
    1        2009-01-01 00:20:00
    2        2009-01-01 00:30:00
    3        2009-01-01 00:40:00
    4        2009-01-01 00:50:00
                     ...        
    420546   2016-12-31 23:20:00
    420547   2016-12-31 23:30:00
    420548   2016-12-31 23:40:00
    420549   2016-12-31 23:50:00
    420550   2017-01-01 00:00:00
    Name: Date Time, Length: 420551, dtype: datetime64[ns]




```python
df = df.set_index(date_time)
```


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
    <tr>
      <th>Date Time</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2009-01-01 00:10:00</th>
      <td>996.52</td>
      <td>-8.02</td>
      <td>265.40</td>
      <td>-8.90</td>
      <td>93.30</td>
      <td>3.33</td>
      <td>3.11</td>
      <td>0.22</td>
      <td>1.94</td>
      <td>3.12</td>
      <td>1307.75</td>
      <td>1.03</td>
      <td>1.75</td>
      <td>152.3</td>
    </tr>
    <tr>
      <th>2009-01-01 00:20:00</th>
      <td>996.57</td>
      <td>-8.41</td>
      <td>265.01</td>
      <td>-9.28</td>
      <td>93.40</td>
      <td>3.23</td>
      <td>3.02</td>
      <td>0.21</td>
      <td>1.89</td>
      <td>3.03</td>
      <td>1309.80</td>
      <td>0.72</td>
      <td>1.50</td>
      <td>136.1</td>
    </tr>
    <tr>
      <th>2009-01-01 00:30:00</th>
      <td>996.53</td>
      <td>-8.51</td>
      <td>264.91</td>
      <td>-9.31</td>
      <td>93.90</td>
      <td>3.21</td>
      <td>3.01</td>
      <td>0.20</td>
      <td>1.88</td>
      <td>3.02</td>
      <td>1310.24</td>
      <td>0.19</td>
      <td>0.63</td>
      <td>171.6</td>
    </tr>
    <tr>
      <th>2009-01-01 00:40:00</th>
      <td>996.51</td>
      <td>-8.31</td>
      <td>265.12</td>
      <td>-9.07</td>
      <td>94.20</td>
      <td>3.26</td>
      <td>3.07</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.08</td>
      <td>1309.19</td>
      <td>0.34</td>
      <td>0.50</td>
      <td>198.0</td>
    </tr>
    <tr>
      <th>2009-01-01 00:50:00</th>
      <td>996.51</td>
      <td>-8.27</td>
      <td>265.15</td>
      <td>-9.04</td>
      <td>94.10</td>
      <td>3.27</td>
      <td>3.08</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.09</td>
      <td>1309.00</td>
      <td>0.32</td>
      <td>0.63</td>
      <td>214.3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2016-12-31 23:20:00</th>
      <td>1000.07</td>
      <td>-4.05</td>
      <td>269.10</td>
      <td>-8.13</td>
      <td>73.10</td>
      <td>4.52</td>
      <td>3.30</td>
      <td>1.22</td>
      <td>2.06</td>
      <td>3.30</td>
      <td>1292.98</td>
      <td>0.67</td>
      <td>1.52</td>
      <td>240.0</td>
    </tr>
    <tr>
      <th>2016-12-31 23:30:00</th>
      <td>999.93</td>
      <td>-3.35</td>
      <td>269.81</td>
      <td>-8.06</td>
      <td>69.71</td>
      <td>4.77</td>
      <td>3.32</td>
      <td>1.44</td>
      <td>2.07</td>
      <td>3.32</td>
      <td>1289.44</td>
      <td>1.14</td>
      <td>1.92</td>
      <td>234.3</td>
    </tr>
    <tr>
      <th>2016-12-31 23:40:00</th>
      <td>999.82</td>
      <td>-3.16</td>
      <td>270.01</td>
      <td>-8.21</td>
      <td>67.91</td>
      <td>4.84</td>
      <td>3.28</td>
      <td>1.55</td>
      <td>2.05</td>
      <td>3.28</td>
      <td>1288.39</td>
      <td>1.08</td>
      <td>2.00</td>
      <td>215.2</td>
    </tr>
    <tr>
      <th>2016-12-31 23:50:00</th>
      <td>999.81</td>
      <td>-4.23</td>
      <td>268.94</td>
      <td>-8.53</td>
      <td>71.80</td>
      <td>4.46</td>
      <td>3.20</td>
      <td>1.26</td>
      <td>1.99</td>
      <td>3.20</td>
      <td>1293.56</td>
      <td>1.49</td>
      <td>2.16</td>
      <td>225.8</td>
    </tr>
    <tr>
      <th>2017-01-01 00:00:00</th>
      <td>999.82</td>
      <td>-4.82</td>
      <td>268.36</td>
      <td>-8.42</td>
      <td>75.70</td>
      <td>4.27</td>
      <td>3.23</td>
      <td>1.04</td>
      <td>2.01</td>
      <td>3.23</td>
      <td>1296.38</td>
      <td>1.23</td>
      <td>1.96</td>
      <td>184.9</td>
    </tr>
  </tbody>
</table>
<p>420551 rows × 14 columns</p>
</div>




```python
train_df = df[:40000] 
test_df = df[40000:41000]
```

# Autoregression (AR)

- https://www.statsmodels.org/stable/generated/statsmodels.tsa.ar_model.AutoReg.html

- Autoregression 에서 Auto 란 자기 자신을 의미한다. 즉 Auto regression 이란 '자기 회귀' 를 의미한다. 
- $ y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} + \varepsilon_{t}, $
    - 위에서 $\varepsilon_{t}$ 는 백색잡음을 나타낸다. 
    - $y_{t}$ 는 t 시점에서의 관측치를 나타낸다.
- 즉 이전 시점의 관측값들을 회귀로서 이용하는것이다.
- 하지만 이때 '회귀' 를 이용하기 때문에 계절성에 취약하고, 이전 관측값만을 모델에서 이용하기때문에 예측값의 변동을 정확하게 잡아내지 못한다는 단점이 있다.
- 단변량이기 때문에 오는 어쩔 수 없는 부정확함이 있다.

- statmodel 에서는 아래와 같은 식으로 나온다


```python
Image('./Pictures/Auto_Regression.PNG')
```




    
<img src="/assets/images/1.Time Series Model/output_14_0.png">
    



- statmodel 의 시계열 생성 순서는
1. 모델 정의
2. fitting 
3. predict 로 구성된다


```python
# AR example
from statsmodels.tsa.ar_model import AutoReg
from random import random
```


```python
# 모델 생성. 우선 외생변수가 없고, lags(모델의 차수) 를 10으로 주자.
model = AutoReg(train_df['rh (%)'],lags=10)
```


```python
# fitting 작업. 
model_fit = model.fit()
```


```python
# 11~12 번째 인덱스
train_df.iloc[11:13,:]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
    <tr>
      <th>Date Time</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2009-01-01 02:00:00</th>
      <td>996.62</td>
      <td>-8.88</td>
      <td>264.54</td>
      <td>-9.77</td>
      <td>93.2</td>
      <td>3.12</td>
      <td>2.90</td>
      <td>0.21</td>
      <td>1.81</td>
      <td>2.91</td>
      <td>1312.25</td>
      <td>0.25</td>
      <td>0.63</td>
      <td>190.3</td>
    </tr>
    <tr>
      <th>2009-01-01 02:10:00</th>
      <td>996.63</td>
      <td>-8.85</td>
      <td>264.57</td>
      <td>-9.70</td>
      <td>93.5</td>
      <td>3.12</td>
      <td>2.92</td>
      <td>0.20</td>
      <td>1.82</td>
      <td>2.93</td>
      <td>1312.11</td>
      <td>0.16</td>
      <td>0.50</td>
      <td>158.3</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 아래와 같이 진행하면, predict 하는 기준은 index 가 된다.
# 아래 predict 구간은 11번째 index ~ 12 번째 인덱스(포함)
model_fit.predict(start = 11,end = 12)
```




    2009-01-01 02:00:00    93.036358
    2009-01-01 02:10:00    93.049346
    Freq: 10T, dtype: float64



## ARX

-  이 모델은 외생변수가 있는 AR 모델로 아래와 같이 작동한다.


```python
# fit model
model = AutoReg(train_df['rh (%)'],
                lags=10, # lag 는 얼마나 많은 차수를 넣을지 결정하는것이다.
                exog=train_df.drop(columns =['rh (%)']) # 외생변수를 고려한다. AR 에서의 모델 설명과 같이 선형으로 들어가며 이를 넣으면 다변량 시계열이 된다.
               ) 
model_fit = model.fit()
```


```python
test_df['rh (%)'].plot()
```




    <AxesSubplot:xlabel='Date Time'>




    
<img src="/assets/images/1.Time Series Model/output_24_1.png">
    



```python
train_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
    <tr>
      <th>Date Time</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2009-01-01 00:10:00</th>
      <td>996.52</td>
      <td>-8.02</td>
      <td>265.40</td>
      <td>-8.90</td>
      <td>93.3</td>
      <td>3.33</td>
      <td>3.11</td>
      <td>0.22</td>
      <td>1.94</td>
      <td>3.12</td>
      <td>1307.75</td>
      <td>1.03</td>
      <td>1.75</td>
      <td>152.3</td>
    </tr>
    <tr>
      <th>2009-01-01 00:20:00</th>
      <td>996.57</td>
      <td>-8.41</td>
      <td>265.01</td>
      <td>-9.28</td>
      <td>93.4</td>
      <td>3.23</td>
      <td>3.02</td>
      <td>0.21</td>
      <td>1.89</td>
      <td>3.03</td>
      <td>1309.80</td>
      <td>0.72</td>
      <td>1.50</td>
      <td>136.1</td>
    </tr>
    <tr>
      <th>2009-01-01 00:30:00</th>
      <td>996.53</td>
      <td>-8.51</td>
      <td>264.91</td>
      <td>-9.31</td>
      <td>93.9</td>
      <td>3.21</td>
      <td>3.01</td>
      <td>0.20</td>
      <td>1.88</td>
      <td>3.02</td>
      <td>1310.24</td>
      <td>0.19</td>
      <td>0.63</td>
      <td>171.6</td>
    </tr>
    <tr>
      <th>2009-01-01 00:40:00</th>
      <td>996.51</td>
      <td>-8.31</td>
      <td>265.12</td>
      <td>-9.07</td>
      <td>94.2</td>
      <td>3.26</td>
      <td>3.07</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.08</td>
      <td>1309.19</td>
      <td>0.34</td>
      <td>0.50</td>
      <td>198.0</td>
    </tr>
    <tr>
      <th>2009-01-01 00:50:00</th>
      <td>996.51</td>
      <td>-8.27</td>
      <td>265.15</td>
      <td>-9.04</td>
      <td>94.1</td>
      <td>3.27</td>
      <td>3.08</td>
      <td>0.19</td>
      <td>1.92</td>
      <td>3.09</td>
      <td>1309.00</td>
      <td>0.32</td>
      <td>0.63</td>
      <td>214.3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2009-10-05 18:00:00</th>
      <td>988.25</td>
      <td>10.61</td>
      <td>284.73</td>
      <td>9.38</td>
      <td>92.1</td>
      <td>12.81</td>
      <td>11.79</td>
      <td>1.01</td>
      <td>7.46</td>
      <td>11.93</td>
      <td>1207.72</td>
      <td>1.13</td>
      <td>2.25</td>
      <td>136.5</td>
    </tr>
    <tr>
      <th>2009-10-05 18:10:00</th>
      <td>988.25</td>
      <td>10.52</td>
      <td>284.64</td>
      <td>9.21</td>
      <td>91.6</td>
      <td>12.73</td>
      <td>11.66</td>
      <td>1.07</td>
      <td>7.37</td>
      <td>11.80</td>
      <td>1208.16</td>
      <td>1.70</td>
      <td>3.00</td>
      <td>134.4</td>
    </tr>
    <tr>
      <th>2009-10-05 18:20:00</th>
      <td>988.25</td>
      <td>10.48</td>
      <td>284.60</td>
      <td>9.16</td>
      <td>91.5</td>
      <td>12.70</td>
      <td>11.62</td>
      <td>1.08</td>
      <td>7.34</td>
      <td>11.75</td>
      <td>1208.35</td>
      <td>1.68</td>
      <td>3.25</td>
      <td>125.8</td>
    </tr>
    <tr>
      <th>2009-10-05 18:30:00</th>
      <td>988.29</td>
      <td>10.37</td>
      <td>284.49</td>
      <td>9.19</td>
      <td>92.4</td>
      <td>12.60</td>
      <td>11.64</td>
      <td>0.96</td>
      <td>7.36</td>
      <td>11.78</td>
      <td>1208.86</td>
      <td>0.78</td>
      <td>1.63</td>
      <td>125.0</td>
    </tr>
    <tr>
      <th>2009-10-05 18:40:00</th>
      <td>988.36</td>
      <td>10.38</td>
      <td>284.49</td>
      <td>9.24</td>
      <td>92.6</td>
      <td>12.61</td>
      <td>11.68</td>
      <td>0.93</td>
      <td>7.38</td>
      <td>11.82</td>
      <td>1208.89</td>
      <td>0.47</td>
      <td>1.00</td>
      <td>136.1</td>
    </tr>
  </tbody>
</table>
<p>40000 rows × 14 columns</p>
</div>




```python
len(train_df)
```




    40000




```python
test_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>p (mbar)</th>
      <th>T (degC)</th>
      <th>Tpot (K)</th>
      <th>Tdew (degC)</th>
      <th>rh (%)</th>
      <th>VPmax (mbar)</th>
      <th>VPact (mbar)</th>
      <th>VPdef (mbar)</th>
      <th>sh (g/kg)</th>
      <th>H2OC (mmol/mol)</th>
      <th>rho (g/m**3)</th>
      <th>wv (m/s)</th>
      <th>max. wv (m/s)</th>
      <th>wd (deg)</th>
    </tr>
    <tr>
      <th>Date Time</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2009-10-05 18:50:00</th>
      <td>988.42</td>
      <td>10.33</td>
      <td>284.44</td>
      <td>9.17</td>
      <td>92.5</td>
      <td>12.57</td>
      <td>11.63</td>
      <td>0.94</td>
      <td>7.35</td>
      <td>11.76</td>
      <td>1209.20</td>
      <td>0.61</td>
      <td>1.25</td>
      <td>42.71</td>
    </tr>
    <tr>
      <th>2009-10-05 19:00:00</th>
      <td>988.51</td>
      <td>10.07</td>
      <td>284.17</td>
      <td>9.02</td>
      <td>93.2</td>
      <td>12.35</td>
      <td>11.51</td>
      <td>0.84</td>
      <td>7.28</td>
      <td>11.65</td>
      <td>1210.47</td>
      <td>0.61</td>
      <td>1.13</td>
      <td>356.10</td>
    </tr>
    <tr>
      <th>2009-10-05 19:10:00</th>
      <td>988.51</td>
      <td>9.92</td>
      <td>284.02</td>
      <td>9.00</td>
      <td>94.0</td>
      <td>12.23</td>
      <td>11.49</td>
      <td>0.73</td>
      <td>7.26</td>
      <td>11.63</td>
      <td>1211.12</td>
      <td>1.02</td>
      <td>1.63</td>
      <td>1.64</td>
    </tr>
    <tr>
      <th>2009-10-05 19:20:00</th>
      <td>988.50</td>
      <td>9.87</td>
      <td>283.97</td>
      <td>9.05</td>
      <td>94.6</td>
      <td>12.19</td>
      <td>11.53</td>
      <td>0.66</td>
      <td>7.29</td>
      <td>11.66</td>
      <td>1211.31</td>
      <td>0.89</td>
      <td>1.50</td>
      <td>30.70</td>
    </tr>
    <tr>
      <th>2009-10-05 19:30:00</th>
      <td>988.48</td>
      <td>9.70</td>
      <td>283.80</td>
      <td>8.94</td>
      <td>95.0</td>
      <td>12.05</td>
      <td>11.45</td>
      <td>0.60</td>
      <td>7.23</td>
      <td>11.58</td>
      <td>1212.05</td>
      <td>1.35</td>
      <td>1.88</td>
      <td>32.56</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2009-10-12 17:00:00</th>
      <td>991.27</td>
      <td>6.25</td>
      <td>280.11</td>
      <td>3.29</td>
      <td>81.3</td>
      <td>9.52</td>
      <td>7.74</td>
      <td>1.78</td>
      <td>4.87</td>
      <td>7.81</td>
      <td>1232.25</td>
      <td>1.57</td>
      <td>3.28</td>
      <td>151.90</td>
    </tr>
    <tr>
      <th>2009-10-12 17:10:00</th>
      <td>991.44</td>
      <td>5.83</td>
      <td>279.68</td>
      <td>3.44</td>
      <td>84.6</td>
      <td>9.25</td>
      <td>7.82</td>
      <td>1.42</td>
      <td>4.92</td>
      <td>7.89</td>
      <td>1234.27</td>
      <td>1.67</td>
      <td>3.46</td>
      <td>215.00</td>
    </tr>
    <tr>
      <th>2009-10-12 17:20:00</th>
      <td>991.66</td>
      <td>5.67</td>
      <td>279.50</td>
      <td>3.44</td>
      <td>85.5</td>
      <td>9.15</td>
      <td>7.82</td>
      <td>1.33</td>
      <td>4.92</td>
      <td>7.89</td>
      <td>1235.24</td>
      <td>1.86</td>
      <td>3.76</td>
      <td>264.00</td>
    </tr>
    <tr>
      <th>2009-10-12 17:30:00</th>
      <td>991.82</td>
      <td>5.64</td>
      <td>279.45</td>
      <td>2.97</td>
      <td>82.9</td>
      <td>9.12</td>
      <td>7.56</td>
      <td>1.56</td>
      <td>4.76</td>
      <td>7.63</td>
      <td>1235.73</td>
      <td>1.97</td>
      <td>4.12</td>
      <td>274.60</td>
    </tr>
    <tr>
      <th>2009-10-12 17:40:00</th>
      <td>992.02</td>
      <td>5.54</td>
      <td>279.34</td>
      <td>2.80</td>
      <td>82.5</td>
      <td>9.06</td>
      <td>7.48</td>
      <td>1.59</td>
      <td>4.70</td>
      <td>7.54</td>
      <td>1236.46</td>
      <td>2.10</td>
      <td>3.80</td>
      <td>255.20</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 14 columns</p>
</div>




```python
y_pred=(pd.DataFrame(model_fit.predict(len(train_df), # 시작 지점은 train_df 이후의 첫 지점부터
                               len(train_df)+len(test_df)-1, # 마지막 index 는 
                               exog_oos = test_df.drop(columns =['rh (%)']))))
```


```python
plt.plot(y_pred)
plt.plot(test_df['rh (%)'])
```




    [<matplotlib.lines.Line2D at 0x1f74899af88>]




    
<img src="/assets/images/1.Time Series Model/output_29_1.png">
    


- 어느정도 prediction 이 원래 prediction 의 추이와 비슷하다.

# Moving Average (MA)


- 이동 평균 모델은 과거 예측값의 오차들을 이용한다.
- $ y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t-1} + \theta_{2}\varepsilon_{t-2} + \dots + \theta_{q}\varepsilon_{t-q} $
    - 이 떄에 $\varepsilon_{t}$ 는 백색잡음을 나타낸다.
    - $y_{t}$ 는 t 시점에서의 관측치를 나타낸다.
- 즉 이전의 '에러(모델이 예측하지 못한 값들)' 의 가중합들이, 그 다음 예측치의 값 이라는 의미이다.
- 사실 AR,MA 는 별 의미가 없는것이 ARMIMA 에서 모두 포함되기 때문이다. 여기서는 간단히 예측하자.


```python
# MA example
from statsmodels.tsa.arima.model import ARIMA
from random import random
# contrived dataset
```


```python
model = ARIMA(train_df['rh (%)'], order=(0, 0, 8))
model_fit = model.fit()
# make prediction
y_pred = model_fit.predict(len(train_df), len(train_df)+3)
print(y_pred)
```

    2009-10-05 18:50:00    91.771171
    2009-10-05 19:00:00    89.996137
    2009-10-05 19:10:00    87.665994
    2009-10-05 19:20:00    84.828505
    Freq: 10T, Name: predicted_mean, dtype: float64
    

# ARIMA

- ARIMA 란 차분과 MR, MA 모델을 합한것이다.
- 차분을 하는 이유는, 대부분의 시계열 모델들은 정상성을 가정하고 모델을 만들기 때문이다.
- 그에 따라 차분을 진행하여(혹은 Log,exp 등의 변환을 하기도 한다) 시계열을 정상화 시키고 MA 등의 모델을 구하려는 편이다. 
- 즉 여기에서는 $y'$ 를 차분을 진행한 시계열이라고 하겠다. 

- $ y_{t}' = c + \phi_{1}y_{t-1}' + \cdots + \phi_{p}y_{t-p}' + \theta_{1}\varepsilon_{t-1} + \cdots + \theta_{q}\varepsilon_{t-q} + \varepsilon_{t} $   
- AR(p), I(차분)(d), MA(q) 모델이 합쳐지면 ARIMA(p, d, q) 가 된다. 
- 여전히 계절성에 대해서는 고려를 하지 않았기 때문에 계절성에는 취약한 모델이다


```python
# ARMA example
from statsmodels.tsa.arima.model import ARIMA
from random import random
# contrived dataset
data = [random() for x in range(1, 100)]
# fit model
model = ARIMA(data, order=(2, 0, 1))
model_fit = model.fit()
# make prediction
yhat = model_fit.predict(len(data), len(data))
print(yhat)
```

    [0.55369458]
    

# SARIMA


- Seasonal Autoregressive Integrated Moving-Average

- $(1 - \phi_{1}B)~(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} = (1 + \theta_{1}B)~ (1 + \Theta_{1}B^{4})\varepsilon_{t} $

- 위의 모델은 $ARIMA(1,1,1)(1,1,1)_4$ 모델로서, 계절성이 있는 경우 계절성 ARMA 를 양옆에 곱해줌으로서 계절성도 고려하게 된다.


```python
# SARIMA example
from statsmodels.tsa.statespace.sarimax import SARIMAX
from random import random
# contrived dataset
data = [x + random() for x in range(1, 100)]
# fit model
model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))
model_fit = model.fit(disp=False)
# make prediction
yhat = model_fit.predict(len(data), len(data))
print(yhat)
```

    [100.57060415]
    

# SARIMAX

- Exogenous(외생변수) 가 있는 경우에 사용 가능하다.
- 위에서 다룬 모델들은 대부분 단변량 시계열인데에 비해, 이 SRIMAX 는 아래와 같이 외생변수 X 를 포함하게 된다. 

- $ y_{t}' = c + \phi_{1}y_{t-1}' + \cdots + \phi_{p}y_{t-p}' + \theta_{1}\varepsilon_{t-1} + \cdots + \theta_{q}\varepsilon_{t-q} + \varepsilon_{t} + X\beta $ 

- 이 때에 외생변수 X 는 원래 시계열 $y_t$ 와 그 길이가 일치해야한다.
- ARIMAX 등의 경우도 위와 같다.



```python
# SARIMAX example
from statsmodels.tsa.statespace.sarimax import SARIMAX
from random import random
# contrived dataset
data1 = [x + random() for x in range(1, 100)]
data2 = [x + random() for x in range(101, 200)]
# fit model
model = SARIMAX(data1, # 원 시계열 
                exog=data2, # 왜생변수
                order=(1, 1, 1), # ARIMA
                seasonal_order=(0, 0, 0, 0) # 계절 ARIMA
               )
model_fit = model.fit(disp=False #disp 는 convergence massage 를 표시할지 안할지이다. 
                     )
# make prediction
exog2 = [200 + random()]
yhat = model_fit.predict(len(data1), # 예측할 처음 index
                         len(data1), # 예측할 마지막 index
                         exog=[exog2] # 외생변수를 표기해 주어야, 이 변수는 예측에서 빼게 된다.
                        ) 
print(yhat)
```

    [100.60496094]
    

# VAR
- Vector Autoregression


- 사용
    - 내생변수들이 외생변수에 영향을끼치는 복잡한 관계를 잘 모델링 하고 싶음
    - 서로 연관되어 있는 시계열 변수들을 예측하기 위해 사용되거나
    - 충격이 모형내 변수에 미치는 영향을 분석하는데에 많이 사용된다.
    
- VAR 모형
    - 모든 변수들을 서로 시차변수로 보고 서로 영향을 준다고 생각한다.
    - 즉 굳이 어떤 변수는 영향을 받고(외생) 영향을 주고(내생) 를 구분하지 않고 그냥 모두 모델링 해 버린다.
    


```python
Image('./Pictures/VAR.PNG')
```




    
<img src="/assets/images/1.Time Series Model/output_47_0.png">
    



- 특징
    - AR 모형의 다변수 버전으로 볼 수 있다. 
    - VAR 모형의 기본 가정은 Let the data speak for themselves 이다. 즉 모형에 별다른 제약을 부여하지 않는다.
    - 모형이 단순해서 어떤 변수가 내생/외생 변수인지 고민하지 않아도 된다.
    - 각 방정식에 대해 통상적으로 OLS 방법이 가능하다. 
    - 하지만 모형 설정시에 Prior(사전정보) 를 넣을 껀덕지가 없어서 사전 지식을 활용하기가 애매하다.
    - 개별 모수들의 추정치에 대해 해석이 어렵다.(서로 영향을 끼치는 관계가 매우 얽혀있으므로)
    - 추정해야 할 모수들이 너무 많다. 
    - m개의 변수를 가지는 var 모형의 경우 m 개의 변수들 모두 정상 시계열이여야 한다. 이는 비정상 시계열인 경우 차분 등을 통해 변환을 해야 한다는 것이다. 하지만 다변량 시계열의 경우 모두 차분을 같이 진행해야 하기 때문에 모두 정상화 하기가 어렵다.


```python
# VAR example
from statsmodels.tsa.vector_ar.var_model import VAR
from random import random
# contrived dataset with dependency
data = list()
for i in range(100):
    v1 = i + random()
    v2 = v1 + random()
    row = [v1, v2]
    data.append(row)
# fit model
model = VAR(data)
model_fit = model.fit()
# make prediction
yhat = model_fit.forecast(model_fit.y, steps=1)
print(yhat)
```

    [[100.73563436 101.34825942]]
    

# VARMA
- Vector Autoregression Moving-Average


- 이 경우 ARMA 의 Vactor 버전이라고 생각하면 쉽다.


```python
# VARMA example
from statsmodels.tsa.statespace.varmax import VARMAX
from random import random
# contrived dataset with dependency
data = list()
for i in range(100):
    v1 = random()
    v2 = v1 + random()
    row = [v1, v2]
    data.append(row)
# fit model
model = VARMAX(data, order=(1, 1))
model_fit = model.fit(disp=False)
# make prediction
yhat = model_fit.forecast()
print(yhat)
```

    [[0.46259388 0.9833181 ]]
    

# VARMAX
- Vector Autoregression Moving-Average with Exogenous Regressors 

- 위의 VARMA 에 외생변수를 넣은것이다.

- 이 때에 외생변수는 다른 변수와 Ind 하다고 생각되어지는 것을 넣으면 딘다.


```python
Image('./Pictures/VARMAX.PNG')
```




    
<img src="/assets/images/1.Time Series Model/output_55_0.png">
    




```python
# VARMAX example
from statsmodels.tsa.statespace.varmax import VARMAX
from random import random
# contrived dataset with dependency
data = list()
for i in range(100):
    v1 = random()
    v2 = v1 + random()
    row = [v1, v2]
    data.append(row)
data_exog = [x + random() for x in range(100)]
# fit model
model = VARMAX(data, exog=data_exog, order=(1, 1))
model_fit = model.fit(disp=False)
# make prediction
data_exog2 = [[100]]
yhat = model_fit.forecast(exog=data_exog2)
print(yhat)
```

    [[0.4550078  0.82452328]]
    

# ARCH

- 내일 코스피 지수는 '오늘 코스피 지수 +- 무작위 오차'식으로 예측할 수 있다. 그러나 지수의 변동성(표준편차, 분산)은 오늘 낮으면 내일도 낮고 오늘 높으면 내일도 높은 식의 관계가 존재한다.(volatility clustering) 바꿔 말해서 박스피는 계속 박스피로 남고, 천천히 올라가는 장이면 계속 천천히 오르거나 내리고, 급락/급등장에서는 또 급락/급등이 이어질 확률이 높다는 것이다. 따라서 지수 자체를 예측하는것은 힘들지만 그 변동성을 예측하기위해 나온 모형이 ARCH, GARCH 모형이다


```python
Image('./Pictures/ARCH.PNG')
```




    
<img src="/assets/images/1.Time Series Model/output_59_0.png">
    



-  위와 같이 시계열 $Y_t$ 의 예측에, 일반적으로 분산이 일정한 백색잡음이 아니아, 이전 에러가 누적되어서 분산이 추정되는 형태를 띠고있다.
- 즉 점점 에러가 커진다면 그에 따라서 분산도 커지는 형태를 띠는것이다.


```python
import random
```


```python
data = [random.gauss(0, i*0.01) for i in range(1,100+1)]
```


```python
# create a simple white noise with increasing variance
from random import gauss
from random import seed
from matplotlib import pyplot
# seed pseudorandom number generator
seed(1)
# create dataset
data = [gauss(0, i*0.01) for i in range(0,100)]
# plot
pyplot.plot(data)
pyplot.show()
```


    
<img src="/assets/images/1.Time Series Model/output_63_0.png">
    



```python
# check correlations of squared observations
from random import gauss
from random import seed
from matplotlib import pyplot
import numpy as np
from statsmodels.graphics.tsaplots import plot_acf
# seed pseudorandom number generator
seed(1)
# create dataset
data = [gauss(0, i*0.01) for i in range(0,100)]
# square the dataset
squared_data = np.array([x**2 for x in data])
# create acf plot
plot_acf(squared_data,lags=40)
# 에러의 제곱이 어느정도 상관관계를 나타내고 있다.
# 그러므로 ARCH 로 그 관계를 잡아내는게 좋아보인다
pyplot.show()
```


    
<img src="/assets/images/1.Time Series Model/output_64_0.png">
    



```python
# example of ARCH model
from random import gauss
from random import seed
from matplotlib import pyplot
from arch import arch_model
# seed pseudorandom number generator
seed(1)
# create dataset
data = [gauss(0, i*0.01) for i in range(0,100)]
# split into train/test
n_test = 10
train, test = data[:-n_test], data[-n_test:]
# define model
model = arch_model(train, mean='Zero', vol='ARCH', p=15)
# fit model
model_fit = model.fit()
# forecast the test set
yhat = model_fit.forecast(horizon=n_test)
# plot the actual variance
var = [i*0.01 for i in range(0,100)]
pyplot.plot(var[-n_test:])
# plot forecast variance
pyplot.plot(yhat.variance.values[-1, :])
pyplot.show()
```

    Iteration:      1,   Func. Count:     18,   Neg. LLF: 88230.5022386919
    Iteration:      2,   Func. Count:     36,   Neg. LLF: 145.16343062922158
    Iteration:      3,   Func. Count:     54,   Neg. LLF: 128.1580370626462
    Iteration:      4,   Func. Count:     72,   Neg. LLF: 109.21755150713977
    Iteration:      5,   Func. Count:     90,   Neg. LLF: 36.505742305639636
    Iteration:      6,   Func. Count:    108,   Neg. LLF: 39.657385490391924
    Iteration:      7,   Func. Count:    126,   Neg. LLF: 28.719725927033075
    Iteration:      8,   Func. Count:    143,   Neg. LLF: 28.020270372022498
    Iteration:      9,   Func. Count:    161,   Neg. LLF: 34.94721660620315
    Iteration:     10,   Func. Count:    180,   Neg. LLF: 30.036228247245106
    Iteration:     11,   Func. Count:    198,   Neg. LLF: 26.916795778948643
    Iteration:     12,   Func. Count:    216,   Neg. LLF: 35.2329891094671
    Iteration:     13,   Func. Count:    235,   Neg. LLF: 25.557781342763224
    Iteration:     14,   Func. Count:    253,   Neg. LLF: 25.49687891972917
    Iteration:     15,   Func. Count:    271,   Neg. LLF: 25.486203599625494
    Iteration:     16,   Func. Count:    289,   Neg. LLF: 25.486111292597755
    Iteration:     17,   Func. Count:    307,   Neg. LLF: 25.48214954406786
    Iteration:     18,   Func. Count:    324,   Neg. LLF: 25.480673723552396
    Iteration:     19,   Func. Count:    341,   Neg. LLF: 25.478363018255724
    Iteration:     20,   Func. Count:    358,   Neg. LLF: 25.47763917869243
    Iteration:     21,   Func. Count:    375,   Neg. LLF: 25.47751081815898
    Iteration:     22,   Func. Count:    392,   Neg. LLF: 25.477507747392423
    Iteration:     23,   Func. Count:    408,   Neg. LLF: 25.47750771371601
    Optimization terminated successfully    (Exit mode 0)
                Current function value: 25.477507747392423
                Iterations: 23
                Function evaluations: 408
                Gradient evaluations: 23
    


    
<img src="/assets/images/1.Time Series Model/output_65_1.png">
    


# GARCH


```python
Image('./Pictures/GARCH.PNG')
```




    
<img src="/assets/images/1.Time Series Model/output_67_0.png">
    



- 위와 같이 그 분산이 이전 분산들과도 연관되어있다. 
- ARCH 의 문제점은 ARCH 의 차수를 결정하는 문제와
- 실제로 필요한 차수 Q 의 값이 너무 클 수 있다는 문제 
- 그리고 모델에서의 제약으로 추정 모수가 모두 non negative 여야 하는데 이런 제약조건과 문제를 해결하기 위해 자연스레 나온것이 GARCH 이며 거의 GARCH(1,1) 모형을 쓴다고 한다.


```python
# define model
model = arch_model(train, mean='Zero', vol='GARCH', p=15, q=15)
```


```python
# example of ARCH model
from random import gauss
from random import seed
from matplotlib import pyplot
from arch import arch_model
# seed pseudorandom number generator
seed(1)
# create dataset
data = [gauss(0, i*0.01) for i in range(0,100)]
# split into train/test
n_test = 10
train, test = data[:-n_test], data[-n_test:]
# define model
model = arch_model(train, mean='Zero', vol='GARCH', p=15, q=15)
# fit model
model_fit = model.fit()
# forecast the test set
yhat = model_fit.forecast(horizon=n_test)
# plot the actual variance
var = [i*0.01 for i in range(0,100)]
pyplot.plot(var[-n_test:])
# plot forecast variance
pyplot.plot(yhat.variance.values[-1, :])
pyplot.show()
```

    Iteration:      1,   Func. Count:     33,   Neg. LLF: 134.2317665883839
    Iteration:      2,   Func. Count:     70,   Neg. LLF: 95218.99536961706
    Iteration:      3,   Func. Count:    103,   Neg. LLF: 544.0666800125144
    Iteration:      4,   Func. Count:    136,   Neg. LLF: 359.8214533896114
    Iteration:      5,   Func. Count:    169,   Neg. LLF: 120.68464234035743
    Iteration:      6,   Func. Count:    202,   Neg. LLF: 57.67987707496318
    Iteration:      7,   Func. Count:    235,   Neg. LLF: 37.23386150760782
    Iteration:      8,   Func. Count:    268,   Neg. LLF: 44.284822402737774
    Iteration:      9,   Func. Count:    301,   Neg. LLF: 30.955486567364034
    Iteration:     10,   Func. Count:    334,   Neg. LLF: 26.957562041761186
    Iteration:     11,   Func. Count:    366,   Neg. LLF: 31.35843339906071
    Iteration:     12,   Func. Count:    400,   Neg. LLF: 30.014600649363558
    Iteration:     13,   Func. Count:    433,   Neg. LLF: 28.46149312768924
    Iteration:     14,   Func. Count:    466,   Neg. LLF: 27.69989027031613
    Iteration:     15,   Func. Count:    499,   Neg. LLF: 26.904432413344225
    Iteration:     16,   Func. Count:    532,   Neg. LLF: 25.516080289875774
    Iteration:     17,   Func. Count:    564,   Neg. LLF: 25.51091128515439
    Iteration:     18,   Func. Count:    597,   Neg. LLF: 25.783699075265094
    Iteration:     19,   Func. Count:    630,   Neg. LLF: 25.492115623407066
    Iteration:     20,   Func. Count:    662,   Neg. LLF: 25.485422726387913
    Iteration:     21,   Func. Count:    694,   Neg. LLF: 25.479684219263245
    Iteration:     22,   Func. Count:    726,   Neg. LLF: 25.477822173984134
    Iteration:     23,   Func. Count:    758,   Neg. LLF: 25.477525737012506
    Iteration:     24,   Func. Count:    790,   Neg. LLF: 25.477511629030527
    Iteration:     25,   Func. Count:    822,   Neg. LLF: 25.4775078504283
    Iteration:     26,   Func. Count:    853,   Neg. LLF: 25.47750781341174
    Optimization terminated successfully    (Exit mode 0)
                Current function value: 25.4775078504283
                Iterations: 26
                Function evaluations: 853
                Gradient evaluations: 26
    


    
<img src="/assets/images/1.Time Series Model/output_70_1.png">
    


# SES
- Simple Exponential Smoothing


```python
# SES example
from statsmodels.tsa.holtwinters import SimpleExpSmoothing
from random import random
# contrived dataset
data = [x + random() for x in range(1, 100)]
# fit model
model = SimpleExpSmoothing(data)
model_fit = model.fit()
# make prediction
yhat = model_fit.predict(len(data), len(data))
print(yhat)
```

    [99.66183441]
    

# HWES
- Simple Exponential Smoothing


```python
# HWES example
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from random import random
# contrived dataset
data = [x + random() for x in range(1, 100)]
# fit model
model = ExponentialSmoothing(data)
model_fit = model.fit()
# make prediction
yhat = model_fit.predict(len(data), len(data))
print(yhat)
```

    [99.29607266]
    

# LSTM

# GRU


# Reference

- https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/
- https://machinelearningmastery.com/develop-arch-and-garch-models-for-time-series-forecasting-in-python/
- Forecasting: Principles and Practice - Rob J Hyndman and George Athanasopoulos
- http://econ22.hosting.paran.com/metric2/applied_timeseries_III.pdf



```python

```
