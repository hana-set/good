---
title:  "1.Keras"
excerpt: "케라스에 대해서 알아봅시다."
categories:
  - Py_Analysis
tags:
  - 22
last_modified_at: 2021-02-07

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
---

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import pandas as pd
from IPython.display import Image
import matplotlib.pyplot as plt

np.random.seed(42)
tf.random.set_seed(42)
```

from tensorflow import keras 로 먼저 import 한다.

# 모델형성

numpy / pandas # 데이터셋 형성하기

## keras.model.Sequential()

instance를 생성한다. 제일 먼저 이루어져야 하는 층

## model.add()

모델을 층(layer)을 쌓아가면서 구성한다.<br>

### keras.layers.Dense()

fully connected layer 를 추가한다.

- units(첫번째 인자) : <br>
    맨 앞에 쓰이는 정수값으로서 output의 갯수를 의미 <Br>
- input_dim : <br> 
    맨 앞의 layer 층에서 쓰인다. 입력데이터 X의 한 샘플에 대한 모양의 dim 을 적어주면 된다. <br>
    주로 X_train.shape[1:] 를 쓴다. (X_train 의 row(데이터 수) 가 제일 앞에 나오고 그 뒤는 데이터의 형태이기 떄문)
- activation : 활성화함수를 추가한다.<br>
    SELU>ELU>Leakyrelu>Relu>Tanh>Sigmoid 일반적 성능 순서 <br>
    - linear
        - 디폴트값. 활성화 함수 없이 그냥 뉴런과 가중치의 계산 결과 그대로 출력
    - sigmoid
        - 주로 이진분류문제의 맨 마지막 출력층에 사용 
    - softmax
        - 주로 다중클래스분류의 맨 마지막 출력층에 사용되어 확률을 출력
    - relu 
        - hidden layer 에 주로 사용되는 활성화함수
        - 널리 사용되어 많은 library/cpu 등이 최적화되어있음 즉 속도가 제일빠르다.
    - LeakyRelu
        - 하이퍼파라미터 a 가 새는 정도를 결정한다.
        - relu 보다 성능이 뛰어나다고 한다.
    - PReLU 
        - a 를 변수취급하여, 모델이 학습할떄에 변한다.
        - 성능이 좋지만, parameter 가 하나 늘어 과적합 우려(즉 데이터셋 클떄 시도해보자)
    - ELU 
        - a=1 일때 모든구간에서 매끄러워 경사하강법의 속도를 올려준다.
        - relu 보다 성능 뛰어남
    - selu
        - 조건 : X_input 이 표준화 / 가중치는 르쿤표준화 / 네트워크는 일렬로 쌓여야 한다.
        - 훈련하는 동안 자기정규화가 일어나서 각 층의 출력이 평균0, 분산1을 유지하는 경향이 있다.(그라디언트 explode/vanishing 을 막아준다.)
        - 아주 깊은 network 에서 매우 좋은 성능을 낸다.
- kernel_initializer="he_normal"<br>
    초기에 연결 가중치를 어떻게 초기화할지의 방법<br>
    kernel initializer 가 초기화하는 부분은 노드 전에 이어져있는 부분이다. -(이부분의 w)- node
    - he_normal/he_uniform : HE 초기화. Relu 함수와 그 변종(LeakyRelu/PRelu/Elu 등) 을 위한 초기화방법<br> 
    - GlorotNormal/GlorotUniform(default) : 글로럿 초기화. 활성화함수가 없거나, tanh/sigmoid/softmax 를 위한 초기화방법<br>
    - lecun_normal : 르쿤 초기화. SELU 를 위한 초기화방법<br>
- kernel_regularizer<br>
    규제를 준다

### keras.layers.Flatten()

input 에 대해서 1dim 의 벡터 형태의 데이터로 변환해준다.
   
- input_shape=[]<br>
    input으로 받는 데이터의 모양 (ex [28,28])
   

### keras.layers.BatchNormalization()

배치 정규화.  입력값에 대해서 정규화를 통해 스케일과 평균을 조절해 데이터의 분포를 비슷하게 맞춰준다. <br>
주로 활성화 함수를 통과하기 전에 실행한다.<br>


   

### keras.layers.Dropout()

- drop out 은 test 시에는 사용하면 안된다! pytorch 에서는 이것을 위해 따로 지정해주어야 하지만 keras 에서는 자동으로 꺼준다 ^^

### keras.layers.AlphaDropout()


## model.compile()

손실함수 정의 등 학습과정 설정

### optimizer

최적화 방법

- sgd      = keras.optimizers.SGD(lr=0.001, momentum=0.9)
- momentum = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)
- Adagrad  = keras.optimizers.Adagrad(lr=0.001)
- RMSprop  = keras.optimizers.RMSprop(lr=0.001, rho=0.9)
- Adam     = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
- Adamax   = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)
- Nadam    = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)<br>
<BR>
이때에 위에 parameter 외에 opimizer 에 추가할수 있는것은
- decay : learning rate 를 학습마다 얼마나 줄어들지
- gradient 제한 (사실 보통의 네트워크는 batch normalization 이면 충분하다. 하지만 RNN 같이 배치정규화를 적용하기 힘든경우 이런방법을 쓴다고 한다.)
    - clipvalue = 1.0 : 그라디언트 모든 값의 상/하안은 1/-1 이다. 즉 [100,0.5] -> [1,0.5] <br>
    - clipnorm = 1.0 : l_2 norm 이 1보다 큰 경우 값이 비율을 유지한채로 줄인다. [100,0.5] -> [0.99995 , 0.0008]

### loss

손실함수. 모델 fitting 시 이걸 기준으로 최적화
 - mse : mean squared error
 - binary_crossentropy <br>
     : 이진분류를 할 떄에 corss entropy계산 <br>
     : 이진분류시 출력층 : sigmoid + 손실함수 binary cross entropy 사용
 - categorical_crossentropy <br>
     : y가 one hot incoding 이 되어있는 경우 cross entropy를 계산 (target과 output의 shape가 같아야 함) <br>
     : softmax 출력층과 같이 사용된다.
 - sparse_categorical_crossentropy <br>
     : y가 ont hot incoding 이 안되있는경우 ex1~10 의 값을 가지는 1dim vector corss entropy 계산 <br>
     : softmax 출력층과 같이 사용된다.

### metrics

모델의 평가 기준 
 - accuracy (Calculates how often predictions equals labels) (회귀떄에는 사용불가. 의미가없음)

## model.fit()

**첫번째 인자**

X_train

**두번째 인자**

y_train

**batch_size = 32(default)**<br>
   
학습할떄 몇개의 Sample 을 묶어서 update 할떄에 적용할지
 
**epochs =5**<br>

총 데이터를 몇번 반복해서 학습할지
       
**shuffle =True**<br>

학습하면서 set 을 random 하게 섞을지
        
**verbose = 1(default)**<br>

학습이 이루어지는 과정을 직접 볼지 안볼지 (default = 1 로 과정을 볼 수 있다.)

**validation_data(X_val,y_val)**

검증데이터를 사용한다.<br>
검증데이터 사용시 각 에포크 마다 검증데이터의 정확도도 같이 출력이 되는데, 이 경우 훈련이 잘 되고있는지를 보여줄 뿐이며 실제로 모델이 검증데이터를 학습하지는 않는다. <br>
검증데이터의 loss 가 낮아지다가 높아지기 시작하면 이는 overfitting 
 
**validation_split = 0.2**<br>
     
검증데이터를 사용하는것은 동일하지만 , 별도로 존재하는 검증데이터를 주는게 아니라 , X_train과 y_train에서 일정 비율을 분리하여 이를 검증 데이터로 사용한다.<br>
역시나 훈련 자체에는 반영되지 않고 훈련 과정을 지켜보기 위한 용도로 사용. validation_data 대신 사용 가능하다.

### callback 

[checkpoint,earlystopping,tensorboard,lr_scheduler]

**1.keras.callbacks.ModelCheckpoint()** <br>

모델이 기준에 따라 제일 좋을떄에 저장하는 옵션

checkpoint = keras.callbacks.ModelCheckpoint() <br>
model.fit(callbacks=[checkpoint]) # 처럼 사용 <br>
- filepath<br>
    문자열. 모델 파일을 저장할 경로 <br>
- monitor='val_loss' <br>
    어떤것을 기준으로 모델을 저장할지 <br>
- verbose=0, <br>
- save_best_only=False <br>
    save_best_only는 모델의 정확도가 최고값을 갱신했을 때만 저장하도록 하는 옵션 <br>
- save_weights_only=False <br>
    true 면 모델의 가중치만 저장된다. <br>
- mode='auto'<br>
    svae_best_only True 일 경우 <br>
    min : 우리가 정한 기준의 최소값보다 낮을떄 저장 덧씌우기(loss)<br>
    max : 우리가 정한 기준의 최대값보다 높을떄 모델 덧씌워서 저장(acc)<br>
    auto : 우리가 정한 loss 에 따라 자동유추(추천!)<br> 
- period=1<br>
    체크포인트 간격 <br>

**2.keras.callbacks.EarlyStopping()** <br>

모델이 기준에 따라 더 학습되지 않는다고 판단하면 멈추는 옵션

earlystop = keras.callbacks.ModelEarlyStopping() <br>
history = model.fit(callbacks=[earlystop]) # fitting 시에 EarlyStopping 이 같이 들어간다.
- monitor='val_loss'<br>
    어떤것을 기준으로 학습을 stop 할지 결정
- min_delta=0 <br>
    우리가 정한 기준을 바탕으로 모델이 향상되었다고 판단할 최소한의 변화. min delta 미만의 절대적 변화는 향상되었다고 판단하지 않는다.
- patience=0<br>
    개선이 없다고 바로 종료하지 않고, 인내를 가지고 기다려주는 값으로 epoch 를 몇번이나 더 허용할지 결정
- verbose=0 <br>
- mode='auto'<br>
    min : 정한 기준값이 더이상 감소하지 않으면 학습이 멈춘다.(loss 와 사용)<br>
    max : 정한 기준값이 더이상 증가하지 않으면 학습이 멈춘다.(acc 와 사용)<br>
    auto : 우리가 정한 기준값에서 방향성이 자동으로 유추된다. <br>
- baseline=None <br>
    관찰하는 수량이 도달해야하는 베이스라인 값 모델이 베이스라인을 초과하는 향상을 보이지 않으면 학습이 멈춘다.
- restore_best_weights=False <br>

**3.keras.callbacks.TensorBoard()**

**4.keras.callbacks.ReduceLROnPlateau()**

학습률을 점점 줄여나가는 스케일링 옵션

lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)<br>
history = model.fit(callbacks=[lr_scheduler])

- factor = 0.1<br>
    모델이 향상이 없으면 learning rate 를 얼마나 줄일지
- patience = 10<br>
    모델이 향상이 없을때 얼마나 더 지켜볼지

## model.evaluate()

**model.evaluate # 모델 평가하기**
테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가한다.

- 첫번째 인자 = X_test

- 두번쨰 인자 = y_test

- batch_size :<br>
    왜 evaluate 에 batch size 가 필요할까? 그건 계산속도를 빠르게 하기 위함이다. 묶어서 계산할뿐이라 값은 거의 차이 안난다.<br>
    The evaluate function of Model has a batch size just in order to speed-up evaluation, as the network can process multiple samples at a time, and with a GPU this makes evaluation much faster. I think the only way to reduce the effect of this would be to set batch_size to one.


## model.predict()

**model.predict() # 모델 예측하기**

- 첫번째 인자 : 예측하고자 하는 데이터 (X_input)
- 가장 높은 클래스에만 관심이 있으면 model.predict_classes 를 쓴다.
    - y_pred = model.predict_classes(X_new)<br>
      y_pred

# 모델 살펴보기

history = model.fit(X,y) 으로 fitting 해서 model 을 형성했다고 하자.

## fit.history()

**fit.history**

fit = model.fit() 으로 학습시키고 나면, 이제 이 fit 에는 우리가 학습시킬때의 과정중에서 loss. accuracy(모델의 성능평가 metric 에서 지정한 기준으로) 가 어떻게 변화하였는지를 알려주는 dictionary 가 형성된다. key와 value 를 이용해서 그래프를 그려서 학습과정을 시각화할 수 있을것이다.<br>
- pd.DataFrame(fit.history).plot()<br>
    - 이 명령어를 사용하면 각 epoch 별 loss,accuracy 의 변화를 살펴볼 수 있다. <br>
    - 이떄에 만일 fit에서 validation data 도 설정하였다면 이 validation set 에서의 loss/ accuracy 또한 같이 산출해준다. <br>
    - 그래프를 보면 초기에 validation에서의 accuracy/loss가 train 에 비해 더 낮아보인다. 이는 있을 수 없는일인데(fitting 이 train 에 대해서 일어나므로) 왜 이런현상이 일어나냐면, validation 의 accuracy/loss 는 epoch 가 끝난 후 측정되고, train 의 accuracy/loss 는 훈련도중에 형성이 되기떄문에, validation 의 경우는 좀 더 완벽한 모델에 대해서 평가가 되기 떄문이다.

## keras.utils.plot_model()
모델이 어떻게 생겼고, 어떤 flow 를 가지는지 모두 보여준다. <br>
keras.utils.plot_model(model, show_shapes=True)

## model.summary()

**model.summary()**<br>
우리가 정의한 모델의 구성요소/ 구조 등을 보여준다.

## model.layers()

**model.layers**<br>
우리가 쌓아올린 model 들의 layer 들을 살펴볼 수 있다. <br>
- hidden1 = model.layers[1] <br>
    weights, biases = hidden1.get_weights() <br>

처럼 layers 의 개별 layer를 하나만 떼어놓은 뒤에 그 layer 의 weights/ biases 값이 구체적으로 어떤값을 가지는지 조사할 수도 있다.

# 모델 저장 및 불러오기

model.fit() 으로 모델을 fitting 했다고 하자. 그러면<br>

## model.save()
model.save("모델이름.h5")<br>
모델을 쥬피터노트북과 같은 경로에 h5 확장자를 가지게 저장한다.<br>

## keras.models.load_model()
model = keras.models.load_model("모델이름.h5")<br>
저장한 모델을 불러온다.

# 재현 가능한 모델 만들기


```python
import tensorflow as tf
import os
import numpy as np
import random

tf.random.set_seed(33)
os.environ['PYTHONHASHSEED'] = str(33)
np.random.seed(33)
random.seed(33)

session_conf = tf.compat.v1.ConfigProto(
    intra_op_parallelism_threads=1, 
    inter_op_parallelism_threads=1)

sess = tf.compat.v1.Session(
    graph=tf.compat.v1.get_default_graph(), 
    config=session_conf)

tf.compat.v1.keras.backend.set_session(sess)

# 이후에 모델을 넣으면 재현가능하게 모델이 형성된다.
# 근데 비용이 비싼 모양이다 ㅜㅜ
```

# keras 모델 예시 및 팁

- x 데이터가 적은 경우에는 hidden layer 를 작게 만들자.
- 데이터가 잡음이 심한경우, 과적합되면 매우 안좋으므로 hidden layer 를 작게 만들자.
- minibatch 의 사이즈는 2의 배수로 설정하자. (컴퓨터 연산의 효율성을 위해서)
- minibatch 의 사이즈는 너무 크게(global min 에 잘 가지만 느림)/ 작게(빠르지만 방향이 안좋음) 말고 적당히 조절해야한다.
- 많은 수의 범주를 분류할 때 중간층의 크기가 너무 작아 네트워크에 정보의 병목이 생기지 않도록 해야 합니다. 예를 들어서 <br>
    model = keras.models.Sequential() <br>
    model.add(keras.layers.Dense(64, activation='relu', input_shape=(10000,))) <br>
    model.add(keras.layers.Dense(4, activation='relu')) <br>
    model.add(keras.layers.Dense(46, activation='softmax')) <br>
    이런식으로 model 을 짠다면 중간의 4개의 hidden nodes 층에서, 많은 정보(46개 클래스의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문입니다. 이 네트워크는 잘 학습하려고 필요한 정보 대부분을 4차원 표현 안에 구겨 넣으려 해도 전부는 넣지는 못했습니다.
- 다중 분류에서 레이블을 다루는 두 가지 방법이 있습니다.
    - 레이블을 범주형 인코딩(또는 원-핫 인코딩)으로 인코딩하고 `categorical_crossentropy` 손실 함수를 사용합니다.
    - 레이블을 정수로 인코딩하고 `sparse_categorical_crossentropy` 손실 함수를 사용합니다.
- X 데이터에 대해, 각 특성이 상이한 스케일을 가진 값을 신경망에 주입하면 문제가 됩니다. 네트워크가 이런 다양한 데이터에 자동으로 맞추려고 할 수 있지만 이는 확실히 학습을 더 어렵게 만듭니다. 이런 데이터를 다룰 때 대표적인 방법은 특성별로 정규화를 하는 것입니다.   
- 회귀는 분류에서 사용했던 것과는 다른 손실 함수를 사용합니다. 평균 제곱 오차(MSE)는 회귀에서 자주 사용되는 손실 함수입니다.
- 비슷하게 회귀에서 사용되는 평가 지표는 분류와 다릅니다. 당연히 정확도 개념은 회귀에 적용되지 않습니다. 일반적인 회귀 지표는 평균 절대 오차(MAE)입니다.<br>
    MAE 를 사용할경우, 해석이 훨씬 쉽다. MAE=5 가 나온다면, 평균적으로 5정도 차이가 난다는것이다.
- parameter 가 많은 네트워크는 epochs 가 진행될수록 loss  매우 빠르게 0에 가까워집니다. 즉 복잡한 네트워크일수록 더 빠르게 훈련 데이터를 모델링할 수 있습니다(결국 훈련 손실이 낮아집니다). 하지만 더욱 과대적합의 가능성이 커집니다. (복잡한 모델이라 train set 에 지나치게 fitting 될 가능성이 크기떄문)
- 과대적합을 피하려면 - 
    * 훈련 데이터를 더 모읍니다.
    * 네트워크의 복잡성을 감소시킵니다.
    * 가중치 규제를 추가합니다.(L1,L2)
    * 드롭아웃을 추가합니다(이제 BEST 인듯!)



## 지름길이 있는 model

- 신경망이 복잡한 패턴(깊게 쌓은 층의 신경망 이용) 과 간단한 패턴(짧은 경로를 사용한 신경망 이용) 을 모두 학습 가능
- 그냥 층을 쌓아서 만들면 간단한 패턴의 데이터가 연속된 층을 많이 지나가면서 왜곡될 수 있다.


```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=42) 
# test size 는 default 로 0.25

# X data scaling
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```


```python
Image("C:/Users/goran/Desktop/Python/Images/keras1.jpg") 
```




    
<img src="/assets/images/1.Keras/output_36_0.jpeg">
    




```python
#------ 모델 만들기 -----#
input_ = keras.layers.Input(shape=X_train.shape[1:])
# input 객체를 만들어야 한다. 이 객체는 shape / dtype 을 포함해서 모델의 입력을 정의
hidden1 = keras.layers.Dense(30,activation="relu")(input_) 
# 옆의 (input_) 를 통해 위에서 정의한 input_ 를 값으로 받는다.
# 이 층은 만들어지자 마자 입력과 함께 함수처럼 호출됩니다. 그래서 함수형 api 라고 한다.
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
# 두번쨰 hidden layer 를 형성합니다. 
concat = keras.layers.concatenate([input_, hidden2])
# concatenate 층을 만들고 또 다시 함수처럼 호출하여 두번쨰 은닉층의 출력과, 처음의 입력을 연결합니다.
output = keras.layers.Dense(1)(concat)
# 하나의 뉴런과 활성화 함수가 없는 출력층을 만들고 Concatenate 층이 만든 결과를 활용해 호출합니다.
model = keras.models.Model(inputs=[input_], outputs=[output]) # 마지막 모델에서는 input / output 을 모두 표시해준다.
# 마지막으로 사용할 입력과 출력을 지정해 케라스 model 을 만듭니다.

#-----모델 compile-----#
model.compile(loss="mean_squared_error", # loss 는 회귀이므로 mse
              optimizer=keras.optimizers.Adam(), # optimizer는 adam (sgd 해보면 처참하다...)
              metrics = ['mae']) # 평가기준은 이상치에 별 영향 덜받고, 해석이 쉬운 MAE

#-----모델 checkpoint-----#
checkpoint = keras.callbacks.ModelCheckpoint(filepath="keras_model.h5", #저장할 모델 이름
                                             monitor = 'val_loss', #monitoring 할 기준
                                             save_best_only=True ) # 

#-----모델 fitting-----#
fit = model.fit(X_train, y_train,
               epochs=20,
               validation_split=0.1,
               callbacks=[checkpoint])

#----- 모델 롤백-----#
model = keras.models.load_model(filepath = "keras_model.h5")

#----- 모델 평가-----#
result = model.evaluate(X_test, y_test)

#-----모델 predict-----#
y_pred = model.predict(X_test)
```

    Train on 13932 samples, validate on 1548 samples
    Epoch 1/20
    13932/13932 [==============================] - 1s 50us/sample - loss: 1.2714 - mae: 0.7803 - val_loss: 0.5292 - val_mae: 0.5308
    Epoch 2/20
    13932/13932 [==============================] - 0s 33us/sample - loss: 0.4575 - mae: 0.4815 - val_loss: 0.4444 - val_mae: 0.4684
    Epoch 3/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.4028 - mae: 0.4528 - val_loss: 0.4167 - val_mae: 0.4612
    Epoch 4/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.3859 - mae: 0.4426 - val_loss: 0.4041 - val_mae: 0.4473
    Epoch 5/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.3777 - mae: 0.4340 - val_loss: 0.3984 - val_mae: 0.4475
    Epoch 6/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.3623 - mae: 0.4266 - val_loss: 0.3939 - val_mae: 0.4420
    Epoch 7/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3529 - mae: 0.4203 - val_loss: 0.3919 - val_mae: 0.4565
    Epoch 8/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3528 - mae: 0.4146 - val_loss: 0.3707 - val_mae: 0.4242
    Epoch 9/20
    13932/13932 [==============================] - 0s 26us/sample - loss: 0.3401 - mae: 0.4106 - val_loss: 0.3718 - val_mae: 0.4382
    Epoch 10/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.3327 - mae: 0.4052 - val_loss: 0.3597 - val_mae: 0.4188
    Epoch 11/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3253 - mae: 0.3996 - val_loss: 0.3531 - val_mae: 0.4077
    Epoch 12/20
    13932/13932 [==============================] - 0s 26us/sample - loss: 0.3326 - mae: 0.3981 - val_loss: 0.3549 - val_mae: 0.4084
    Epoch 13/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3199 - mae: 0.3941 - val_loss: 0.3451 - val_mae: 0.4098
    Epoch 14/20
    13932/13932 [==============================] - 1s 38us/sample - loss: 0.3134 - mae: 0.3906 - val_loss: 0.3410 - val_mae: 0.4103
    Epoch 15/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3154 - mae: 0.3899 - val_loss: 0.3394 - val_mae: 0.4134
    Epoch 16/20
    13932/13932 [==============================] - 0s 32us/sample - loss: 0.3089 - mae: 0.3871 - val_loss: 0.3398 - val_mae: 0.4016
    Epoch 17/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3038 - mae: 0.3838 - val_loss: 0.3418 - val_mae: 0.3968
    Epoch 18/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3093 - mae: 0.3857 - val_loss: 0.3329 - val_mae: 0.4041
    Epoch 19/20
    13932/13932 [==============================] - 0s 30us/sample - loss: 0.3055 - mae: 0.3818 - val_loss: 0.3270 - val_mae: 0.3973
    Epoch 20/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.2974 - mae: 0.3791 - val_loss: 0.3276 - val_mae: 0.3939
    5160/5160 [==============================] - 0s 27us/sample - loss: 0.3075 - mae: 0.3845
    


```python
keras.utils.plot_model(model, show_shapes=True)
```




    
<img src="/assets/images/1.Keras/output_38_0.png">
    




```python
model.summary() # parameter 수가 1239개로 model 에 비해 적은거같기도하고,,
```

    Model: "model_12"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_13 (InputLayer)           [(None, 8)]          0                                            
    __________________________________________________________________________________________________
    dense_36 (Dense)                (None, 30)           270         input_13[0][0]                   
    __________________________________________________________________________________________________
    dense_37 (Dense)                (None, 30)           930         dense_36[0][0]                   
    __________________________________________________________________________________________________
    concatenate_12 (Concatenate)    (None, 38)           0           input_13[0][0]                   
                                                                     dense_37[0][0]                   
    __________________________________________________________________________________________________
    dense_38 (Dense)                (None, 1)            39          concatenate_12[0][0]             
    ==================================================================================================
    Total params: 1,239
    Trainable params: 1,239
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
 pd.DataFrame(fit.history).plot() ; # 과적합은 안일어난듯. 계속되면서 val_loss가 줄어든다.
```


    
<img src="/assets/images/1.Keras/output_40_0.png">
    


와이드나 딥 경로에 다른 입력 특성을 전달하면 어떻게 될까요? (특성 0에서 4까지) 5개의 특성을 와이드 경로에 보내고 (특성 2에서 7까지) 6개의 특성을 딥 경로에 전달하겠습니다. 3개의 특성(특성 2, 3, 4)은 양쪽에 모두 전달됩니다.

## 입력층이 여러개인 model

- 일부 특성은 짧은 경로로 전달하고, 다른 특성은 깊은 경로로 전달하고싶을때에 사용한다.


```python
Image("C:/Users/goran/Desktop/Python/Images/keras2.jpg") 
```




    
<img src="/assets/images/1.Keras/output_44_0.jpeg">
    



- 먼저 X 의 특성 중 매우 간단할것 같은것들은 짧은 경로를 이용하게 하고 싶을떄에 INPUT A 에 넣고
- X 의 특성중 매우 복잡해보이는것 같아서 깊은 경로를 이용하고싶을 떄에 INPUT B 에 넣자. (A,B 경로에 중복될 수 있다!)


```python
X_train.shape
```




    (15480, 8)




```python
X_train_A, X_train_B = X_train[:, 0:5], X_train[:, 5:]
X_test_A, X_test_B = X_test[:, 0:5], X_test[:, 5:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]
```


```python
#----- 모델 만들기-----#
input_A = keras.layers.Input(shape=[5], name="wide_input") 
# name 은 이름을 붙여주는 것이다. 모델이 복잡해지면, 가장 중요한 층에는 이름을 붙여주는것이 나중에 해석할떄에 용이하다.
# input A 의 input 특성은 5개인 1dim vector 
input_B = keras.layers.Input(shape=[3], name="deep_input")
# imput B 의 input 특성은 3개인 1dim vector
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)
model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])

#------모델 compile-----#
model.compile(loss="mean_squared_error", # loss 는 회귀이므로 mse
              optimizer=keras.optimizers.Adam(), # optimizer는 adam (sgd 해보면 처참하다...)
              metrics = ['mae']) # 평가기준은 이상치에 별 영향 덜받고, 해석이 쉬운 MAE

#------ 모델 checkpoint-----#
checkpoint = keras.callbacks.ModelCheckpoint(filepath="keras_model2.h5", #저장할 모델 이름
                                             monitor = 'val_loss', #monitoring 할 기준
                                             save_best_only=True ) # 

#------ 모델 fitting ------#
fit = model.fit((X_train_A, X_train_B), y_train,
                   epochs=20,
                   validation_split=0.1,
               callbacks=[checkpoint])
# fit 과정에서 X_train 부분이 2dim 의 tuple 로 대체되었음을 기억하자.

#------ 모델 롤백 ------#
model = keras.models.load_model(filepath = "keras_model2.h5")

#----- 모델 평가-----#
result = model.evaluate((X_test_A, X_test_B), y_test)

#-----모델 predict-----#
y_pred = model.predict((X_test_A, X_test_B))
```

    Train on 13932 samples, validate on 1548 samples
    Epoch 1/20
    13932/13932 [==============================] - 1s 51us/sample - loss: 1.9678 - mae: 0.9584 - val_loss: 0.8532 - val_mae: 0.6661
    Epoch 2/20
    13932/13932 [==============================] - 0s 32us/sample - loss: 0.6454 - mae: 0.5834 - val_loss: 0.5605 - val_mae: 0.5385
    Epoch 3/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.4604 - mae: 0.4914 - val_loss: 0.4588 - val_mae: 0.4945
    Epoch 4/20
    13932/13932 [==============================] - 0s 31us/sample - loss: 0.4035 - mae: 0.4580 - val_loss: 0.4304 - val_mae: 0.4856
    Epoch 5/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3843 - mae: 0.4443 - val_loss: 0.4091 - val_mae: 0.4536
    Epoch 6/20
    13932/13932 [==============================] - 0s 30us/sample - loss: 0.3839 - mae: 0.4393 - val_loss: 0.4024 - val_mae: 0.4589
    Epoch 7/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3790 - mae: 0.4345 - val_loss: 0.3906 - val_mae: 0.4438
    Epoch 8/20
    13932/13932 [==============================] - 0s 31us/sample - loss: 0.3765 - mae: 0.4309 - val_loss: 0.3849 - val_mae: 0.4405
    Epoch 9/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3719 - mae: 0.4277 - val_loss: 0.3786 - val_mae: 0.4390
    Epoch 10/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3734 - mae: 0.4253 - val_loss: 0.3763 - val_mae: 0.4317
    Epoch 11/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3554 - mae: 0.4219 - val_loss: 0.3720 - val_mae: 0.4311
    Epoch 12/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3534 - mae: 0.4187 - val_loss: 0.3697 - val_mae: 0.4267
    Epoch 13/20
    13932/13932 [==============================] - 0s 28us/sample - loss: 0.3979 - mae: 0.4224 - val_loss: 0.3776 - val_mae: 0.4222
    Epoch 14/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3500 - mae: 0.4165 - val_loss: 0.3698 - val_mae: 0.4285
    Epoch 15/20
    13932/13932 [==============================] - 0s 29us/sample - loss: 0.3523 - mae: 0.4169 - val_loss: 0.3660 - val_mae: 0.4218
    Epoch 16/20
    13932/13932 [==============================] - 0s 31us/sample - loss: 0.3560 - mae: 0.4163 - val_loss: 0.3660 - val_mae: 0.4267
    Epoch 17/20
    13932/13932 [==============================] - 0s 30us/sample - loss: 0.3476 - mae: 0.4140 - val_loss: 0.3669 - val_mae: 0.4213
    Epoch 18/20
    13932/13932 [==============================] - 0s 30us/sample - loss: 0.3466 - mae: 0.4127 - val_loss: 0.3648 - val_mae: 0.4244
    Epoch 19/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.3513 - mae: 0.4138 - val_loss: 0.3807 - val_mae: 0.4339
    Epoch 20/20
    13932/13932 [==============================] - 0s 27us/sample - loss: 0.4286 - mae: 0.4160 - val_loss: 0.3666 - val_mae: 0.4337
    5160/5160 [==============================] - 0s 26us/sample - loss: 0.3471 - mae: 0.4150
    


```python
keras.utils.plot_model(model, show_shapes=True)
```




    
<img src="/assets/images/1.Keras/output_49_0.png">
    




```python
model.summary() #
```

    Model: "model_21"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    deep_input (InputLayer)         [(None, 3)]          0                                            
    __________________________________________________________________________________________________
    dense_55 (Dense)                (None, 30)           120         deep_input[0][0]                 
    __________________________________________________________________________________________________
    wide_input (InputLayer)         [(None, 5)]          0                                            
    __________________________________________________________________________________________________
    dense_56 (Dense)                (None, 30)           930         dense_55[0][0]                   
    __________________________________________________________________________________________________
    concatenate_21 (Concatenate)    (None, 35)           0           wide_input[0][0]                 
                                                                     dense_56[0][0]                   
    __________________________________________________________________________________________________
    output (Dense)                  (None, 1)            36          concatenate_21[0][0]             
    ==================================================================================================
    Total params: 1,086
    Trainable params: 1,086
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
 pd.DataFrame(fit.history).plot() ; # 과적합은 안일어난듯. 계속되면서 val_loss가 줄어든다.
```


    
<img src="/assets/images/1.Keras/output_51_0.png">
    


## 출력층에 여러개인 model

- 여려 출력이 필요한 작업일떄 사용. 예를 들어 그림에 있는 주요 물체를 분류하고 위치를 알고싶을떄에 회귀작업(물체의 위치) + 분류작업(물체의 종류) 를 동시에 해야되기 떄문에 출력층이 여러개가 필요하다
- 동일한 데이터에서 독립적인 여러작업을 수행할때에. 예를 들어서 사람 얼굴 데이터에서 행복/슬픔 분류와 안경 썻는지의 유무 분류를 동시에 하고싶다고 하자. 두 작업을 따로따로 다른 신경망을 이용해서 학습시킬 수 있지만, 보통 작업마다 하나의 출력을 가진 단일 신경망을 훈련시키는것이 보통 더 나은 결과를 낸다고 한다. 
- 규제기법으로 사용 가능하다.(과대적합을 피하기 위해) 예를 들어 신경망 구조 안에 보조출력을 아래 그림처럼 추가할 수 있다. 보조출력을 사용해 하위 네트워크가 나머지 네트워크에 의존하지 않고 그 자체로 유용한것을 학습하는지 확인할 수 있다.


```python
Image("C:/Users/goran/Desktop/Python/Images/keras3.jpg") 
```




    
<img src="/assets/images/1.Keras/output_54_0.jpeg">
    



 ** 우리는 규제 기법으로 사용해본다고 하자. **
- 이번에는 0 부터 4번 특성을 짧은 경로로 보내고 2~7번 특성을 깊은 특성으로 보낸다고 하자.
- 보조출력 (aux) 가 y 값을 제대로 예측한다면, 이는 input B 의 특성과 HIDDEN LAYER 1,2 만으로 쓸만한 예측은 한다는 의미이다.
- 주요출력이 보조출력보다 훨씬 예측을 잘한다면 A input 의 특성이 필요하고, 주 출력 아래의 구조가 모두 필요한 것이 된다.


```python
housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42) 
# test size 는 default 로 0.25
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)
# 이 역시 valid size 는 defaualt 로 0.25

# X data scaling
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid) 
X_test = scaler.transform(X_test)
X_new = X_test[:3] 

X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]
```


```python
# 모델 만들기
input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="main_output")(concat)
aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)
model = keras.models.Model(inputs=[input_A, input_B],
                           outputs=[output, aux_output])

#모델 compile
model.compile(loss=["mse", "mse"],  # output 이 둘다 regression 인 loss 이므로 mse
              loss_weights=[0.9, 0.1],  # output 이 2개이므로 각 가중치 결정. 이떄는 aux_output 보다 output 을 중시하는듯!
              optimizer=keras.optimizers.SGD(lr=1e-3))

# 모델 fitting
history = model.fit([X_train_A, X_train_B], [y_train, y_train],
                    epochs=3,
                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))
#모델 loss 비교하기
total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], 
                                                 [y_test, y_test])

#predict
y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])
```

    Train on 11610 samples, validate on 3870 samples
    Epoch 1/3
    11610/11610 [==============================] - 1s 91us/sample - loss: 1.8883 - main_output_loss: 1.6250 - aux_output_loss: 4.2522 - val_loss: 2.8143 - val_main_output_loss: 2.7901 - val_aux_output_loss: 3.0215
    Epoch 2/3
    11610/11610 [==============================] - 0s 33us/sample - loss: 0.9823 - main_output_loss: 0.8154 - aux_output_loss: 2.4865 - val_loss: 1.1709 - val_main_output_loss: 1.0813 - val_aux_output_loss: 1.9739
    Epoch 3/3
    11610/11610 [==============================] - 0s 32us/sample - loss: 0.8026 - main_output_loss: 0.6846 - aux_output_loss: 1.8636 - val_loss: 0.7997 - val_main_output_loss: 0.7003 - val_aux_output_loss: 1.6921
    5160/5160 [==============================] - 0s 17us/sample - loss: 0.7431 - main_output_loss: 0.6384 - aux_output_loss: 1.7135
    


```python
keras.utils.plot_model(model, show_shapes=True)
```




    
<img src="/assets/images/1.Keras/output_58_0.png">
    




```python
model.summary()
```

    Model: "model_28"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    deep_input (InputLayer)         [(None, 6)]          0                                            
    __________________________________________________________________________________________________
    dense_69 (Dense)                (None, 30)           210         deep_input[0][0]                 
    __________________________________________________________________________________________________
    wide_input (InputLayer)         [(None, 5)]          0                                            
    __________________________________________________________________________________________________
    dense_70 (Dense)                (None, 30)           930         dense_69[0][0]                   
    __________________________________________________________________________________________________
    concatenate_28 (Concatenate)    (None, 35)           0           wide_input[0][0]                 
                                                                     dense_70[0][0]                   
    __________________________________________________________________________________________________
    main_output (Dense)             (None, 1)            36          concatenate_28[0][0]             
    __________________________________________________________________________________________________
    aux_output (Dense)              (None, 1)            31          dense_70[0][0]                   
    ==================================================================================================
    Total params: 1,207
    Trainable params: 1,207
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    # 처음층은 Flatten
    keras.layers.Dense(300, kernel_initializer="he_normal"),
    # initial weight activation 을 LeakyRelu 를 쓰기위해 weight 를 초기화 하는 방법으로 he_normal 을 쓴다.
    # 이 kernel initializer 가 초기화하는 부분은 노드 전에 이어져있는 부분이다. -(이부분의 w)- node --
    keras.layers.LeakyReLU(),
    # default 로 LeakyRelu 의 a 값은 0.3 이다.
    keras.layers.Dense(100, kernel_initializer="he_normal"),
    keras.layers.LeakyReLU(),
    keras.layers.Dense(10, activation="softmax") 
    # softmax activation 을 쓰므로 굳이 initializier 를 쓰지는 않는다.
])

```

## LeakyRelu


```python
model = keras.models.Sequential([
        keras.layers.Flatten(input_shape=[28, 28]),
        # 처음층은 Flatten
        keras.layers.Dense(300, kernel_initializer="he_normal"),
        # initial weight activation 을 LeakyRelu 를 쓰기위해 weight 를 초기화 하는 방법으로 he_normal 을 쓴다.
        # 이 kernel initializer 가 초기화하는 부분은 노드 전에 이어져있는 부분이다. -(이부분의 w)- node --
        keras.layers.LeakyReLU(),
        # default 로 LeakyRelu 의 a 값은 0.3 이다.
        keras.layers.Dense(100, kernel_initializer="he_normal"),
        keras.layers.LeakyReLU(),
        keras.layers.Dense(10, activation="softmax") 
        # softmax activation 을 쓰므로 굳이 initializier 를 쓰지는 않는다.
        ])
```

## PReLU


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, kernel_initializer="he_normal"),
    keras.layers.PReLU(),
    keras.layers.Dense(100, kernel_initializer="he_normal"),
    keras.layers.PReLU(),
    keras.layers.Dense(10, activation="softmax")
])
```

## SELU


```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="selu",
                             kernel_initializer="lecun_normal"))
for layer in range(99):
    # layer 변수는 없지만 그냥 99번 반복한다는 의미로 for 문을 쓴것.
    model.add(keras.layers.Dense(100, activation="selu",
                                 kernel_initializer="lecun_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

## 매우 깊은 신경망

for x in range(n): 를 쓰면 된다.


```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="selu",
                             kernel_initializer="lecun_normal"))
for layer in range(50): # 층이 50개!
    model.add(keras.layers.Dense(100, activation="selu",
                                 kernel_initializer="lecun_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

## 차원이 높은 X_data

아래처럼 Flatten 층을 만들어서 1dim vector 로 변환시킨뒤에 넣어야한다. 아래경우에는 40000 * 28 * 28 의 imput_X


```python
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="selu",
                             kernel_initializer="lecun_normal"))
for layer in range(50): # 층이 50개!
    model.add(keras.layers.Dense(100, activation="selu",
                                 kernel_initializer="lecun_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))
```

## BatchNormalization


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(), 
    #배치 정규화.주로 활성화 함수를 통과하기 전에 실행한다. 즉 먼저 normalization 을 실행한 후에 activation 을 실행해야하므로, act 전에 batch normal 층이 들어간다.
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

일반적으로 bn은 활성화 함수전에 BN을 적용하면 잘 동작한다고하지만...(여기에는 논란의 여지가 있습니다). <br>
또한 `BatchNormalization` 층 이전의 층은 편향을 위한 항이 필요 없습니다. `BatchNormalization` 층이 이를 무효화하기 때문입니다.(편향이 있다고 해도 처음에 normalization 으로 평균을 0으로 만드는데에다가, 편향과 비슷한 역활을 하는 moving average/var 항이 있기떄문에 무쓸모) 따라서 필요 없는 파라미터이므로 `use_bias=False`를 지정하여 층을 만들 수 있습니다:


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(), 
    #배치 정규화.주로 활성화 함수를 통과하기 전에 실행한다. 즉 먼저 normalization 을 실행한 후에 activation 을 실행해야하므로, act 전에 batch normal 층이 들어간다.
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
```

## L1과 L2 규제


```python
# l1 규제는 많은 연결을 0으로 만든다. 마치 lasso 처럼
```


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)), 
    #가중치 w 에 대해 l2 규제가 0.01 이 곱해진 후 들어간다.
    keras.layers.Dense(100, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)),
    keras.layers.Dense(10, activation="softmax",
                       kernel_regularizer=keras.regularizers.l2(0.01))
])
```

    Train on 3000 samples, validate on 1000 samples
    Epoch 1/2
    3000/3000 [==============================] - 1s 340us/sample - loss: 6.1249 - accuracy: 0.7300 - val_loss: 3.6329 - val_accuracy: 0.8130
    Epoch 2/2
    3000/3000 [==============================] - 0s 132us/sample - loss: 2.5630 - accuracy: 0.8033 - val_loss: 1.8545 - val_accuracy: 0.7830
    

## 드롭아웃 규제

- 매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃될 확률 P 를 가진다.(즉 이번 훈련에서는 무시된다.)
- p 를 드롭아웃 비율이라고 하고, 보통 0.1 ~ 0.5 를 지정한다.
- 이렇게 하면 뉴런들이 서로 의존적이지 않게 되어 더 안정적인 네트워크가 되고
- 각 훈련스텝에서 고유의 네트워크가 (2^n) 가능해져서 , 10000번의 훈련을 하면 10000개의 서로다른 신경망을 훈련하게되는거와 동일해진다. 결과적으로 만들어진 신경망은 모든 신경망의 평균의 앙상블로 볼 수 있다.


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    # 드롭아웃 비율. 
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
```

    Train on 3000 samples, validate on 1000 samples
    Epoch 1/2
    3000/3000 [==============================] - 1s 333us/sample - loss: 0.8582 - accuracy: 0.6943 - val_loss: 0.5637 - val_accuracy: 0.8090
    Epoch 2/2
    3000/3000 [==============================] - 0s 137us/sample - loss: 0.5891 - accuracy: 0.7987 - val_loss: 0.5779 - val_accuracy: 0.7950
    

## 알파 드롭아웃 (SELU) 규제

SELU 를 위한 규제이다.


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
```

    Train on 3000 samples, validate on 1000 samples
    Epoch 1/3
    3000/3000 [==============================] - 1s 224us/sample - loss: 1.7262 - accuracy: 0.3863 - val_loss: 1.0407 - val_accuracy: 0.7100
    Epoch 2/3
    3000/3000 [==============================] - 0s 97us/sample - loss: 1.1510 - accuracy: 0.5670 - val_loss: 0.9109 - val_accuracy: 0.7480
    Epoch 3/3
    3000/3000 [==============================] - 0s 100us/sample - loss: 1.0511 - accuracy: 0.6127 - val_loss: 0.9423 - val_accuracy: 0.7380
    

## 맥스 Norm 규제


```python
from functools import partial

MaxNormDense = partial(keras.layers.Dense,
                       activation="selu", kernel_initializer="lecun_normal",
                       kernel_constraint=keras.constraints.max_norm(1.))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    MaxNormDense(300),
    MaxNormDense(100),
    keras.layers.Dense(10, activation="softmax")
])
```


```python

```
