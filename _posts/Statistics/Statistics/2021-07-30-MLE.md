---
title:  "MLE"
excerpt: "왜 MLE가 그렇게 중요하고 자주 쓰이는가"
categories:
  - Stat
tags:
  - 1
last_modified_at: 2021-07-30

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math: true
---

- https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf

#  1. MLE 란..?

- Maximum Likelihood Estimation(MLE) 는 가우스에 의해 처음 제시되었고, 이는 Fisher 가 재발견 하여 통계에서 본격적으로 쓰이게 되었습니다. 

![png](/assets/images/Stat/22_1.png)

- 위와 같이 Likelihood 함수는 데이터 샘플들에 대해서, 이 데이터가 얼마나 Likely 한지를 나타내는 값입니다. 

![png](/assets/images/Stat/22_2.png)

- 위와 같이 Likelihood 를 최대로 하는 추정량을 $\hat{\theta}$ 로 정합니다. 

<br>

# 2. Regularity Condition

- 우리는 먼저 MLE 를 살펴보기 전에, 일반적인 분포(Regularity Condition) 의 조건에 대해서 살펴봅시다.

![png](/assets/images/Stat/22_4.png)

- 위와 같은 Condition 을 R0~R2 라 합니다.

<br>

# 3. Property 1 : Consistency

![png](/assets/images/Stat/22_3.png)

- 먼저 위와 같이 Consistency 를 상기합시다. 
- Consistency 가 중요한 이유는 '샘플 수가 커지면, Estimator 가 좋아진다.' 를 보장하기 떄문입니다.
- 위의 말이 당연한듯이 보이지만, 이를 만족하지 않는 경우도 있다고 합니다.

- MLE 는 R0 ~ R3  이 성립할때에 Consistency 가 보장됩니다.

![png](/assets/images/Stat/22_7.png)

- 이는 위와 같이 좀 더 직관적으로 이해할 수 있는데요
- $L_n (\theta) = \frac{1}{n} \sum log f(X_i \mid \theta)$ 
- $L_n(\theta) \to E_{\theta_0} l(X\mid \theta) = L(\theta)$ by LLN
- 위와 같이 Likelihood 함수는 N 이 커질수록 LLN 에 의해서, 가까워 지고 이는 MLE가 결국 True Estimator 로 일치하게 되는것을 의미합니다. 

<br>

# 4. Property 2 : Assymptotic Normality

![png](/assets/images/Stat/22_5.png)

- 위와 같이, MLE 는 FIsher information 의 역수를 분산으로 가지는 Normal 로 Assymptotic 한 Normal 을 가집니다. 

## Example

![png](/assets/images/Stat/22_6.png)

<br>

# Property 3 : Invariant

![png](/assets/images/Stat/22_9.png)

- 위와 같이 x 가 MLE 라면, MLE 에 대해서 마구 함수를 취해도 MLE 라는 의미입니다. 

<br>

# 5. Regularity Check

![png](/assets/images/Stat/22_8.png)

- 위와 같은 정리는, 'Regularity Condition' , 즉 일반적인 분포일떄에 성립하는데요, 위와 같이, Likelihood 에 대해서 '확률값이 같은' 구간이 존재하면 안됩니다. 
- 즉 단조증가 / 단조 감소의 형태를 띠어야한다는 의미입니다.
- 이 밖에도 분포가 Regularity 를 지키려면, 다양한 제약이 있습니다.
- 하지만 우리가 주로 사용하는 분포 (Norma , Exponential Family ..) 등은 모두 Regulaity 를 만족하므로, 좋은 분포들입니다.

<br>

# Pros and Cons

== 장점 ==

- MLE 로 Estimator 를 구성하면 자동으로 Consistency 가 확보됩니다. 
- MLE 는 Assymptotic 하게 MVUE(Minimum Variance Unbiased Estimator) 가 됩니다. (Rao-Cramer Lower Bound 에서 Variance 는 $1/nI(\theta)$ 로 bound 가 되므로)
- 또한 Approximately 하게 Normal 이므로, 통계량을 MLE 로 구성하면 극한분포가 보장되어있기 떄문에, Inference 가 자동으로 가능합니다. 
  - 즉 다양한 Model 에서, 각 통계량에 대한 Inference 를 어떻게 하냐면, 먼저 MLE 함수가 Convex 하다는것을 증명한 뒤에, Approximation 이 Unique 하다는 것을 증명합니다. 그 이후에 이러한 Approximation 으로 구한 추정값은 Normal 을 따를

== 단점 ==

- MLE 는 Sample 수가 적을떄 너무 왜곡이 심합니다. 
- MLE 를 구하려면, Distribution을 명시해야 합니다. 
  - 머신러닝 모델은 Error 만 정의하면 그 이후는 Optimization 으로 알아서 구해지는것을 기억합시다! 



# Refer

- <https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Consistency>

- https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture3.pdf

- https://www.itl.nist.gov/div898/handbook/eda/section3/eda3652.htm

  
