---
title:  "Multicollinearity Remedy"
excerpt: "다중 공선성을 치료해보자"
categories:
  - Regression
tags:
  - 1
last_modified_at: 2021-09-08

toc: true
toc_label: "Table Of Contents"
toc_icon: "cog"
toc_sticky: true

use_math : true
---

# Intro

- 이전 글에서 Regression model 에서는 다중공선성이 존재하면 매우 안좋은것을 알아보았습니다.
- 이제 , '다중공선성' 이 존재한다면 어떻게 대처해야할지에 대해서 알아봅시다.

# Remedy

## 1. 더미값을 살펴보자.

- 만약 X 변수가 (여름,가을,겨울) 이라고 하자. 이때, 단순하게 3개의 변수를 모두 onehot vector 로 만들어버리면 이떄에는 완벽한 다중공선성이 존재하게 됩니다.
- 즉 일반적으로 통계모델을 사용할때에는 하나의 변수를 Drop 하여 다중공선성 문제를 해결하는 편입니다.

## 2. 그냥 놔두자

- 물론 Multicollinearity 가 있으면, Regression 을 안좋게 만드는것은 맞습니다. 
- 하지만 이전에 살펴보았듯, Regression 의 예측이 이전의 예측을 따르며, 데이터 구간 안쪽에 존재한다면, 데이터의 예측력은 괜찮은 편입니다.
  - 물론 변수 해석에서 안좋은 결과가 나옵니다.

## 3. 변수를 쳐내자.

- 다중 공선성을 지닌다면 결국 변수해석이 어려워지고 모델이 불안정해지므로 삭제하는것이 좋을것입니다. 
- 그 방법론은 다양하게 있지만 여기에서는 대표적인 방법만 소개하겠습니다.
  - Correlation 가 큰 값을 가진다면 두 변수는 다중공선성이 존재할것이다.
  - VIF 확인 

## 4. 변수의 변형을 잘 살펴보자

- 여기에서 변수의 변형이란 $x^2$ 등을 나타낸다.

![png](/assets/images/Stat/56_1.png)

- 위의 그래프는 x 제곱의 그래프입니다. 특정한 범위 내에서는 , 그래프의 변화도가 크지 않기 때문에 x 변수와 선형을 나타냅니다.
- 이는 log x , x^3 등에서도 나타나는 현상입니다.

## 5. Scaling

- 스케일링을 하게되면 변수의 다중공선성이 줄어듭니다. 
  - 이에 대해서는  추후에 좀 더 깊게 다루어보도록 하겠습니다.

## 6. Ridge , Lasso regression

- Ridge , Lasso 와 같은 모델링의 경우, 계수추정을 하게 될때에 , OLS 보다 훨씬 안정적이 됩니다 .

![png](/assets/images/Stat/56_2.png)

- 그 이유는 위와 같습니다. (위의 식은 Ridge regression 의 경우)
- 위처럼, Regularization Term 이 있는 경우에는 최적화할때에, Multicolinearity 가 있어도 좀 더 안정적으로 추정됩니다. 

## 7.PCA 

![png](/assets/images/Stat/56_3.png)

- PCA 는 위처럼 데이터의 분산을 최대한 보존하는 , 직교하는 축을 찾아 데이터를 변형하는 방법입니다.

![png](/assets/images/Stat/56_4.png)

- 이떄에 축들은 위처럼 각 변수들의 선형 결합으로 이루어지게 됩니다.
  - 즉, 변수들 간의 상관성을 낮추고 , Dimension Reduction 의 효과가 있습니다.

![png](/assets/images/Stat/56_5.png)

- 하지만 해석력이 줄어들고 (각 축은 원래 변수의 선형 결합이 되기 때문) 
- 연산량이 많아 느려진다는 단점이 있습니다.



# Refer

- https://en.wikipedia.org/wiki/Multicollinearity#cite_ref-8
- https://humankind.tistory.com/20

